{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"../data/generated/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.optimize import fmin_bfgs, basinhopping, differential_evolution\n",
    "import scipy.optimize as opt \n",
    "from numpy import hstack, zeros, ones, array, mat, tile, reshape, squeeze, eye, asmatrix\n",
    "from gmm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start and end dates of the analysis period\n",
    "start_date = pd.to_datetime('1975-01-01')\n",
    "end_date = pd.to_datetime('2008-04-30')\n",
    "\n",
    "ts = pd.read_csv(resource + 'time_series_1.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts.index.freq = 'M'\n",
    "\n",
    "# Set analysis period\n",
    "ts1 = ts.loc[(ts.index >= start_date) & (ts.index <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Desired error not necessarily achieved due to precision loss.\n",
       "  success: False\n",
       "   status: 2\n",
       "      fun: 0.001689611555707051\n",
       "        x: [-3.604e+01  1.696e+02  8.137e+01 -1.558e+02  3.320e+01\n",
       "            -6.113e+00]\n",
       "      nit: 139\n",
       "      jac: [-1.150e-09  4.802e-10 -1.746e-10  1.019e-10  6.112e-10\n",
       "             6.257e-10]\n",
       " hess_inv: [[ 1.755e+05  1.120e+05 ...  1.777e+05 -3.468e+04]\n",
       "            [ 1.120e+05  8.404e+04 ...  9.996e+04 -2.345e+04]\n",
       "            ...\n",
       "            [ 1.777e+05  9.996e+04 ...  2.224e+05 -7.138e+04]\n",
       "            [-3.468e+04 -2.345e+04 ... -7.138e+04  6.554e+04]]\n",
       "     nfev: 1267\n",
       "     njev: 181"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_port = pd.read_csv(resource + f'bm_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts3 = pd.merge(ts1, bm_port, on='date', how='left')\n",
    "macro_factors = ts3[['myp','ui','dsv','ats','sts','fx']].values\n",
    "financial_factors = ts3[['ex_mkt','smb','hml','mom']].values\n",
    "riskfree = ts3['rf'].values\n",
    "portfolios = ts3[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "T,N = portfolios.shape\n",
    "excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "K = np.size(macro_factors,1)\n",
    "\n",
    "startingVals = zeros(6)\n",
    "# startingVals = [\n",
    "# -49.5247,\n",
    "# -4.7737,\n",
    "# 113.7766,\n",
    "# 25.9245,\n",
    "# 90.8717,\n",
    "# 16.7917\n",
    "# ]\n",
    "# startingVals = [\n",
    "# 1000,\n",
    "# 1000,\n",
    "# 1000,\n",
    "# 1000,\n",
    "# 1000,\n",
    "# 1000\n",
    "#  ]\n",
    "\n",
    "Winv = eye(N)\n",
    "args = (excessRet, macro_factors, Winv)\n",
    "iteration = 0\n",
    "lastValue = 0\n",
    "functionCount = 0\n",
    "# fit_gmm = fmin_bfgs(gmm_objective_b, startingVals, args=(excessRet, macro_factors, eye(N)), callback=iter_print)\n",
    "# fit_gmm = basinhopping(gmm_objective_b, startingVals, minimizer_kwargs={'args': args})\n",
    "# fit_gmm = differential_evolution(gmm_objective_b, bounds=[(1e-10, 1000), (1e-10, 1000), (1e-10, 1000), (1e-10, 1000), (1e-10, 1000), (1e-10, 1000)], args=args)\n",
    "fit_gmm = opt.minimize(gmm_objective_b, startingVals, args=(excessRet, macro_factors, eye(N)),\n",
    "                       method='BFGS', options={'gtol': 1e-10, 'maxiter': 1000})\n",
    "# fit_gmm = opt.minimize(gmm_objective_b, startingVals, args=(excessRet, macro_factors, eye(N)),\n",
    "#                        method='Nelder-Mead')\n",
    "# fit_gmm = opt.minimize(gmm_objective_b, startingVals, args=(excessRet, macro_factors, eye(N)), callback=iter_print, method='L-BFGS-B')\n",
    "fit_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000004\n",
      "         Iterations: 138\n",
      "         Function evaluations: 174\n",
      "         Gradient evaluations: 174\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>estimate_b Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  Hansen J:          </th> <td>0.001690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>estimate_b</td>    <th>  Prob (Hansen J):   </th>  <td>  1.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>GMM</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 10 May 2023</td> <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:52:38</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th> <td>  -36.0391</td> <td>   54.753</td> <td>   -0.658</td> <td> 0.510</td> <td> -143.352</td> <td>   71.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th> <td>  169.5382</td> <td>  202.575</td> <td>    0.837</td> <td> 0.403</td> <td> -227.501</td> <td>  566.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th> <td>   81.3747</td> <td>   56.940</td> <td>    1.429</td> <td> 0.153</td> <td>  -30.225</td> <td>  192.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th> <td> -155.8005</td> <td>  140.894</td> <td>   -1.106</td> <td> 0.269</td> <td> -431.947</td> <td>  120.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th> <td>   33.2021</td> <td>   86.896</td> <td>    0.382</td> <td> 0.702</td> <td> -137.111</td> <td>  203.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th> <td>   -6.1122</td> <td>   34.160</td> <td>   -0.179</td> <td> 0.858</td> <td>  -73.065</td> <td>   60.841</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                              estimate_b Results                              \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   Hansen J:                     0.001690\n",
       "Model:                     estimate_b   Prob (Hansen J):                  1.00\n",
       "Method:                           GMM                                         \n",
       "Date:                Wed, 10 May 2023                                         \n",
       "Time:                        14:52:38                                         \n",
       "No. Observations:                 400                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1           -36.0391     54.753     -0.658      0.510    -143.352      71.274\n",
       "x2           169.5382    202.575      0.837      0.403    -227.501     566.577\n",
       "x3            81.3747     56.940      1.429      0.153     -30.225     192.974\n",
       "x4          -155.8005    140.894     -1.106      0.269    -431.947     120.346\n",
       "x5            33.2021     86.896      0.382      0.702    -137.111     203.515\n",
       "x6            -6.1122     34.160     -0.179      0.858     -73.065      60.841\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.sandbox.regression.gmm import GMM\n",
    "\n",
    "class estimate_b(GMM):\n",
    "    def momcond(self, params):\n",
    "        fRets = self.exog\n",
    "        pRets = self.instrument\n",
    "        T,N = pRets.shape\n",
    "        T,K = fRets.shape\n",
    "        b = params\n",
    "        b = squeeze(array(params)) \n",
    "        b = reshape(b,(K,1))\n",
    "        sdf = ones((T,1)) - fRets @ b\n",
    "        moments = pRets * kron(sdf,ones((1,N)))\n",
    "        return moments\n",
    "\n",
    "gmm_mod = estimate_b(endog=zeros(macro_factors.shape[0]), exog=macro_factors, instrument=excessRet)\n",
    "gmm_fit8 = gmm_mod.fit(start_params=zeros(6), maxiter=1, inv_weights=eye(N), weights_method='hac', wargs={'maxlag':12}, optim_method='bfgs', optim_args={'gtol': 1e-10, 'maxiter': 1000})\n",
    "gmm_fit8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01571008063240392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b = ([ -49.5247,\n",
    "# -4.7737,\n",
    "# 113.7766,\n",
    "# 25.9245,\n",
    "# 90.8717,\n",
    "# 16.7917])\n",
    "# b = ([-46.47,  4.675,  108.4,  4.868,  94.44,\n",
    "                #   12.19])\n",
    "# b = ([ -50.01969851,  22.80054891, 107.80604282, -27.68159424,\n",
    "#         85.04568974,   4.42476452])\n",
    "# b = ([ -30.11158833,  237.66483381,   88.98292253, -148.44282895,\n",
    "#          52.95375349,  -13.72251736])\n",
    "b = ([ -2.448e+01,  2.359e+02,  8.662e+01, -1.550e+02,  6.203e+01,\n",
    "            -1.383e+01])\n",
    "# b = ([ 20.24410124, -1.89313026, 16.9439695 , -2.33814035, -1.56179756,\n",
    "#        -2.41001749])\n",
    "J = gmm_objective_b(b, excessRet, macro_factors, eye(N))\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class mimick(GMM):\n",
    "#     def momcond(self, params):\n",
    "#         x = self.exog\n",
    "#         y = self.endog\n",
    "#         T,K = x.shape\n",
    "#         y = reshape(y,(T,1))\n",
    "#         b = params\n",
    "#         b = squeeze(array(params)) \n",
    "#         b = reshape(b,(K,1))\n",
    "#         err = (y - x @ b)\n",
    "#         moments = x * kron(err,ones((1,K)))\n",
    "#         return moments\n",
    "\n",
    "# # gmm_mod = mimick(endog=log_indprod_growth_nextyear, exog=macro_factors, instrument=None)\n",
    "# # gmm_fit8 = gmm_mod.fit(start_params=zeros(23), maxiter=1, inv_weights=eye(N), weights_method='hac', wargs={'maxlag':11}, optim_method='bfgs', optim_args={'gtol': 1e-10, 'maxiter': 1000})\n",
    "# # gmm_fit8.summary()\n",
    "\n",
    "# mimick_exog = ts1[['ex_mkt',\n",
    "#                    'ex_b10ret',\n",
    "#                    'ex_b5ret',\n",
    "#                 #    'ex_high_yd_bd_ret',\n",
    "#                    'ex_gold_ret', \n",
    "#                    'slope_ex_mkt_87', \n",
    "#                    'slope_ex_mkt_9602', \n",
    "#                    'rf', \n",
    "#                    'lag_10y_3m_gov_bd_yd',\n",
    "#                    'lag_1y_3m_gov_bd_yd',\n",
    "#                    'lag_Baa_Aaa_bd_yd',\n",
    "#                    'lag_sp_div_yd',\n",
    "#                    'log_indprod_growth_lastyear',\n",
    "#                    'infl_lastyear',\n",
    "#                    'ex_mkt_lastyear',\n",
    "#                    'lag_ex_mkt',\n",
    "#                    'lag_ex_b10ret',\n",
    "#                    'lag_ex_b5ret',\n",
    "#                    'lag_ex_gold_ret',\n",
    "#                    'lag_slope_ex_mkt_87',\n",
    "#                    'lag_slope_ex_mkt_9602'\n",
    "#                    ]]\n",
    "# mimick_exog = sm.add_constant(mimick_exog)\n",
    "# mimick_endog = ts1['log_indprod_growth_nextyear']\n",
    "# T,K = mimick_exog.shape\n",
    "\n",
    "\n",
    "# mimick_mod = mimick(endog=mimick_endog, exog=mimick_exog, instrument=None)\n",
    "# mimick_fit = mimick_mod.fit(start_params=np.zeros(K), maxiter=1, inv_weights=np.eye(K), weights_method='hac', wargs={'maxlag':11}, optim_method='bfgs', optim_args={'gtol': 1e-12, 'maxiter': 1000})\n",
    "# # gmm_fit8 = gmm_mod.fit(start_params=np.zeros(K), maxiter=1, inv_weights=np.eye(K), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 10000})\n",
    "# mimick_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the one-year ahead industrial production growth expectatitions factor\n",
    "# coef = mimickfull.params\n",
    "# ts['myp'] = coef['ex_mkt'] * ts['ex_mkt'] + coef['ex_long_gov_ret'] * ts['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * ts['ex_medium_gov_ret'] + coef['ex_high_yd_bd_ret'] * ts['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * ts['ex_gold_ret'] + coef['slope_ex_mkt_87'] * ts['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * ts['slope_ex_mkt_9602']\n",
    "# ts[f'myp{i}'] = coef['ex_mkt'] * ts['ex_mkt'] + coef['ex_b10ret'] * ts['ex_b10ret'] + coef['ex_b5ret'] * ts['ex_b5ret'] + coef['ex_gold_ret'] * ts['ex_gold_ret'] + coef['slope_ex_mkt_87'] * ts['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * ts['slope_ex_mkt_9602'] + coef['slope_ex_mkt_0709'] * ts['slope_ex_mkt_0709']\n",
    "\n",
    "# def compute_myp(row):\n",
    "#     if pd.isna(row['ex_high_yd_bd_ret']):\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_long_gov_ret'] * row['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * row['ex_medium_gov_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "#     else:\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_long_gov_ret'] * row['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * row['ex_medium_gov_ret'] + coef['ex_high_yd_bd_ret'] * row['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "    \n",
    "# def compute_myp(row):\n",
    "#     if pd.isna(row['ex_high_yd_bd_ret']):\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_b10ret'] * row['ex_b10ret'] + coef['ex_b5ret'] * row['ex_b5ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "#     else:\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_b10ret'] * row['ex_b10ret'] + coef['ex_b5ret'] * row['ex_b5ret'] + coef['ex_high_yd_bd_ret'] * row['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "\n",
    "# ts['myp'] = ts.apply(compute_myp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the complete dataset\n",
    "ts.to_csv(results + 'time_series_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 2\n",
    "# ts_var = ts.loc[(ts.index >= sdate[i] + MonthBegin(-1)) & (ts.index <= edate[i])]\n",
    "# print(sdate[i] + MonthBegin(-1))\n",
    "# print(sdate[i])\n",
    "\n",
    "# # Var (without gmm)\n",
    "# var = VAR(ts_var[['hml','smb','mom',f'myp{i}','ui','dsv','ats','sts','fx']]).fit(maxlags=1, trend='c')\n",
    "# print(var.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r premia_port\n",
    "# %store -r premia_t_stat_port\n",
    "\n",
    "# # Plot market price of risk\n",
    "# row_name = ([r'Market price of risk ($\\lambda$) (\\times 100)'])\n",
    "\n",
    "# col_name = (['MYP',\n",
    "#              'UI', \n",
    "#              'DSV', \n",
    "#              'ATS',\n",
    "#              'STS', \n",
    "#              'FX'])\n",
    "\n",
    "# tex = []\n",
    "# for index, i in enumerate([0,1,2]): \n",
    "#     premia_table = table_to_latex(pd.DataFrame(premia_port[i] * 100), pd.DataFrame(premia_t_stat_port[i]), col_name, row_name)\n",
    "#     tex.append(premia_table)\n",
    "\n",
    "# # Create files for tables and graphs\n",
    "# for index, t in enumerate(tex):\n",
    "#     # Open a .tex file for writing\n",
    "#     with open(results + tables + f'riskpremia{index+1}.tex', 'w') as f:\n",
    "#         # Write the LaTeX string to the file\n",
    "#         f.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r lambeta_params2\n",
    "%store -r lambeta_bse2\n",
    "%store -r lambeta_tvalues2\n",
    "%store -r lambeta_pvalues2\n",
    "\n",
    "macro_factors = ['MYP', 'UI', 'DSV', 'ATS', 'STS', 'FX']\n",
    "portfolios = ['BM deciles', 'Size deciles', 'Momentum deciles','Profit deciles','Invest deciles','Accruals deciles']\n",
    "\n",
    "for index, j in enumerate([0,1,2,3,4,5]):\n",
    "        # Create a figure with subplots for each column\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(40, 5))\n",
    "        # Generate random data for example\n",
    "        coefficients = lambeta_params2[j][17:-6].reshape((10,6))\n",
    "        # coefficients = np.arange(1, beta.shape[0]+1)\n",
    "        t_stats = lambeta_tvalues2[j][17:-6].reshape((10,6))\n",
    "        # t_stats = np.arange(1, beta_t_stat.shape[0]+1)\n",
    "\n",
    "        # Loop through each column and plot the histogram and line plot\n",
    "        for index2, i in enumerate(range(coefficients.shape[1])):\n",
    "            # Plot histogram of coefficients on left y-axis\n",
    "            axs[i].bar(np.arange(1, coefficients.shape[0]+1), coefficients[:, i], color='blue')\n",
    "            axs[i].set_ylabel('Coefficients', color='blue', fontsize=15)\n",
    "            axs[i].tick_params(axis='y', labelcolor='blue', labelsize=15)\n",
    "            \n",
    "            # Add line plot of t-stats on right y-axis\n",
    "            axs2 = axs[i].twinx()\n",
    "            axs2.plot(np.arange(1, t_stats.shape[0]+1), t_stats[:, i], color='red', linewidth=3)\n",
    "            axs2.set_ylabel('T-Stats', color='red', fontsize=15)\n",
    "            axs2.tick_params(axis='y', labelcolor='red', labelsize=15)\n",
    "            \n",
    "            # Set x-axis label and tick marks\n",
    "            axs[i].set_xlabel(portfolios[index], fontsize=25)\n",
    "            axs[i].set_xticks(np.arange(1, coefficients.shape[0]+1))\n",
    "            axs[i].tick_params(axis='x', labelsize=15)\n",
    "            axs[i].set_title(macro_factors[index2], fontsize=35)\n",
    "\n",
    "        # Increase space between subplots\n",
    "        fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "        # Save the figure to the specified location\n",
    "        fig.savefig(results + plots + f'betahist{index+1}_sample2.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r lambeta_params_list\n",
    "%store -r lambeta_bse_list\n",
    "%store -r lambeta_tvalues_list\n",
    "%store -r lambeta_pvalues_list\n",
    "\n",
    "macro_factors = ['MYP', 'UI', 'DSV', 'ATS', 'STS', 'FX']\n",
    "portfolios = ['BM deciles', 'Size deciles', 'Momentum deciles','Profit deciles','Invest deciles','Accruals deciles']\n",
    "\n",
    "\n",
    "for k in range(len(lambeta_params_list)):\n",
    "    lambeta_params = lambeta_params_list[k]\n",
    "    lambeta_tvalues = lambeta_tvalues_list[k]\n",
    "    for index, j in enumerate([0,1,2,3,4,5]):\n",
    "        # Create a figure with subplots for each column\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(40, 5))\n",
    "        # Generate random data for example\n",
    "        coefficients = lambeta_params[j][-66:-6].reshape((10,6))\n",
    "        # coefficients = np.arange(1, beta.shape[0]+1)\n",
    "        t_stats = lambeta_tvalues[j][-66:-6].reshape((10,6))\n",
    "        # t_stats = np.arange(1, beta_t_stat.shape[0]+1)\n",
    "\n",
    "        # Loop through each column and plot the histogram and line plot\n",
    "        for index2, i in enumerate(range(coefficients.shape[1])):\n",
    "            # Plot histogram of coefficients on left y-axis\n",
    "            axs[i].bar(np.arange(1, coefficients.shape[0]+1), coefficients[:, i], color='blue')\n",
    "            axs[i].set_ylabel('Coefficients', color='blue', fontsize=15)\n",
    "            axs[i].tick_params(axis='y', labelcolor='blue', labelsize=15)\n",
    "            \n",
    "            # Add line plot of t-stats on right y-axis\n",
    "            axs2 = axs[i].twinx()\n",
    "            axs2.plot(np.arange(1, t_stats.shape[0]+1), t_stats[:, i], color='red', linewidth=3)\n",
    "            axs2.set_ylabel('T-Stats', color='red', fontsize=15)\n",
    "            axs2.tick_params(axis='y', labelcolor='red', labelsize=15)\n",
    "            \n",
    "            # Set x-axis label and tick marks\n",
    "            axs[i].set_xlabel(portfolios[index], fontsize=25)\n",
    "            axs[i].set_xticks(np.arange(1, coefficients.shape[0]+1))\n",
    "            axs[i].tick_params(axis='x', labelsize=15)\n",
    "            axs[i].set_title(macro_factors[index2], fontsize=35)\n",
    "\n",
    "        # Increase space between subplots\n",
    "        fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "        # Save the figure to the specified location\n",
    "        fig.savefig(results + plots + f'betahist{index+1}_sample{k}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r beta_port\n",
    "# %store -r beta_t_stat_port\n",
    "\n",
    "# macro_factors = ['MYP', 'UI', 'DSV', 'ATS', 'STS', 'FX']\n",
    "# portfolios = ['BM deciles', 'Size deciles', 'Momentum deciles']\n",
    "\n",
    "# for index, j in enumerate([0,1,2]):\n",
    "#     # Create a figure with subplots for each column\n",
    "#     fig, axs = plt.subplots(1, 6, figsize=(40, 5))\n",
    "#     # Generate random data for example\n",
    "#     coefficients = beta_port[j]\n",
    "#     # coefficients = np.arange(1, beta.shape[0]+1)\n",
    "#     t_stats = beta_t_stat_port[j]\n",
    "#     # t_stats = np.arange(1, beta_t_stat.shape[0]+1)\n",
    "\n",
    "#     # Loop through each column and plot the histogram and line plot\n",
    "#     for index2, i in enumerate(range(coefficients.shape[1])):\n",
    "#         # Plot histogram of coefficients on left y-axis\n",
    "#         axs[i].bar(np.arange(1, coefficients.shape[0]+1), coefficients[:, i], color='blue')\n",
    "#         axs[i].set_ylabel('Coefficients', color='blue', fontsize=15)\n",
    "#         axs[i].tick_params(axis='y', labelcolor='blue', labelsize=15)\n",
    "        \n",
    "#         # Add line plot of t-stats on right y-axis\n",
    "#         axs2 = axs[i].twinx()\n",
    "#         axs2.plot(np.arange(1, t_stats.shape[0]+1), t_stats[:, i], color='red', linewidth=3)\n",
    "#         axs2.set_ylabel('T-Stats', color='red', fontsize=15)\n",
    "#         axs2.tick_params(axis='y', labelcolor='red', labelsize=15)\n",
    "        \n",
    "#         # Set x-axis label and tick marks\n",
    "#         axs[i].set_xlabel(portfolios[index], fontsize=25)\n",
    "#         axs[i].set_xticks(np.arange(1, coefficients.shape[0]+1))\n",
    "#         axs[i].tick_params(axis='x', labelsize=15)\n",
    "#         axs[i].set_title(macro_factors[index2], fontsize=35)\n",
    "\n",
    "#     # Increase space between subplots\n",
    "#     fig.subplots_adjust(wspace=0.5)\n",
    "\n",
    "#     # Save the figure to the specified location\n",
    "#     fig.savefig(results + plots + f'betahist{index+1}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class step1_lambeta(GMM):\n",
    "    def momcond(self, params):\n",
    "        fRets = self.exog\n",
    "        pRets = self.endog\n",
    "        coeff = squeeze(array(params)) \n",
    "        T,P = pRets.shape\n",
    "        T,M = fRets.shape\n",
    "        beta = squeeze(array(coeff[:(P*M)]))\n",
    "        lam = squeeze(array(coeff[(P*M):]))\n",
    "        beta = reshape(beta,(P,M))\n",
    "        lam = reshape(lam,(M,1))\n",
    "        betalam = beta @ lam\n",
    "        expectedRet = fRets @ beta.T\n",
    "        e = pRets - expectedRet\n",
    "        moments_beta = kron(e,ones((1,M))) * tile(fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "        moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "        moments = hstack((moments_beta,moments_lam))\n",
    "\n",
    "        return moments\n",
    "    \n",
    "class gmm_lambeta(GMM):\n",
    "    def momcond(self, params):\n",
    "        global nbase\n",
    "        fRets = self.exog\n",
    "        pRets = self.endog\n",
    "        mimick = self.instrument\n",
    "        coeff = squeeze(array(params)) \n",
    "\n",
    "        # first stage\n",
    "        mimick_endog = mimick[:,0]\n",
    "        mimick_exog = mimick[:,1:]\n",
    "        mimick_exog = sm.add_constant(mimick_exog)\n",
    "        T,K = mimick_exog.shape\n",
    "        mimick_exog = reshape(mimick_exog,(T,K))\n",
    "        mimick_endog = reshape(mimick_endog,(T,1))\n",
    "        mimick_coeff = coeff[:K]\n",
    "        mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "        mimick_err = (mimick_endog - mimick_exog @ mimick_coeff)\n",
    "        moments1 = mimick_exog * kron(mimick_err,ones((1,K)))\n",
    "\n",
    "        # compute myp\n",
    "        myp = mimick_exog[:,1:(nbase+1)] @ mimick_coeff[1:(nbase+1)] \n",
    "\n",
    "        # gmm\n",
    "        full_fRets = column_stack((myp, fRets))\n",
    "        T,P = pRets.shape\n",
    "        T,M = full_fRets.shape\n",
    "        betalam_params = params[K:]\n",
    "        beta = squeeze(array(betalam_params[:(P*M)]))\n",
    "        lam = squeeze(array(betalam_params[(P*M):]))\n",
    "        beta = reshape(beta,(P,M))\n",
    "        lam = reshape(lam,(M,1))\n",
    "        betalam = beta @ lam\n",
    "        expectedRet = full_fRets @ beta.T\n",
    "        e = pRets - expectedRet\n",
    "        moments_beta = kron(e,ones((1,M))) * tile(full_fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "        moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "        moments2 = hstack((moments_beta,moments_lam))\n",
    "\n",
    "        return column_stack((moments1,moments2))\n",
    "\n",
    "# function to compute the derivative of the moments with respect to the parameters\n",
    "def gmm_G(params, p_rets, f_rets):\n",
    "    t,n = p_rets.shape\n",
    "    t,k = f_rets.shape\n",
    "    beta = squeeze(array(params[:(n*k)]))\n",
    "    lam = squeeze(array(params[(n*k):]))\n",
    "    beta = reshape(beta,(n,k))\n",
    "    lam = reshape(lam,(k,1))\n",
    "    G = np.zeros((n*k+k,n*k+n))\n",
    "    ffp = (f_rets.T @ f_rets) / t\n",
    "    G[:(n*k),:(n*k)] = kron(eye(n),ffp)\n",
    "    G[:(n*k),(n*k):] = kron(eye(n),-lam)\n",
    "    G[(n*k):,(n*k):] = -beta.T\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Add the derivative of the moment conditions from the mimicking portfolio estimate\n",
    "def add_mimick_deriv(G, mimick):\n",
    "    mimick = sm.add_constant(mimick)\n",
    "    t,k = mimick.shape\n",
    "    i,j = G.shape\n",
    "    D = np.zeros((k+i,k+j))\n",
    "    D[:k,:k] = (mimick.T @ mimick) / t\n",
    "    D[k:,k:] = G\n",
    "    return D.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_exogcolnames = ['ui','dsv','ats','sts','fx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambeta_est_list = []\n",
    "lambeta_params_list = [] \n",
    "lambeta_bse_list = [] \n",
    "lambeta_tvalues_list = []\n",
    "lambeta_pvalues_list = [] \n",
    "\n",
    "for i in range(len(sdate)):\n",
    "    # if i == 0:\n",
    "    #     continue\n",
    "    lambeta_est = []\n",
    "    lambeta_params = [] \n",
    "    lambeta_bse = [] \n",
    "    lambeta_tvalues = []\n",
    "    lambeta_pvalues = [] \n",
    "\n",
    "    for index, port in enumerate(['bm','size','mom','op','inv','ac']):\n",
    "\n",
    "        ##############################################\n",
    "        ######### Prepare the data ###################\n",
    "        ##############################################\n",
    "\n",
    "        # Merge with the portfolio return data\n",
    "        ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "        ts4 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "        # Set the start and end dates of the analysis period\n",
    "        ts4 = ts4.loc[(ts4.index >= sdate[i]) & (ts4.index <= edate[i])]\n",
    "\n",
    "        # set mimick portfolio variables for the subsample\n",
    "        nbase = 7\n",
    "        mimick_colnames_sample = mimick_colnames\n",
    "        if (ts4['slope_ex_mkt_87'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_87' not in colname]\n",
    "            nbase -= 1\n",
    "        if (ts4['slope_ex_mkt_9602'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_9602' not in colname]\n",
    "            nbase -= 1\n",
    "        if (ts4['slope_ex_mkt_0709'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_0709' not in colname]\n",
    "            nbase -= 1\n",
    "        mimick = ts4[mimick_colnames_sample].values\n",
    "\n",
    "        # Set variables and dimensions\n",
    "        T,K = mimick.shape\n",
    "        exog_macro_factors = ts4[macro_exogcolnames].values\n",
    "        T,M = exog_macro_factors.shape\n",
    "        myp = ts4[[f'myp{i}']].values\n",
    "        riskfree = ts4['rf'].values\n",
    "        portfolios = ts4[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "        T,P = portfolios.shape\n",
    "        excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "\n",
    "        ##############################################\n",
    "        ######### First stage ########################\n",
    "        ##############################################\n",
    "\n",
    "        # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "        betas = []\n",
    "        for j in range(P):\n",
    "            res = sm.OLS(excessRet[:,j],sm.add_constant(column_stack((myp, exog_macro_factors)))).fit()\n",
    "            betas.append(res.params[1:])\n",
    "        avgReturn = excessRet.mean(axis=0)\n",
    "        avgReturn.shape = P,1\n",
    "        betas = array(betas)\n",
    "        res = sm.OLS(avgReturn, betas).fit()\n",
    "        riskPremia = res.params\n",
    "        riskPremia.shape = M+1\n",
    "        startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "        # Estimation    \n",
    "        step1lambeta_mod = step1_lambeta(endog=excessRet, exog=column_stack((myp, exog_macro_factors)), instrument=None)\n",
    "        step1lambeta_fit = step1lambeta_mod.fit(start_params=startingVals, \n",
    "                                                maxiter=1, \n",
    "                                                inv_weights=eye(P*(M+2)),\n",
    "                                                weights_method='hac', \n",
    "                                                wargs={'maxlag':12}, \n",
    "                                                optim_method='bfgs', \n",
    "                                                optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "        # print(step1lambeta_fit.summary())\n",
    "\n",
    "        ##############################################\n",
    "        ######### Second stage #######################\n",
    "        ##############################################\n",
    "\n",
    "        # Compute the optimal weights from the first stage\n",
    "        step1optW = step1lambeta_fit.model.calc_weightmatrix(step1lambeta_fit.model.momcond(step1lambeta_fit.params),\n",
    "                                                        weights_method='hac',\n",
    "                                                        wargs={'maxlag':12}\n",
    "                                                        )\n",
    "\n",
    "        # Compute the weights for the second stage \n",
    "        # (force parameters to be equal to parameters from estimating mimicking portfolio and gmm separately)\n",
    "        Grad = step1lambeta_fit.model.gradient_momcond(params=step1lambeta_fit.params, epsilon=1e-4, centered=True).T\n",
    "        npar = Grad.shape[0]\n",
    "        A = zeros((K+npar, K+P*(M+2)))\n",
    "        A[:K, :K] = eye(K)\n",
    "        A[-npar:, -P*(M+2):] = Grad @ step1optW\n",
    "        A = A * 1000000\n",
    "        Winv = A.T @ A\n",
    "\n",
    "        # Starting value \n",
    "        output1_array = np.array(mimick_list[i].params)\n",
    "        output1_array_reshaped = output1_array.reshape(1, -1)\n",
    "        output2_array = np.array(step1lambeta_fit.params)\n",
    "        output2_array_reshaped = output2_array.reshape(1, -1)\n",
    "        startVals = np.concatenate([output1_array_reshaped, output2_array_reshaped], axis=1)\n",
    "\n",
    "        # Estimation    \n",
    "        lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "        lambeta_fit = lambeta_mod.fit(start_params=startVals, \n",
    "                                    maxiter=1, \n",
    "                                    inv_weights=Winv, \n",
    "                                    #   weights_method='hac', \n",
    "                                    #   wargs={'maxlag':12}, \n",
    "                                    optim_method='bfgs', \n",
    "                                    optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "        # print(lambeta_fit.summary())\n",
    "\n",
    "        # Append results for a certain subsample\n",
    "        lambeta_est.append(lambeta_fit)\n",
    "        lambeta_params.append(lambeta_fit.params)\n",
    "        lambeta_bse.append(lambeta_fit.bse)\n",
    "        lambeta_tvalues.append(lambeta_fit.tvalues)\n",
    "        lambeta_pvalues.append(lambeta_fit.pvalues)\n",
    "    \n",
    "    # Append results for all subsample\n",
    "    lambeta_est_list.append(lambeta_est)\n",
    "    lambeta_params_list.append(lambeta_params)\n",
    "    lambeta_bse_list.append(lambeta_bse)\n",
    "    lambeta_tvalues_list.append(lambeta_tvalues)\n",
    "    lambeta_pvalues_list.append(lambeta_pvalues)\n",
    "\n",
    "%store lambeta_params_list\n",
    "%store lambeta_bse_list\n",
    "%store lambeta_tvalues_list\n",
    "%store lambeta_pvalues_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sdate)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if i == 1:\n",
    "        continue\n",
    "    lambeta_est2 = []\n",
    "    lambeta_params2 = [] \n",
    "    lambeta_bse2 = [] \n",
    "    lambeta_tvalues2 = []\n",
    "    lambeta_pvalues2 = [] \n",
    "\n",
    "    for index, port in enumerate(['bm','size','mom','op','inv','ac']):\n",
    "    # for index, port in enumerate(['bm','size']):\n",
    "\n",
    "        ##############################################\n",
    "        ######### Prepare the data ###################\n",
    "        ##############################################\n",
    "\n",
    "        # Merge with the portfolio return data\n",
    "        ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "        ts4 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "        # Set the start and end dates of the analysis period\n",
    "        ts4 = ts4.loc[(ts4.index >= sdate[i]) & (ts4.index <= edate[i])]\n",
    "\n",
    "        # set mimick portfolio variables for the subsample\n",
    "        nbase = 6\n",
    "        mimick_colnames_sample = mimick_colnames\n",
    "        if (ts4['slope_ex_mkt_87'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_87' not in colname]\n",
    "            nbase -= 1\n",
    "        if (ts4['slope_ex_mkt_9602'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_9602' not in colname]\n",
    "            nbase -= 1\n",
    "        mimick = ts4[mimick_colnames_sample].values\n",
    "\n",
    "        # Set variables and dimensions\n",
    "        T,K = mimick.shape\n",
    "        exog_macro_factors = ts4[macro_exogcolnames].values\n",
    "        T,M = exog_macro_factors.shape\n",
    "        myp = ts4[[f'myp{i}']].values\n",
    "        riskfree = ts4['rf'].values\n",
    "        portfolios = ts4[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "        T,P = portfolios.shape\n",
    "        excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "\n",
    "        ##############################################\n",
    "        ######### First stage ########################\n",
    "        ##############################################\n",
    "\n",
    "        # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "        betas = []\n",
    "        for j in range(P):\n",
    "            res = sm.OLS(excessRet[:,j],sm.add_constant(column_stack((myp, exog_macro_factors)))).fit()\n",
    "            betas.append(res.params[1:])\n",
    "        avgReturn = excessRet.mean(axis=0)\n",
    "        avgReturn.shape = P,1\n",
    "        betas = array(betas)\n",
    "        res = sm.OLS(avgReturn, betas).fit()\n",
    "        riskPremia = res.params\n",
    "        riskPremia.shape = M+1\n",
    "        startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "        # Estimation    \n",
    "        step1lambeta_mod = step1_lambeta(endog=excessRet, exog=column_stack((myp, exog_macro_factors)), instrument=None)\n",
    "        step1lambeta_fit = step1lambeta_mod.fit(start_params=startingVals, \n",
    "                                                maxiter=1, \n",
    "                                                inv_weights=eye(P*(M+2)),\n",
    "                                                # weights_method='hac', \n",
    "                                                # wargs={'maxlag':12}, \n",
    "                                                optim_method='bfgs', \n",
    "                                                optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "        # print(step1lambeta_fit.summary())\n",
    "\n",
    "        ##############################################\n",
    "        ######### Second stage #######################\n",
    "        ##############################################\n",
    "\n",
    "        # Compute the optimal weights from the first stage\n",
    "        step1optW = step1lambeta_fit.model.calc_weightmatrix(step1lambeta_fit.model.momcond(step1lambeta_fit.params),\n",
    "                                                        weights_method='hac',\n",
    "                                                        wargs={'maxlag':12},\n",
    "                                                        params=step1lambeta_fit.params)\n",
    "\n",
    "        # Compute the weights for the second stage \n",
    "        # (force parameters to be equal to parameters from estimating mimicking portfolio and gmm separately)\n",
    "        # G = gmm_G(step1lambeta_fit.params*1000, excessRet*1000, column_stack((myp, exog_macro_factors))*1000)\n",
    "        # D = add_mimick_deriv(G, mimick[:,1:]*1000)\n",
    "        # npar = G.shape[0]\n",
    "        # A = zeros((K+npar, K+P*(M+2)))\n",
    "        # A[:K, :K] = eye(K) * 1000\n",
    "        # A[-npar:, -P*(M+2):] = G @ (step1optW * 1000)\n",
    "        # # W1 = eye(K+P*(M+2)) * 1000 - D @ inv(A @ D) @ A\n",
    "        # # S = zeros((K+P*(M+2), K+P*(M+2)))\n",
    "        # # S[:K, :K] = eye(K) * 1000\n",
    "        # # S[K:, K:] = step1optW * 1000\n",
    "        # # Winv = W1 @ S @ W1.T \n",
    "        # Winv = A.T @ A\n",
    "        Grad = step1lambeta_fit.model.gradient_momcond(params=step1lambeta_fit.params, \n",
    "                                                       epsilon=1e-4, \n",
    "                                                       centered=True\n",
    "                                                       ).T\n",
    "        npar = Grad.shape[0]\n",
    "        A = zeros((K+npar, K+P*(M+2)))\n",
    "        A[:K, :K] = eye(K)\n",
    "        A[-npar:, -P*(M+2):] = Grad @ step1optW\n",
    "        A = A * 1000000\n",
    "        Winv = A.T @ A\n",
    "\n",
    "        # Starting value \n",
    "        output1_array = np.array(mimick_list[i].params)\n",
    "        output1_array_reshaped = output1_array.reshape(1, -1)\n",
    "        output2_array = np.array(step1lambeta_fit.params)\n",
    "        output2_array_reshaped = output2_array.reshape(1, -1)\n",
    "        startVals = np.concatenate([output1_array_reshaped, output2_array_reshaped], axis=1)\n",
    "\n",
    "        # Estimation    \n",
    "        lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "        lambeta_fit = lambeta_mod.fit(start_params=startVals, \n",
    "                                    maxiter=1, \n",
    "                                    inv_weights=Winv, \n",
    "                                    #   weights_method='hac', \n",
    "                                    #   wargs={'maxlag':12}, \n",
    "                                    optim_method='bfgs', \n",
    "                                    optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "        # print(lambeta_fit.summary())\n",
    "\n",
    "        # Append results for a certain subsample\n",
    "        lambeta_est2.append(lambeta_fit)\n",
    "        lambeta_params2.append(lambeta_fit.params)\n",
    "        lambeta_bse2.append(lambeta_fit.bse)\n",
    "        lambeta_tvalues2.append(lambeta_fit.tvalues)\n",
    "        lambeta_pvalues2.append(lambeta_fit.pvalues)\n",
    "\n",
    "%store lambeta_params2\n",
    "%store lambeta_bse2\n",
    "%store lambeta_tvalues2\n",
    "%store lambeta_pvalues2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "port = 'bm'\n",
    "# Merge with the portfolio return data\n",
    "ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts4 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "# Set the start and end dates of the analysis period\n",
    "ts4 = ts4.loc[(ts4.index >= sdate[i]) & (ts4.index <= edate[i])]\n",
    "\n",
    "# set mimick portfolio variables for the subsample\n",
    "nbase = 6\n",
    "mimick_colnames_sample = mimick_colnames\n",
    "if (ts4['slope_ex_mkt_87'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_87' not in colname]\n",
    "            nbase -= 1\n",
    "if (ts4['slope_ex_mkt_9602'] == 0).all():\n",
    "            mimick_colnames_sample = [colname for colname in mimick_colnames_sample if 'slope_ex_mkt_9602' not in colname]\n",
    "            nbase -= 1\n",
    "mimick = ts4[mimick_colnames_sample].values\n",
    "\n",
    "T,K = mimick.shape\n",
    "exog_macro_factors = ts4[macro_exogcolnames].values\n",
    "T,M = exog_macro_factors.shape\n",
    "myp = ts4[[f'myp{i}']].values\n",
    "riskfree = ts4['rf'].values\n",
    "portfolios = ts4[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "T,P = portfolios.shape\n",
    "excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "\n",
    "        ##############################################\n",
    "        ######### First stage ########################\n",
    "        ##############################################\n",
    "\n",
    "        # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "betas = []\n",
    "for j in range(P):\n",
    "            res = sm.OLS(excessRet[:,j],sm.add_constant(column_stack((myp, exog_macro_factors)))).fit()\n",
    "            betas.append(res.params[1:])\n",
    "avgReturn = excessRet.mean(axis=0)\n",
    "avgReturn.shape = P,1\n",
    "betas = array(betas)\n",
    "res = sm.OLS(avgReturn, betas).fit()\n",
    "riskPremia = res.params\n",
    "riskPremia.shape = M+1\n",
    "startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "        # Estimation    \n",
    "step1lambeta_mod = step1_lambeta(endog=excessRet, exog=column_stack((myp, exog_macro_factors)), instrument=None)\n",
    "step1lambeta_fit = step1lambeta_mod.fit(start_params=startingVals, \n",
    "                                                maxiter=1, \n",
    "                                                inv_weights=eye(P*(M+2)),\n",
    "                                                # weights_method='hac', \n",
    "                                                # wargs={'maxlag':12}, \n",
    "                                                optim_method='bfgs', \n",
    "                                                optim_args={'gtol': 1e-15, 'maxiter': 100000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1optW = step1lambeta_fit.model.calc_weightmatrix(step1lambeta_fit.model.momcond(step1lambeta_fit.params),\n",
    "                                                        weights_method='hac',\n",
    "                                                        wargs={'maxlag':12}\n",
    "                                                        # params=step1lambeta_fit.params\n",
    "                                                        )\n",
    "# step1optW = eye(step1optW.shape[0])\n",
    "\n",
    "        # Compute the weights for the second stage \n",
    "        # (force parameters to be equal to parameters from estimating mimicking portfolio and gmm separately)\n",
    "Grad = step1lambeta_fit.model.gradient_momcond(params=step1lambeta_fit.params, epsilon=1e-4, centered=True).T\n",
    "npar = Grad.shape[0]\n",
    "A = zeros((K+npar, K+P*(M+2)))\n",
    "A[:K, :K] = eye(K)\n",
    "A[-npar:, -P*(M+2):] = Grad @ step1optW\n",
    "# A[-npar:, -P*(M+2):] = Grad @ eye(P*(M+2))\n",
    "A = A * 1000000  # number are too low\n",
    "Winv = A.T @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting value \n",
    "output1_array = np.array(mimick_list[i].params)\n",
    "output1_array_reshaped = output1_array.reshape(1, -1)\n",
    "output2_array = np.array(step1lambeta_fit.params)\n",
    "output2_array_reshaped = output2_array.reshape(1, -1)\n",
    "startVals = np.concatenate([output1_array_reshaped, output2_array_reshaped], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation    \n",
    "lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "lambeta_fit = lambeta_mod.fit(start_params=startVals, \n",
    "                                    maxiter=0, \n",
    "                                    inv_weights=Winv,\n",
    "                                    # weights_method='hac', \n",
    "                                    # wargs={'maxlag':12},\n",
    "                                #     has_optimal_weights=False, \n",
    "                                    optim_method='bfgs', \n",
    "                                    optim_args={'gtol': 1e-15, 'maxiter': 100000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs, basinhopping, differential_evolution\n",
    "\n",
    "\n",
    "def momcond(params,exog,endog,instrument):\n",
    "        fRets = exog\n",
    "        pRets = endog\n",
    "        mimick = instrument\n",
    "        coeff = squeeze(array(params)) \n",
    "\n",
    "        # first stage\n",
    "        mimick_endog = mimick[:,0]\n",
    "        mimick_exog = mimick[:,1:]\n",
    "        mimick_exog = sm.add_constant(mimick_exog)\n",
    "        T,K = mimick_exog.shape\n",
    "        mimick_exog = reshape(mimick_exog,(T,K))\n",
    "        mimick_endog = reshape(mimick_endog,(T,1))\n",
    "        mimick_coeff = coeff[:K]\n",
    "        mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "        mimick_err = (mimick_endog - mimick_exog @ mimick_coeff)\n",
    "        moments1 = mimick_exog * kron(mimick_err,ones((1,K)))\n",
    "\n",
    "        # compute myp\n",
    "        # myp = mimick_exog[:, 1:8] @ mimick_coeff[1:8]\n",
    "        myp = mimick_exog[:, 1:7] @ mimick_coeff[1:7] \n",
    "\n",
    "        # gmm\n",
    "        full_fRets = column_stack((myp, fRets))\n",
    "        T,P = pRets.shape\n",
    "        T,M = full_fRets.shape\n",
    "        betalam_params = params[K:]\n",
    "        beta = squeeze(array(betalam_params[:(P*M)]))\n",
    "        lam = squeeze(array(betalam_params[(P*M):]))\n",
    "        beta = reshape(beta,(P,M))\n",
    "        lam = reshape(lam,(M,1))\n",
    "        betalam = beta @ lam\n",
    "        expectedRet = full_fRets @ beta.T\n",
    "        e = pRets - expectedRet\n",
    "        moments_beta = kron(e,ones((1,M))) * tile(full_fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "        moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "        moments2 = hstack((moments_beta,moments_lam))\n",
    "\n",
    "        return column_stack((moments1,moments2))\n",
    "\n",
    "def momcond_mean(params,exog,endog,instrument):\n",
    "        momcond1 = momcond(params,exog,endog,instrument)\n",
    "        nobs_moms, k_moms = momcond1.shape\n",
    "        return momcond1.mean(0)\n",
    "\n",
    "def gmmobjective(params,weights,exog,endog,instrument):\n",
    "        moms = momcond_mean(params,exog,endog,instrument)\n",
    "        return np.dot(np.dot(moms, weights), moms)\n",
    "\n",
    "optimizer = fmin_bfgs\n",
    "weights = np.linalg.pinv(Winv)\n",
    "\n",
    "params=optimizer(gmmobjective, startVals, args=(weights,exog_macro_factors,excessRet,mimick), gtol= 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.sandwich_covariance as smcov\n",
    "from statsmodels.tools.numdiff import approx_fprime\n",
    "\n",
    "\n",
    "def calc_weightmatrix(moms, weights_method='cov', wargs=(),\n",
    "                          params=None):\n",
    "        nobs, k_moms = moms.shape\n",
    "\n",
    "        centered = not ('centered' in wargs and not wargs['centered'])\n",
    "        if not centered:\n",
    "            # caller does not want centered moment conditions\n",
    "            moms_ = moms\n",
    "        else:\n",
    "            moms_ = moms - moms.mean()\n",
    "        if weights_method == 'cov':\n",
    "            w = np.dot(moms_.T, moms_)\n",
    "            if 'ddof' in wargs:\n",
    "                # caller requests degrees of freedom correction\n",
    "                if wargs['ddof'] == 'k_params':\n",
    "                    w /= (nobs - self.k_params)\n",
    "                else:\n",
    "                    w /= (nobs - wargs['ddof'])\n",
    "            else:\n",
    "                # default: divide by nobs\n",
    "                w /= nobs\n",
    "\n",
    "        elif weights_method == 'flatkernel':\n",
    "            #uniform cut-off window\n",
    "            # This was a trial version, can use HAC with flatkernel\n",
    "            if 'maxlag' not in wargs:\n",
    "                raise ValueError('flatkernel requires maxlag')\n",
    "\n",
    "            maxlag = wargs['maxlag']\n",
    "            h = np.ones(maxlag + 1)\n",
    "            w = np.dot(moms_.T, moms_)/nobs\n",
    "            for i in range(1,maxlag+1):\n",
    "                w += (h[i] * np.dot(moms_[i:].T, moms_[:-i]) / (nobs-i))\n",
    "\n",
    "        elif weights_method == 'hac':\n",
    "            maxlag = wargs['maxlag']\n",
    "            if 'kernel' in wargs:\n",
    "                weights_func = wargs['kernel']\n",
    "            else:\n",
    "                weights_func = smcov.weights_bartlett\n",
    "                wargs['kernel'] = weights_func\n",
    "\n",
    "            w = smcov.S_hac_simple(moms_, nlags=maxlag,\n",
    "                                   weights_func=weights_func)\n",
    "            w /= nobs #(nobs - self.k_params)\n",
    "\n",
    "        elif weights_method == 'iid':\n",
    "            # only when we have instruments and residual mom = Z * u\n",
    "            # TODO: problem we do not have params in argument\n",
    "            #       I cannot keep everything in here w/o params as argument\n",
    "            u = self.get_error(params)\n",
    "\n",
    "            if centered:\n",
    "                # Note: I'm not centering instruments,\n",
    "                #    should not we always center u? Ok, with centered as default\n",
    "                u -= u.mean(0)  #demean inplace, we do not need original u\n",
    "\n",
    "            instrument = self.instrument\n",
    "            w = np.dot(instrument.T, instrument).dot(np.dot(u.T, u)) / nobs\n",
    "            if 'ddof' in wargs:\n",
    "                # caller requests degrees of freedom correction\n",
    "                if wargs['ddof'] == 'k_params':\n",
    "                    w /= (nobs - self.k_params)\n",
    "                else:\n",
    "                    # assume ddof is a number\n",
    "                    w /= (nobs - wargs['ddof'])\n",
    "            else:\n",
    "                # default: divide by nobs\n",
    "                w /= nobs\n",
    "\n",
    "        else:\n",
    "            raise ValueError('weight method not available')\n",
    "\n",
    "        return w\n",
    "\n",
    "def gradient_momcond(momcond_mean, params, epsilon=1e-4, centered=True):\n",
    "\n",
    "        momcond = momcond_mean\n",
    "\n",
    "        # TODO: approx_fprime has centered keyword\n",
    "        if centered:\n",
    "            gradmoms = (approx_fprime(params, momcond, epsilon=epsilon) +\n",
    "                    approx_fprime(params, momcond, epsilon=-epsilon))/2\n",
    "        else:\n",
    "            gradmoms = approx_fprime(params, momcond, epsilon=epsilon)\n",
    "\n",
    "        return gradmoms\n",
    "\n",
    "\n",
    "omegahat = calc_weightmatrix(momcond(params,exog_macro_factors,excessRet,mimick), weights_method='cov', wargs=(),\n",
    "                          params=params)\n",
    "\n",
    "# gradmoms = gradient_momcond(momcond_mean(params,exog_macro_factors,excessRet,mimick),params, epsilon=1e-4, centered=True)\n",
    "gradmoms = approx_fprime(params, momcond_mean, kwargs={'exog':exog_macro_factors,'endog':excessRet,'instrument':mimick}, epsilon=1e-4)\n",
    "\n",
    "# cov = np.linalg.inv(np.dot(gradmoms.T,\n",
    "#                     np.dot(np.linalg.inv(omegahat), gradmoms)))\n",
    "\n",
    "cov = (np.dot(gradmoms.T,\n",
    "                    np.dot(np.linalg.inv(omegahat), gradmoms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covparams = lambeta_fit.model.momcond(lambeta_fit.params)\n",
    "covgradmoms = lambeta_fit.model.gradient_momcond(params=lambeta_fit.params, epsilon=1e-4, centered=True)\n",
    "\n",
    "lambeta_fit.calc_cov_params(covparams,\n",
    "                            covgradmoms,\n",
    "                            has_optimal_weights=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "index = 1\n",
    "\n",
    "plots = \"plots/\"\n",
    "\n",
    "macro_factors = ['MYP', 'UI', 'DSV', 'ATS', 'STS', 'FX']\n",
    "portfolios = ['BM deciles', 'Size deciles', 'Momentum deciles','Profit deciles','Invest deciles','Accruals deciles']\n",
    "# Create a figure with subplots for each column\n",
    "fig, axs = plt.subplots(1, 6, figsize=(40, 5))\n",
    "        # Generate random data for example\n",
    "coefficients = lambeta_fit.params[21:-6].reshape((10,6))\n",
    "        # coefficients = np.arange(1, beta.shape[0]+1)\n",
    "t_stats = lambeta_fit.tvalues[21:-6].reshape((10,6))\n",
    "        # t_stats = np.arange(1, beta_t_stat.shape[0]+1)\n",
    "\n",
    "        # Loop through each column and plot the histogram and line plot\n",
    "for index2, l in enumerate(range(coefficients.shape[1])):\n",
    "            # Plot histogram of coefficients on left y-axis\n",
    "            axs[l].bar(np.arange(1, coefficients.shape[0]+1), coefficients[:, l], color='blue')\n",
    "            axs[l].set_ylabel('Coefficients', color='blue', fontsize=15)\n",
    "            axs[l].tick_params(axis='y', labelcolor='blue', labelsize=15)\n",
    "            \n",
    "            # Add line plot of t-stats on right y-axis\n",
    "            axs2 = axs[l].twinx()\n",
    "            axs2.plot(np.arange(1, t_stats.shape[0]+1), t_stats[:, l], color='red', linewidth=3)\n",
    "            axs2.set_ylabel('T-Stats', color='red', fontsize=15)\n",
    "            axs2.tick_params(axis='y', labelcolor='red', labelsize=15)\n",
    "            \n",
    "            # Set x-axis label and tick marks\n",
    "            axs[l].set_xlabel(portfolios[index], fontsize=25)\n",
    "            axs[l].set_xticks(np.arange(1, coefficients.shape[0]+1))\n",
    "            axs[l].tick_params(axis='x', labelsize=15)\n",
    "            axs[l].set_title(macro_factors[index2], fontsize=35)\n",
    "\n",
    "        # Increase space between subplots\n",
    "fig.subplots_adjust(wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, port in enumerate(['bm','size','mom','op','inv','ac']):\n",
    "    # for index, port in enumerate(['inv']):\n",
    "\n",
    "        ##############################################\n",
    "        ######### Prepare the data ###################\n",
    "        # ##############################################\n",
    "        # port = 'inv'\n",
    "        # # Merge with the portfolio return data\n",
    "        # ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "        # ts4 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "        # # Set the start and end dates of the analysis period\n",
    "        # ts4 = ts4.loc[(ts4.index >= sdate[i]) & (ts4.index <= edate[i])]\n",
    "\n",
    "        # mimick = ts4[mimick_colnames].values\n",
    "        # T,K = mimick.shape\n",
    "        # exog_macro_factors = ts4[macro_exogcolnames].values\n",
    "        # T,M = exog_macro_factors.shape\n",
    "        # myp = ts4[[f'myp{i}']].values\n",
    "        # riskfree = ts4['rf'].values\n",
    "        # portfolios = ts4[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "        # T,P = portfolios.shape\n",
    "        # excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "\n",
    "        # ##############################################\n",
    "        # ######### First stage ########################\n",
    "        # ##############################################\n",
    "\n",
    "        # # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "        # betas = []\n",
    "        # for j in range(P):\n",
    "        #     res = sm.OLS(excessRet[:,j],sm.add_constant(column_stack((myp, exog_macro_factors)))).fit()\n",
    "        #     betas.append(res.params[1:])\n",
    "        # avgReturn = excessRet.mean(axis=0)\n",
    "        # avgReturn.shape = P,1\n",
    "        # betas = array(betas)\n",
    "        # res = sm.OLS(avgReturn, betas).fit()\n",
    "        # riskPremia = res.params\n",
    "        # riskPremia.shape = M+1\n",
    "        # startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "        # # Estimation    \n",
    "        # step1lambeta_mod = step1_lambeta(endog=excessRet, exog=column_stack((myp, exog_macro_factors)), instrument=None)\n",
    "        # step1lambeta_fit = step1lambeta_mod.fit(start_params=startingVals, \n",
    "        #                                         maxiter=1, \n",
    "        #                                         inv_weights=eye(P*(M+2)),\n",
    "        #                                         weights_method='hac', \n",
    "        #                                         wargs={'maxlag':12}, \n",
    "        #                                         optim_method='bfgs', \n",
    "        #                                         optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "        # print(step1lambeta_fit.summary())\n",
    "\n",
    "        ##############################################\n",
    "        ######### Second stage #######################\n",
    "        ##############################################\n",
    "\n",
    "        # Compute the optimal weights from the first stage\n",
    "        step1optW = step1lambeta_fit.model.calc_weightmatrix(step1lambeta_fit.model.momcond(step1lambeta_fit.params),\n",
    "                                                        weights_method='hac',\n",
    "                                                        wargs={'maxlag':12},\n",
    "                                                        params=step1lambeta_fit.params)\n",
    "\n",
    "        # Compute the weights for the second stage \n",
    "        # (force parameters to be equal to parameters from estimating mimicking portfolio and gmm separately)\n",
    "        G = gmm_G(step1lambeta_fit.params*10000, excessRet*10000, column_stack((myp, exog_macro_factors))*10000)\n",
    "        D = add_mimick_deriv(G, mimick[:,1:]*10000)\n",
    "        npar = G.shape[0]\n",
    "        A = zeros((K+npar, K+P*(M+2)))\n",
    "        A[:K, :K] = eye(K) * 10000\n",
    "        A[-npar:, -P*(M+2):] = G @ (step1optW * 10000)\n",
    "        # W1 = eye(K+P*(M+2)) * 1000 - D @ inv(A @ D) @ A\n",
    "        # S = zeros((K+P*(M+2), K+P*(M+2)))\n",
    "        # S[:K, :K] = eye(K) * 1000\n",
    "        # S[K:, K:] = step1optW * 1000\n",
    "        # Winv = W1 @ S @ W1.T \n",
    "        Winv = A.T @ A\n",
    "\n",
    "        # Starting value \n",
    "        output1_array = np.array(mimick_list[i].params)\n",
    "        output1_array_reshaped = output1_array.reshape(1, -1)\n",
    "        output2_array = np.array(step1lambeta_fit.params)\n",
    "        output2_array_reshaped = output2_array.reshape(1, -1)\n",
    "        startVals = np.concatenate([output1_array_reshaped, output2_array_reshaped], axis=1)\n",
    "\n",
    "        # Estimation    \n",
    "        lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "        lambeta_fit = lambeta_mod.fit(start_params=startVals, \n",
    "                                    maxiter=1, \n",
    "                                    inv_weights=Winv, \n",
    "                                    #   weights_method='hac', \n",
    "                                    #   wargs={'maxlag':12}, \n",
    "                                    optim_method='bfgs', \n",
    "                                    optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "        # print(lambeta_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# port = 'bm'\n",
    "# ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "# ts4 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "# # Set the start and end dates of the analysis period\n",
    "# ts4 = ts4.loc[(ts4.index >= start_date) & (ts4.index <= end_date)]\n",
    "\n",
    "# mimick = ts4[['log_indprod_growth_nextyear',\n",
    "#                 'ex_mkt',\n",
    "#                    'ex_b10ret',\n",
    "#                    'ex_b5ret',\n",
    "#                 #    'ex_high_yd_bd_ret',\n",
    "#                    'ex_gold_ret', \n",
    "#                    'slope_ex_mkt_87', \n",
    "#                    'slope_ex_mkt_9602', \n",
    "#                    'rf', \n",
    "#                    'lag_10y_3m_gov_bd_yd',\n",
    "#                    'lag_1y_3m_gov_bd_yd',\n",
    "#                    'lag_Baa_Aaa_bd_yd',\n",
    "#                    'lag_sp_div_yd',\n",
    "#                    'log_indprod_growth_lastyear',\n",
    "#                    'infl_lastyear',\n",
    "#                    'ex_mkt_lastyear',\n",
    "#                    'lag_ex_mkt',\n",
    "#                    'lag_ex_b10ret',\n",
    "#                    'lag_ex_b5ret',\n",
    "#                    'lag_ex_gold_ret',\n",
    "#                    'lag_slope_ex_mkt_87',\n",
    "#                    'lag_slope_ex_mkt_9602'\n",
    "#                    ]].values\n",
    "# T,K = mimick.shape\n",
    "# exog_macro_factors = ts4[['ui','dsv','ats','sts','fx']].values\n",
    "# T,M = exog_macro_factors.shape\n",
    "# myp = ts4[['myp']].values\n",
    "# riskfree = ts4['rf'].values\n",
    "# portfolios = ts4[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "# T,P = portfolios.shape\n",
    "# excessRet = portfolios - reshape(riskfree,(T,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class step1_lambeta(GMM):\n",
    "#     def momcond(self, params):\n",
    "#         fRets = self.exog\n",
    "#         pRets = self.endog\n",
    "#         coeff = squeeze(array(params)) \n",
    "\n",
    "#         T,P = pRets.shape\n",
    "#         T,M = fRets.shape\n",
    "#         # var_coeff = reshape(var_coeff,(N,M))\n",
    "#         # print(P, M)\n",
    "#         beta = squeeze(array(coeff[:(P*M)]))\n",
    "#         lam = squeeze(array(coeff[(P*M):]))\n",
    "#         beta = reshape(beta,(P,M))\n",
    "#         lam = reshape(lam,(M,1))\n",
    "#         betalam = beta @ lam\n",
    "#         expectedRet = fRets @ beta.T\n",
    "#         e = pRets - expectedRet\n",
    "#         # print(M, P)\n",
    "#         moments_beta = kron(e,ones((1,M))) * tile(fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "#         moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "#         moments = hstack((moments_beta,moments_lam))\n",
    "\n",
    "#         return moments\n",
    "\n",
    "# # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "# betas = []\n",
    "# for i in range(P):\n",
    "#     res = sm.OLS(excessRet[:,i],sm.add_constant(column_stack((myp, exog_macro_factors)))).fit()\n",
    "#     betas.append(res.params[1:])\n",
    "# avgReturn = excessRet.mean(axis=0)\n",
    "# avgReturn.shape = P,1\n",
    "# betas = array(betas)\n",
    "# res = sm.OLS(avgReturn, betas).fit()\n",
    "# riskPremia = res.params\n",
    "# riskPremia.shape = M+1\n",
    "# startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "# # Estimation    \n",
    "# step1lambeta_mod = step1_lambeta(endog=excessRet, exog=column_stack((myp, exog_macro_factors)), instrument=None)\n",
    "# step1lambeta_fit = step1lambeta_mod.fit(start_params=startingVals, \n",
    "#                                         maxiter=1, \n",
    "#                                         inv_weights=eye(P*(M+2)), \n",
    "#                                         weights_method='hac', \n",
    "#                                         wargs={'maxlag':12}, \n",
    "#                                         optim_method='bfgs', \n",
    "#                                         optim_args={'gtol': 1e-15, 'maxiter': 10000})\n",
    "# step1lambeta_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optW = step1lambeta_fit.model.calc_weightmatrix(step1lambeta_fit.model.momcond(step1lambeta_fit.params),\n",
    "#                                                     weights_method='hac',\n",
    "#                                                     wargs={'maxlag':12},\n",
    "#                                                     params=step1lambeta_fit.params)\n",
    "# optW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gmm_G(params, p_rets, f_rets):\n",
    "#     t,n = p_rets.shape\n",
    "#     t,k = f_rets.shape\n",
    "#     beta = squeeze(array(params[:(n*k)]))\n",
    "#     lam = squeeze(array(params[(n*k):]))\n",
    "#     beta = reshape(beta,(n,k))\n",
    "#     lam = reshape(lam,(k,1))\n",
    "#     G = np.zeros((n*k+k,n*k+n))\n",
    "#     ffp = (f_rets.T @ f_rets) / t\n",
    "#     G[:(n*k),:(n*k)] = kron(eye(n),ffp)\n",
    "#     G[:(n*k),(n*k):] = kron(eye(n),-lam)\n",
    "#     G[(n*k):,(n*k):] = -beta.T\n",
    "#     return G\n",
    "\n",
    "# def add_mimick_deriv(G, mimick):\n",
    "#     mimick = sm.add_constant(mimick)\n",
    "#     t,k = mimick.shape\n",
    "#     i,j = G.shape\n",
    "#     D = np.zeros((k+i,k+j))\n",
    "#     D[:k,:k] = (mimick.T @ mimick) / t\n",
    "#     D[k:,k:] = G\n",
    "#     return D.T\n",
    "\n",
    "# G = gmm_G(step1lambeta_fit.params*1000, excessRet*1000, column_stack((myp, exog_macro_factors))*1000)\n",
    "# D = add_mimick_deriv(G, mimick[:,1:]*1000)\n",
    "# # G = gmm_G(step1lambeta_fit.params, excessRet, column_stack((myp, exog_macro_factors)))\n",
    "# # D = add_mimick_deriv(G, mimick[:,1:])\n",
    "# npar = G.shape[0]\n",
    "# A = zeros((K+npar, K+P*(M+2)))\n",
    "# # A[:K, :K] = eye(K) * 1000000000\n",
    "# # A[-npar:, -P*(M+2):] = G @ optW * 1000000000\n",
    "# # A[:K, :K] = eye(K)\n",
    "# A[:K, :K] = eye(K) * 1000\n",
    "# # A[-npar:, -P*(M+2):] = G @ optW * 100\n",
    "# # A[-npar:, -P*(M+2):] = G @ (optW * 1000)\n",
    "# A[-npar:, -P*(M+2):] = G @ (eye(P*(M+2)) * 1000)\n",
    "\n",
    "# W1 = eye(K+P*(M+2)) * 1000 - D @ inv(A @ D) @ A\n",
    "\n",
    "# S = zeros((K+P*(M+2), K+P*(M+2)))\n",
    "# S[:K, :K] = eye(K) * 1000\n",
    "# # S[K:, K:] = optW * 100\n",
    "# S[K:, K:] = optW * 1000\n",
    "\n",
    "# # Winv = W1 @ S @ W1.T * 100000\n",
    "# # Winv = W1 @ S @ W1.T \n",
    "\n",
    "# Winv = A.T @ A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class gmm_lambeta(GMM):\n",
    "#     def momcond(self, params):\n",
    "#         fRets = self.exog\n",
    "#         pRets = self.endog\n",
    "#         mimick = self.instrument\n",
    "#         coeff = squeeze(array(params)) \n",
    "        \n",
    "#         # first stage\n",
    "#         mimick_endog = mimick[:,0]\n",
    "#         mimick_exog = mimick[:,1:]\n",
    "#         mimick_exog = sm.add_constant(mimick_exog)\n",
    "#         T,K = mimick_exog.shape\n",
    "#         mimick_exog = reshape(mimick_exog,(T,K))\n",
    "#         mimick_endog = reshape(mimick_endog,(T,1))\n",
    "#         mimick_coeff = coeff[:K]\n",
    "#         mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "#         mimick_err = (mimick_endog - mimick_exog @ mimick_coeff)\n",
    "#         moments1 = mimick_exog * kron(mimick_err,ones((1,K)))\n",
    "\n",
    "#         # compute myp\n",
    "#         myp = mimick_exog[:, 1:7] @ mimick_coeff[1:7] \n",
    "\n",
    "#         # gmm\n",
    "#         full_fRets = column_stack((myp, fRets))\n",
    "#         T,P = pRets.shape\n",
    "#         T,M = full_fRets.shape\n",
    "#         betalam_params = params[K:]\n",
    "#         # var_coeff = reshape(var_coeff,(N,M))\n",
    "#         # print(P, M)\n",
    "#         # print(betalam_params.shape)\n",
    "#         beta = squeeze(array(betalam_params[:(P*M)]))\n",
    "#         lam = squeeze(array(betalam_params[(P*M):]))\n",
    "#         beta = reshape(beta,(P,M))\n",
    "#         lam = reshape(lam,(M,1))\n",
    "#         betalam = beta @ lam\n",
    "#         expectedRet = full_fRets @ beta.T\n",
    "#         e = pRets - expectedRet\n",
    "#         # print(M, P)\n",
    "#         moments_beta = kron(e,ones((1,M))) * tile(full_fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "#         moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "#         moments2 = hstack((moments_beta,moments_lam))\n",
    "#         # print(moments2.shape)\n",
    "\n",
    "#         return column_stack((moments1,moments2))\n",
    "    \n",
    "\n",
    "# # Starting value \n",
    "# output2_array = np.array(step1lambeta_fit.params)\n",
    "# output2_array_reshaped = output2_array.reshape(1, -1)\n",
    "# startVals = np.concatenate([output1_array_reshaped, output2_array_reshaped], axis=1)\n",
    "    \n",
    "# lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "# lambeta_fit = lambeta_mod.fit(start_params=startVals, \n",
    "#                               maxiter=1, \n",
    "#                               inv_weights=Winv, \n",
    "#                             #   weights_method='hac', \n",
    "#                             #   wargs={'maxlag':12}, \n",
    "#                               optim_method='bfgs', \n",
    "#                               optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "# lambeta_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tabprintin.beautify import *\n",
    "# import matplotlib.pyplot as plt\n",
    "# from varname import nameof\n",
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# macro_factors = ['MYP', 'UI', 'DSV', 'ATS', 'STS', 'FX']\n",
    "# portfolios = ['BM deciles', 'Size deciles', 'Momentum deciles']\n",
    "\n",
    "# # Create a figure with subplots for each column\n",
    "# fig, axs = plt.subplots(1, 6, figsize=(40, 5))\n",
    "# # Generate random data for example\n",
    "# coefficients = reshape(lambeta_fit.params[21:-6],(10,6))\n",
    "#     # coefficients = np.arange(1, beta.shape[0]+1)\n",
    "# t_stats = reshape(lambeta_fit.tvalues[21:-6],(10,6))\n",
    "#     # t_stats = np.arange(1, beta_t_stat.shape[0]+1)\n",
    "\n",
    "#     # Loop through each column and plot the histogram and line plot\n",
    "# for index2, i in enumerate(range(coefficients.shape[1])):\n",
    "#         # Plot histogram of coefficients on left y-axis\n",
    "#         axs[i].bar(np.arange(1, coefficients.shape[0]+1), coefficients[:, i], color='blue')\n",
    "#         axs[i].set_ylabel('Coefficients', color='blue', fontsize=15)\n",
    "#         axs[i].tick_params(axis='y', labelcolor='blue', labelsize=15)\n",
    "        \n",
    "#         # Add line plot of t-stats on right y-axis\n",
    "#         axs2 = axs[i].twinx()\n",
    "#         axs2.plot(np.arange(1, t_stats.shape[0]+1), t_stats[:, i], color='red', linewidth=3)\n",
    "#         axs2.set_ylabel('T-Stats', color='red', fontsize=15)\n",
    "#         axs2.tick_params(axis='y', labelcolor='red', labelsize=15)\n",
    "        \n",
    "#         # Set x-axis label and tick marks\n",
    "#         axs[i].set_xlabel(portfolios[index], fontsize=25)\n",
    "#         axs[i].set_xticks(np.arange(1, coefficients.shape[0]+1))\n",
    "#         axs[i].tick_params(axis='x', labelsize=15)\n",
    "#         axs[i].set_title(macro_factors[index2], fontsize=35)\n",
    "    \n",
    "#     # Increase space between subplots\n",
    "# fig.subplots_adjust(wspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
