{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8130ac7-479c-4e2a-b49d-960773ada07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"../../data/generated/\"\n",
    "results = \"../../data/generated/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69dba9df-277d-4c64-8e8c-be8345b7e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linearmodels.panel import PanelOLS, compare\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from gmm import *\n",
    "from linearmodels.asset_pricing import LinearFactorModelGMM\n",
    "from tabprintin.beautify import *\n",
    "from statsmodels.sandbox.regression.gmm import GMM\n",
    "from numpy import hstack, zeros, ones, array, mat, tile, reshape, squeeze, eye, asmatrix, column_stack, roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb3f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start and end dates of the analysis period\n",
    "# start_date = pd.to_datetime('1978-01-01')\n",
    "start_date = pd.to_datetime('1975-01-01')\n",
    "end_date = pd.to_datetime('2008-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(resource + 'time_series.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts.index.freq = 'M'\n",
    "\n",
    "# Compute the log change of industrial production over next 12 months (or just growth)\n",
    "# ts['log_indprod_growth_nextyear'] = np.log(ts['ind_prod'].shift(-12) / ts['ind_prod'].shift(-1)) #wrong\n",
    "ts['log_indprod_growth_nextyear'] = np.log(ts['ind_prod'].shift(-12) / ts['ind_prod'])\n",
    "# ts['indprod_growth_nextyear'] = ts['ind_prod'].shift(-12) / ts['ind_prod'].shift(-1) - 1\n",
    "\n",
    "##################\n",
    "## Base assets ###\n",
    "##################\n",
    "\n",
    "# Market return\n",
    "# ts['ex_mkt'] = ts['ex_mkt'] /100\n",
    "ts['lag_ex_mkt'] = ts['ex_mkt'].shift(1)\n",
    "\n",
    "# Compute the excess return of the long-term government bond portfolio (bloomberg data)\n",
    "# ts['ex_long_gov_ret'] = ts['long_gov_ret'] - ts['rf']\n",
    "# ts['lag_ex_long_gov_ret'] = ts['ex_long_gov_ret'].shift(1)\n",
    "\n",
    "# Compute the excess return of the 10 year treasury government bond\n",
    "ts['ex_b10ret'] = ts['b10ret'] - ts['rf']\n",
    "ts['lag_ex_b10ret'] = ts['ex_b10ret'].shift(1)\n",
    "\n",
    "# Compute the excess return of the intermediate-term government bond portfolio (bloomberg data)\n",
    "# ts['ex_medium_gov_ret'] = ts['medium_gov_ret'] - ts['rf']\n",
    "# ts['lag_ex_medium_gov_ret'] = ts['ex_medium_gov_ret'].shift(1)\n",
    "\n",
    "# Compute the excess return of the 5 year treasury government bond\n",
    "ts['ex_b5ret'] = ts['b5ret'] - ts['rf']\n",
    "ts['lag_ex_b5ret'] = ts['ex_b5ret'].shift(1)\n",
    "\n",
    "# Compute the excess return of the high-yield bond portfolio\n",
    "ts['ex_high_yd_bd_ret'] = ts['high_yd_bd_ret'] - ts['rf']\n",
    "ts['lag_ex_high_yd_bd_ret'] = ts['ex_high_yd_bd_ret'].shift(1)\n",
    "\n",
    "# Compute the return for gold index\n",
    "ts['ex_gold_ret'] = ts['gold'].pct_change() - ts['rf']\n",
    "ts['lag_ex_gold_ret'] = ts['ex_gold_ret'].shift(1)\n",
    "\n",
    "# Create dummies for 1987 (stock market crash) and 1996-2002 (Internet bubble period)\n",
    "ts['dummy_87'] = (ts.index.year == 1987).astype(int)\n",
    "ts['dummy_96_02'] = ((ts.index.year >= 1996) & (ts.index.year <= 2002)).astype(int)\n",
    "\n",
    "#########################\n",
    "### Control Variables ###\n",
    "#########################\n",
    "# Compute the 10 year minus 3 month government bond yield\n",
    "ts['lag_10y_3m_gov_bd_yd'] = (ts['DGS10'] - ts['DTB3']).shift(1)\n",
    "\n",
    "# Compute the 1 year minus 3 month government bond yield\n",
    "ts['lag_1y_3m_gov_bd_yd'] = (ts['DGS1'] - ts['DTB3']).shift(1)\n",
    "\n",
    "# Baa minus Aaa corporate bond yield\n",
    "ts['lag_Baa_Aaa_bd_yd'] = (ts['BAA'] - ts['AAA']).shift(1)\n",
    "\n",
    "# Compute the dividend yield on the S&P 500 index\n",
    "ts['lag_sp_div_yd'] = (ts['sp_div'] / ts['sp_price']).shift(1)\n",
    "\n",
    "# Compute the log change of industrial production over last 12 months (or just growth)\n",
    "ts['log_indprod_growth_lastyear'] = np.log(ts['ind_prod'].shift(13) / ts['ind_prod'].shift(1))\n",
    "# ts['indprod_growth_lastyear'] = ts['ind_prod'].shift(13) / ts['ind_prod'].shift(1)  - 1\n",
    "\n",
    "# Compute the inflation over last 12 months\n",
    "# ts['infl_lastyear'] = (ts['cpi'].shift(1) - ts['cpi'].shift(13)) / ts['cpi'].shift(13)\n",
    "ts['infl_lastyear'] = np.log(ts['cpi'].shift(1) / ts['cpi'].shift(13))\n",
    "\n",
    "# Compute the market portfolio excess return over last 12 months\n",
    "# [Controllare e sbagliato]\n",
    "ts['ex_mkt_lastyear'] = (((ts['ex_mkt'] + 100)/100).rolling(13).apply(lambda x: x[:-1].prod()) - 1) * 100\n",
    "\n",
    "# Interactions\n",
    "ts['slope_ex_mkt_87'] = ts['ex_mkt'] * ts['dummy_87']\n",
    "ts['slope_ex_mkt_9602'] = ts['ex_mkt'] * ts['dummy_96_02']\n",
    "\n",
    "ts['lag_slope_ex_mkt_87'] = ts['slope_ex_mkt_87'].shift(1)\n",
    "ts['lag_slope_ex_mkt_9602'] = ts['slope_ex_mkt_9602'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f7ae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mimick1' (RegressionResultsWrapper)\n",
      "Stored 'mimick2' (RegressionResultsWrapper)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>log_indprod_growth_nextyear</td> <th>  R-squared:         </th> <td>   0.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                        <td>OLS</td>             <th>  Adj. R-squared:    </th> <td>   0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>Least Squares</td>        <th>  F-statistic:       </th> <td>   8.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                  <td>Wed, 10 May 2023</td>       <th>  Prob (F-statistic):</th> <td>5.67e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                      <td>17:42:50</td>           <th>  Log-Likelihood:    </th> <td>  817.04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>           <td>   400</td>            <th>  AIC:               </th> <td>  -1592.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>               <td>   379</td>            <th>  BIC:               </th> <td>  -1508.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>                   <td>    20</td>            <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>              <td>HAC</td>             <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                   <td>    0.0263</td> <td>    0.015</td> <td>    1.703</td> <td> 0.089</td> <td>   -0.004</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_mkt</th>                      <td>    0.1306</td> <td>    0.086</td> <td>    1.515</td> <td> 0.130</td> <td>   -0.038</td> <td>    0.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_b10ret</th>                   <td>   -0.4243</td> <td>    0.188</td> <td>   -2.262</td> <td> 0.024</td> <td>   -0.792</td> <td>   -0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_b5ret</th>                    <td>    0.4648</td> <td>    0.226</td> <td>    2.057</td> <td> 0.040</td> <td>    0.022</td> <td>    0.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_gold_ret</th>                 <td>   -0.0481</td> <td>    0.028</td> <td>   -1.699</td> <td> 0.089</td> <td>   -0.104</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>slope_ex_mkt_87</th>             <td>   -0.0809</td> <td>    0.084</td> <td>   -0.968</td> <td> 0.333</td> <td>   -0.245</td> <td>    0.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>slope_ex_mkt_9602</th>           <td>    0.0098</td> <td>    0.104</td> <td>    0.094</td> <td> 0.925</td> <td>   -0.194</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rf</th>                          <td>   -9.4570</td> <td>    4.197</td> <td>   -2.253</td> <td> 0.024</td> <td>  -17.683</td> <td>   -1.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_10y_3m_gov_bd_yd</th>        <td>   -0.1142</td> <td>    0.543</td> <td>   -0.210</td> <td> 0.833</td> <td>   -1.178</td> <td>    0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_1y_3m_gov_bd_yd</th>         <td>    1.6497</td> <td>    0.723</td> <td>    2.282</td> <td> 0.022</td> <td>    0.233</td> <td>    3.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_Baa_Aaa_bd_yd</th>           <td>   -0.2982</td> <td>    1.616</td> <td>   -0.185</td> <td> 0.854</td> <td>   -3.465</td> <td>    2.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_sp_div_yd</th>               <td>    1.7956</td> <td>    0.564</td> <td>    3.184</td> <td> 0.001</td> <td>    0.690</td> <td>    2.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_indprod_growth_lastyear</th> <td>    0.1367</td> <td>    0.107</td> <td>    1.275</td> <td> 0.202</td> <td>   -0.074</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>infl_lastyear</th>               <td>   -0.5101</td> <td>    0.226</td> <td>   -2.255</td> <td> 0.024</td> <td>   -0.953</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ex_mkt_lastyear</th>             <td>    0.1010</td> <td>    0.018</td> <td>    5.589</td> <td> 0.000</td> <td>    0.066</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_ex_mkt</th>                  <td>    0.1197</td> <td>    0.091</td> <td>    1.321</td> <td> 0.187</td> <td>   -0.058</td> <td>    0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_ex_b10ret</th>               <td>   -0.1474</td> <td>    0.131</td> <td>   -1.124</td> <td> 0.261</td> <td>   -0.405</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_ex_b5ret</th>                <td>   -0.0241</td> <td>    0.200</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.416</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_ex_gold_ret</th>             <td>   -0.0573</td> <td>    0.026</td> <td>   -2.242</td> <td> 0.025</td> <td>   -0.107</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_slope_ex_mkt_87</th>         <td>   -0.1133</td> <td>    0.107</td> <td>   -1.056</td> <td> 0.291</td> <td>   -0.323</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lag_slope_ex_mkt_9602</th>       <td>   -0.1132</td> <td>    0.126</td> <td>   -0.902</td> <td> 0.367</td> <td>   -0.359</td> <td>    0.133</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>148.221</td> <th>  Durbin-Watson:     </th> <td>   0.233</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 737.201</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.521</td>  <th>  Prob(JB):          </th> <td>8.29e-161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.914</td>  <th>  Cond. No.          </th> <td>1.07e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 11 lags and without small sample correction<br/>[2] The condition number is large, 1.07e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:     log_indprod_growth_nextyear   R-squared:                       0.420\n",
       "Model:                                     OLS   Adj. R-squared:                  0.390\n",
       "Method:                          Least Squares   F-statistic:                     8.595\n",
       "Date:                         Wed, 10 May 2023   Prob (F-statistic):           5.67e-21\n",
       "Time:                                 17:42:50   Log-Likelihood:                 817.04\n",
       "No. Observations:                          400   AIC:                            -1592.\n",
       "Df Residuals:                              379   BIC:                            -1508.\n",
       "Df Model:                                   20                                         \n",
       "Covariance Type:                           HAC                                         \n",
       "===============================================================================================\n",
       "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "Intercept                       0.0263      0.015      1.703      0.089      -0.004       0.057\n",
       "ex_mkt                          0.1306      0.086      1.515      0.130      -0.038       0.300\n",
       "ex_b10ret                      -0.4243      0.188     -2.262      0.024      -0.792      -0.057\n",
       "ex_b5ret                        0.4648      0.226      2.057      0.040       0.022       0.908\n",
       "ex_gold_ret                    -0.0481      0.028     -1.699      0.089      -0.104       0.007\n",
       "slope_ex_mkt_87                -0.0809      0.084     -0.968      0.333      -0.245       0.083\n",
       "slope_ex_mkt_9602               0.0098      0.104      0.094      0.925      -0.194       0.214\n",
       "rf                             -9.4570      4.197     -2.253      0.024     -17.683      -1.231\n",
       "lag_10y_3m_gov_bd_yd           -0.1142      0.543     -0.210      0.833      -1.178       0.950\n",
       "lag_1y_3m_gov_bd_yd             1.6497      0.723      2.282      0.022       0.233       3.066\n",
       "lag_Baa_Aaa_bd_yd              -0.2982      1.616     -0.185      0.854      -3.465       2.869\n",
       "lag_sp_div_yd                   1.7956      0.564      3.184      0.001       0.690       2.901\n",
       "log_indprod_growth_lastyear     0.1367      0.107      1.275      0.202      -0.074       0.347\n",
       "infl_lastyear                  -0.5101      0.226     -2.255      0.024      -0.953      -0.067\n",
       "ex_mkt_lastyear                 0.1010      0.018      5.589      0.000       0.066       0.136\n",
       "lag_ex_mkt                      0.1197      0.091      1.321      0.187      -0.058       0.297\n",
       "lag_ex_b10ret                  -0.1474      0.131     -1.124      0.261      -0.405       0.110\n",
       "lag_ex_b5ret                   -0.0241      0.200     -0.120      0.904      -0.416       0.368\n",
       "lag_ex_gold_ret                -0.0573      0.026     -2.242      0.025      -0.107      -0.007\n",
       "lag_slope_ex_mkt_87            -0.1133      0.107     -1.056      0.291      -0.323       0.097\n",
       "lag_slope_ex_mkt_9602          -0.1132      0.126     -0.902      0.367      -0.359       0.133\n",
       "==============================================================================\n",
       "Omnibus:                      148.221   Durbin-Watson:                   0.233\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              737.201\n",
       "Skew:                          -1.521   Prob(JB):                    8.29e-161\n",
       "Kurtosis:                       8.914   Cond. No.                     1.07e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 11 lags and without small sample correction\n",
       "[2] The condition number is large, 1.07e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set analysis period\n",
    "ts1 = ts.loc[(ts.index >= start_date) & (ts.index <= end_date)]\n",
    "# ts1 = ts.loc[(ts.index >= pd.to_datetime('1975-02-01')) & (ts.index <= end_date)]\n",
    "\n",
    "# with Bloomberg bond portfolio return data\n",
    "# mod1 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_long_gov_ret+ex_medium_gov_ret+ex_high_yd_bd_ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear',\n",
    "#                 data=ts1)\n",
    "\n",
    "# mod2 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_long_gov_ret+ex_medium_gov_ret+ex_high_yd_bd_ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear+ lag_ex_mkt+lag_ex_long_gov_ret+lag_ex_medium_gov_ret+lag_ex_high_yd_bd_ret+lag_ex_gold_ret+lag_slope_ex_mkt_87+lag_slope_ex_mkt_9602',\n",
    "#                 data=ts1)\n",
    "\n",
    "# mod1 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_b10ret+ex_b5ret+ex_high_yd_bd_ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear',\n",
    "#                 data=ts1)\n",
    "\n",
    "# mod2 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_b10ret+ex_b5ret+ex_high_yd_bd_ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear+ lag_ex_mkt+lag_ex_b10ret+lag_ex_b5ret+lag_ex_high_yd_bd_ret+lag_ex_gold_ret+lag_slope_ex_mkt_87+lag_slope_ex_mkt_9602',\n",
    "#                 data=ts1)\n",
    "\n",
    "mod1 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_b10ret+ex_b5ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear',\n",
    "                data=ts1)\n",
    "\n",
    "mod2 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_b10ret+ex_b5ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear+ lag_ex_mkt+lag_ex_b10ret+lag_ex_b5ret+lag_ex_gold_ret+lag_slope_ex_mkt_87+lag_slope_ex_mkt_9602',\n",
    "                data=ts1)\n",
    "\n",
    "mimick1 = mod1.fit(cov_type='HAC',cov_kwds={'maxlags':11})\n",
    "mimick2 = mod2.fit(cov_type='HAC',cov_kwds={'maxlags':11})\n",
    "\n",
    "%store mimick1\n",
    "%store mimick2\n",
    "mimick2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e530c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class mimick(GMM):\n",
    "#     def momcond(self, params):\n",
    "#         x = self.exog\n",
    "#         y = self.endog\n",
    "#         T,K = x.shape\n",
    "#         y = reshape(y,(T,1))\n",
    "#         b = params\n",
    "#         b = squeeze(array(params)) \n",
    "#         b = reshape(b,(K,1))\n",
    "#         err = (y - x @ b)\n",
    "#         moments = x * kron(err,ones((1,K)))\n",
    "#         return moments\n",
    "\n",
    "# # gmm_mod = mimick(endog=log_indprod_growth_nextyear, exog=macro_factors, instrument=None)\n",
    "# # gmm_fit8 = gmm_mod.fit(start_params=zeros(23), maxiter=1, inv_weights=eye(N), weights_method='hac', wargs={'maxlag':11}, optim_method='bfgs', optim_args={'gtol': 1e-10, 'maxiter': 1000})\n",
    "# # gmm_fit8.summary()\n",
    "\n",
    "# mimick_exog = ts1[['ex_mkt',\n",
    "#                    'ex_b10ret',\n",
    "#                    'ex_b5ret',\n",
    "#                 #    'ex_high_yd_bd_ret',\n",
    "#                    'ex_gold_ret', \n",
    "#                    'slope_ex_mkt_87', \n",
    "#                    'slope_ex_mkt_9602', \n",
    "#                    'rf', \n",
    "#                    'lag_10y_3m_gov_bd_yd',\n",
    "#                    'lag_1y_3m_gov_bd_yd',\n",
    "#                    'lag_Baa_Aaa_bd_yd',\n",
    "#                    'lag_sp_div_yd',\n",
    "#                    'log_indprod_growth_lastyear',\n",
    "#                    'infl_lastyear',\n",
    "#                    'ex_mkt_lastyear',\n",
    "#                    'lag_ex_mkt',\n",
    "#                    'lag_ex_b10ret',\n",
    "#                    'lag_ex_b5ret',\n",
    "#                    'lag_ex_gold_ret',\n",
    "#                    'lag_slope_ex_mkt_87',\n",
    "#                    'lag_slope_ex_mkt_9602'\n",
    "#                    ]]\n",
    "# mimick_exog = sm.add_constant(mimick_exog)\n",
    "# mimick_endog = ts1['log_indprod_growth_nextyear']\n",
    "# T,K = mimick_exog.shape\n",
    "\n",
    "\n",
    "# mimick_mod = mimick(endog=mimick_endog, exog=mimick_exog, instrument=None)\n",
    "# mimick_fit = mimick_mod.fit(start_params=np.zeros(K), maxiter=1, inv_weights=np.eye(K), weights_method='hac', wargs={'maxlag':11}, optim_method='bfgs', optim_args={'gtol': 1e-12, 'maxiter': 1000})\n",
    "# # gmm_fit8 = gmm_mod.fit(start_params=np.zeros(K), maxiter=1, inv_weights=np.eye(K), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 10000})\n",
    "# mimick_fit.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acbbb9e5",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9c2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the one-year ahead industrial production growth expectatitions factor\n",
    "coef = mimick2.params\n",
    "# ts['myp'] = coef['ex_mkt'] * ts['ex_mkt'] + coef['ex_long_gov_ret'] * ts['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * ts['ex_medium_gov_ret'] + coef['ex_high_yd_bd_ret'] * ts['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * ts['ex_gold_ret'] + coef['slope_ex_mkt_87'] * ts['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * ts['slope_ex_mkt_9602']\n",
    "ts['myp'] = coef['ex_mkt'] * ts['ex_mkt'] + coef['ex_b10ret'] * ts['ex_b10ret'] + coef['ex_b5ret'] * ts['ex_b5ret'] + coef['ex_gold_ret'] * ts['ex_gold_ret'] + coef['slope_ex_mkt_87'] * ts['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * ts['slope_ex_mkt_9602']\n",
    "\n",
    "# def compute_myp(row):\n",
    "#     if pd.isna(row['ex_high_yd_bd_ret']):\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_long_gov_ret'] * row['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * row['ex_medium_gov_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "#     else:\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_long_gov_ret'] * row['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * row['ex_medium_gov_ret'] + coef['ex_high_yd_bd_ret'] * row['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "    \n",
    "# def compute_myp(row):\n",
    "#     if pd.isna(row['ex_high_yd_bd_ret']):\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_b10ret'] * row['ex_b10ret'] + coef['ex_b5ret'] * row['ex_b5ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "#     else:\n",
    "#         return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_b10ret'] * row['ex_b10ret'] + coef['ex_b5ret'] * row['ex_b5ret'] + coef['ex_high_yd_bd_ret'] * row['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "\n",
    "# ts['myp'] = ts.apply(compute_myp, axis=1)\n",
    "\n",
    "# Get the unexpected inflation factor\n",
    "ts['infl'] = np.log(ts['cpi'] / ts['cpi'].shift(1))\n",
    "ts['delta_infl'] = ts['infl'] - ts['infl'].shift(1)\n",
    "inf_ma1 = sm.tsa.arima.ARIMA(ts['delta_infl'], order=(0,0,1)).fit()\n",
    "ts['fit_delta_infl'] = inf_ma1.fittedvalues\n",
    "ts['ui'] = ts['delta_infl'] - ts['fit_delta_infl']\n",
    "\n",
    "# Get the change in the aggregate survival probability factor (with my dsv)\n",
    "mod3 = smf.ols('v_dsv ~ 1 + my_dsv', data=ts).fit()\n",
    "parm = mod3.params\n",
    "ts['fit_dsv'] = parm['Intercept'] + parm['my_dsv'] * ts['my_dsv']\n",
    "ts['dsv'] = np.where(ts['v_dsv'].notna(), ts['v_dsv'], ts['fit_dsv'])\n",
    "\n",
    "# Get the change in the aggregate survival probability factor (with dsv from CRI)\n",
    "# mod3 = smf.ols('v_dsv ~ 1 + cri_dsv', data=ts).fit()\n",
    "# parm = mod3.params\n",
    "# ts['fit_dsv'] = parm['Intercept'] + parm['cri_dsv'] * ts['cri_dsv']\n",
    "# ts['dsv'] = np.where(ts['v_dsv'].notna(), ts['v_dsv'], ts['fit_dsv'])\n",
    "\n",
    "# Get the change in the average level of the term structure factor\n",
    "ts['mean_term_structure'] = ts[['DTB3', 'DGS10']].mean(axis=1)\n",
    "ts['ats'] = ts['mean_term_structure'] - ts['mean_term_structure'].shift(1)\n",
    "\n",
    "# Get the change in the slope of the term strucutre factor\n",
    "ts['diff_term_structure'] = ts['DGS10'] - ts['DTB3']\n",
    "ts['sts'] = ts['diff_term_structure'] - ts['diff_term_structure'].shift(1)\n",
    "\n",
    "# Get the change in the multilateral US dollar exchange rate factor\n",
    "mod4 = smf.ols('TWEXM ~ 1 + DTWEXAFEGS', data=ts).fit()\n",
    "parm = mod4.params\n",
    "ts['fit_TWEXM'] = parm['Intercept'] + parm['DTWEXAFEGS'] * ts['DTWEXAFEGS']\n",
    "ts['exchange_rate'] = np.where(ts['TWEXM'].notna(), ts['TWEXM'], ts['fit_TWEXM'])\n",
    "ts['fx'] = ts['exchange_rate'] - ts['exchange_rate'].shift(1)\n",
    "# ts['fx'] = np.log(ts['exchange_rate'] / ts['exchange_rate'].shift(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847a9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts['lag_hml'] = ts['hml'].shift(1)\n",
    "# ts['lag_smb'] = ts['smb'].shift(1)\n",
    "# ts['lag_mom'] = ts['mom'].shift(1)\n",
    "\n",
    "# ts['lag_myp'] = ts['myp'].shift(1)\n",
    "# ts['lag_ui'] = ts['ui'].shift(1)\n",
    "# ts['lag_dsv'] = ts['dsv'].shift(1)\n",
    "# ts['lag_ats'] = ts['ats'].shift(1)\n",
    "# ts['lag_sts'] = ts['sts'].shift(1)\n",
    "# ts['lag_fx'] = ts['fx'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85e2b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset so far\n",
    "ts.to_csv(results + 'time_series_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d32923d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'summary' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Set the start and end dates of the analysis period\n",
    "# start_date = pd.to_datetime('1975-01-01') ##############\n",
    "# end_date = pd.to_datetime('1999-12-31') ##############\n",
    "ts2 = ts.loc[(ts.index >= start_date) & (ts.index <= end_date)]\n",
    "\n",
    "summary = ts2[['myp','ui','dsv','ats','sts','fx']].describe().T  \n",
    "%store summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a80c551",
   "metadata": {},
   "source": [
    "Granger causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58dd71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAR model with constant term\n",
    "# model = VAR(ts2[['hml','smb','mom','myp','ui','dsv','ats','sts','fx']])\n",
    "# var = model.fit(maxlags=1, trend='c')\n",
    "# %store var\n",
    "# var.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15b900fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class var(GMM):\n",
    "#     def momcond(self, params):\n",
    "#         x = self.exog\n",
    "#         y = self.endog\n",
    "#         T,K = x.shape\n",
    "#         T,N = y.shape\n",
    "#         y = reshape(y,(T,N))\n",
    "#         x = reshape(x,(T,K))\n",
    "#         b = params\n",
    "#         b = squeeze(array(params)) \n",
    "#         b = reshape(b,(K,N))\n",
    "#         err = (y - x @ b)\n",
    "#         moments = tile(x, (1, N)) * kron(err,ones((1,K)))\n",
    "#         return moments\n",
    "    \n",
    "# var_exog = ts2[['lag_hml','lag_smb','lag_mom','lag_myp','lag_ui','lag_dsv','lag_ats','lag_sts','lag_fx']]\n",
    "# # var_exog = var_exog.iloc[1:]\n",
    "# var_exog = sm.add_constant(var_exog)\n",
    "# var_endog = ts2[['hml','smb','mom','myp','ui','dsv','ats','sts','fx']]\n",
    "# # var_endog = var_endog.iloc[1:]\n",
    "# T,K = var_exog.shape\n",
    "# T,N = var_endog.shape\n",
    "\n",
    "# var_mod = var(endog=var_endog, exog=var_exog, instrument=None)\n",
    "# var_fit = var_mod.fit(start_params=zeros((K,N)), maxiter=1, inv_weights=eye(N*K), optim_method='bfgs', optim_args={'gtol': 1e-12, 'maxiter': 1000})\n",
    "# # var_fit = var_mod.fit(start_params=zeros((K+1),9), maxiter=1, inv_weights=eye(N*(K+1)), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 1000})\n",
    "# # gmm_fit8 = gmm_mod.fit(start_params=np.zeros(K), maxiter=1, inv_weights=np.eye(K), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 10000})\n",
    "# print(var_fit.summary())\n",
    "# coeff = reshape(var_fit.params,(K,N))\n",
    "# print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "206051db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 2857\n",
      "         Function evaluations: 2914\n",
      "         Gradient evaluations: 2914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>var Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  Hansen J:          </th> <td>1.606e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>var</td>       <th>  Prob (Hansen J):   </th>  <td>   nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                  <td>GMM</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 10 May 2023</td> <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:07:10</td>     <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 0</th>  <td>    0.0256</td> <td>    0.007</td> <td>    3.825</td> <td> 0.000</td> <td>    0.013</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 1</th>  <td>    0.1195</td> <td>    0.065</td> <td>    1.833</td> <td> 0.067</td> <td>   -0.008</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 2</th>  <td>   -0.4215</td> <td>    0.193</td> <td>   -2.182</td> <td> 0.029</td> <td>   -0.800</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 3</th>  <td>    0.4659</td> <td>    0.315</td> <td>    1.479</td> <td> 0.139</td> <td>   -0.152</td> <td>    1.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 4</th>  <td>   -0.0443</td> <td>    0.032</td> <td>   -1.374</td> <td> 0.170</td> <td>   -0.108</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 5</th>  <td>   -0.0657</td> <td>    0.087</td> <td>   -0.759</td> <td> 0.448</td> <td>   -0.235</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 6</th>  <td>    0.0202</td> <td>    0.099</td> <td>    0.205</td> <td> 0.837</td> <td>   -0.173</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 7</th>  <td>   -9.2779</td> <td>    1.946</td> <td>   -4.768</td> <td> 0.000</td> <td>  -13.092</td> <td>   -5.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 8</th>  <td>   -0.0905</td> <td>    0.247</td> <td>   -0.367</td> <td> 0.714</td> <td>   -0.574</td> <td>    0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 9</th>  <td>    1.6812</td> <td>    0.471</td> <td>    3.567</td> <td> 0.000</td> <td>    0.757</td> <td>    2.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p10</th>  <td>   -0.2899</td> <td>    0.771</td> <td>   -0.376</td> <td> 0.707</td> <td>   -1.801</td> <td>    1.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p11</th>  <td>    1.7601</td> <td>    0.286</td> <td>    6.148</td> <td> 0.000</td> <td>    1.199</td> <td>    2.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p12</th>  <td>    0.1327</td> <td>    0.051</td> <td>    2.596</td> <td> 0.009</td> <td>    0.033</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p13</th>  <td>   -0.5132</td> <td>    0.111</td> <td>   -4.610</td> <td> 0.000</td> <td>   -0.731</td> <td>   -0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p14</th>  <td>    0.1029</td> <td>    0.010</td> <td>   10.378</td> <td> 0.000</td> <td>    0.083</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p15</th>  <td>    0.1238</td> <td>    0.064</td> <td>    1.940</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p16</th>  <td>   -0.1422</td> <td>    0.189</td> <td>   -0.752</td> <td> 0.452</td> <td>   -0.513</td> <td>    0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p17</th>  <td>   -0.0317</td> <td>    0.279</td> <td>   -0.114</td> <td> 0.910</td> <td>   -0.579</td> <td>    0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p18</th>  <td>   -0.0562</td> <td>    0.029</td> <td>   -1.925</td> <td> 0.054</td> <td>   -0.113</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p19</th>  <td>   -0.1168</td> <td>    0.082</td> <td>   -1.423</td> <td> 0.155</td> <td>   -0.278</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p20</th>  <td>   -0.1198</td> <td>    0.095</td> <td>   -1.262</td> <td> 0.207</td> <td>   -0.306</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p21</th>  <td>    0.0009</td> <td>    0.001</td> <td>    1.538</td> <td> 0.124</td> <td>   -0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p22</th>  <td>    0.0034</td> <td>    0.002</td> <td>    2.132</td> <td> 0.033</td> <td>    0.000</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p23</th>  <td>    0.0026</td> <td>    0.002</td> <td>    1.544</td> <td> 0.123</td> <td>   -0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p24</th>  <td>    0.0101</td> <td>    0.002</td> <td>    4.457</td> <td> 0.000</td> <td>    0.006</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p25</th>  <td>   -0.0002</td> <td>    0.000</td> <td>   -1.182</td> <td> 0.237</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p26</th>  <td> 7.898e-06</td> <td>    0.000</td> <td>    0.025</td> <td> 0.980</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p27</th>  <td>   -0.0002</td> <td>    0.000</td> <td>   -0.688</td> <td> 0.491</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p28</th>  <td>    0.0001</td> <td>    0.000</td> <td>    0.448</td> <td> 0.654</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p29</th>  <td>   -0.0004</td> <td>    0.001</td> <td>   -0.366</td> <td> 0.714</td> <td>   -0.002</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p30</th>  <td>   -0.0726</td> <td>    0.111</td> <td>   -0.653</td> <td> 0.514</td> <td>   -0.291</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p31</th>  <td>    0.4242</td> <td>    0.375</td> <td>    1.130</td> <td> 0.259</td> <td>   -0.312</td> <td>    1.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p32</th>  <td>    0.5111</td> <td>    0.469</td> <td>    1.089</td> <td> 0.276</td> <td>   -0.408</td> <td>    1.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p33</th>  <td>   -1.1493</td> <td>    0.566</td> <td>   -2.030</td> <td> 0.042</td> <td>   -2.259</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p34</th>  <td>    0.0614</td> <td>    0.031</td> <td>    1.951</td> <td> 0.051</td> <td>   -0.000</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p35</th>  <td>    0.0980</td> <td>    0.071</td> <td>    1.384</td> <td> 0.166</td> <td>   -0.041</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p36</th>  <td>    0.0339</td> <td>    0.072</td> <td>    0.470</td> <td> 0.639</td> <td>   -0.108</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p37</th>  <td>   -0.0193</td> <td>    0.063</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.142</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p38</th>  <td>    0.0891</td> <td>    0.213</td> <td>    0.418</td> <td> 0.676</td> <td>   -0.329</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p39</th>  <td>   -0.0221</td> <td>    0.017</td> <td>   -1.295</td> <td> 0.195</td> <td>   -0.055</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p40</th>  <td>    0.1980</td> <td>    0.078</td> <td>    2.554</td> <td> 0.011</td> <td>    0.046</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p41</th>  <td>   -0.0322</td> <td>    0.069</td> <td>   -0.464</td> <td> 0.643</td> <td>   -0.168</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p42</th>  <td>   -0.1287</td> <td>    0.127</td> <td>   -1.017</td> <td> 0.309</td> <td>   -0.377</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p43</th>  <td>    0.0052</td> <td>    0.005</td> <td>    1.064</td> <td> 0.287</td> <td>   -0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p44</th>  <td>   -0.0026</td> <td>    0.012</td> <td>   -0.210</td> <td> 0.834</td> <td>   -0.027</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p45</th>  <td>   -0.0027</td> <td>    0.010</td> <td>   -0.260</td> <td> 0.795</td> <td>   -0.023</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p46</th>  <td>   -0.0006</td> <td>    0.010</td> <td>   -0.062</td> <td> 0.950</td> <td>   -0.020</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p47</th>  <td>   -0.0165</td> <td>    0.040</td> <td>   -0.417</td> <td> 0.677</td> <td>   -0.094</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p48</th>  <td>    0.0145</td> <td>    0.012</td> <td>    1.163</td> <td> 0.245</td> <td>   -0.010</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p49</th>  <td>    0.1020</td> <td>    0.083</td> <td>    1.230</td> <td> 0.219</td> <td>   -0.060</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p50</th>  <td>   -0.0214</td> <td>    0.120</td> <td>   -0.178</td> <td> 0.859</td> <td>   -0.258</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p51</th>  <td>    0.1052</td> <td>    0.107</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.105</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p52</th>  <td>    0.0101</td> <td>    0.005</td> <td>    2.191</td> <td> 0.028</td> <td>    0.001</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p53</th>  <td>    0.0238</td> <td>    0.011</td> <td>    2.122</td> <td> 0.034</td> <td>    0.002</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p54</th>  <td>    0.0062</td> <td>    0.006</td> <td>    1.081</td> <td> 0.280</td> <td>   -0.005</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p55</th>  <td>   -0.0058</td> <td>    0.007</td> <td>   -0.841</td> <td> 0.400</td> <td>   -0.019</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p56</th>  <td>   -0.0172</td> <td>    0.033</td> <td>   -0.515</td> <td> 0.607</td> <td>   -0.083</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p57</th>  <td>   -0.0149</td> <td>    0.014</td> <td>   -1.057</td> <td> 0.291</td> <td>   -0.043</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p58</th>  <td>   -0.0766</td> <td>    0.061</td> <td>   -1.247</td> <td> 0.212</td> <td>   -0.197</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p59</th>  <td>   -0.0873</td> <td>    0.052</td> <td>   -1.688</td> <td> 0.091</td> <td>   -0.189</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p60</th>  <td>   -0.0483</td> <td>    0.089</td> <td>   -0.541</td> <td> 0.589</td> <td>   -0.224</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p61</th>  <td>    0.0061</td> <td>    0.003</td> <td>    1.856</td> <td> 0.063</td> <td>   -0.000</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p62</th>  <td>   -0.0082</td> <td>    0.009</td> <td>   -0.876</td> <td> 0.381</td> <td>   -0.026</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p63</th>  <td>    0.0042</td> <td>    0.008</td> <td>    0.555</td> <td> 0.579</td> <td>   -0.011</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p64</th>  <td>   -0.0062</td> <td>    0.007</td> <td>   -0.923</td> <td> 0.356</td> <td>   -0.019</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p65</th>  <td>   -0.0290</td> <td>    0.027</td> <td>   -1.058</td> <td> 0.290</td> <td>   -0.083</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p66</th>  <td>    0.0085</td> <td>    0.129</td> <td>    0.066</td> <td> 0.947</td> <td>   -0.244</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p67</th>  <td>    0.5963</td> <td>    0.495</td> <td>    1.205</td> <td> 0.228</td> <td>   -0.374</td> <td>    1.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p68</th>  <td>   -1.2846</td> <td>    0.546</td> <td>   -2.352</td> <td> 0.019</td> <td>   -2.355</td> <td>   -0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p69</th>  <td>   -0.1152</td> <td>    0.716</td> <td>   -0.161</td> <td> 0.872</td> <td>   -1.519</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p70</th>  <td>    0.3619</td> <td>    0.054</td> <td>    6.743</td> <td> 0.000</td> <td>    0.257</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p71</th>  <td>   -0.1501</td> <td>    0.103</td> <td>   -1.459</td> <td> 0.145</td> <td>   -0.352</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p72</th>  <td>    0.0629</td> <td>    0.084</td> <td>    0.752</td> <td> 0.452</td> <td>   -0.101</td> <td>    0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p73</th>  <td>    0.1135</td> <td>    0.080</td> <td>    1.427</td> <td> 0.154</td> <td>   -0.042</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p74</th>  <td>   -0.0109</td> <td>    0.343</td> <td>   -0.032</td> <td> 0.975</td> <td>   -0.684</td> <td>    0.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p75</th>  <td>   -0.0152</td> <td>    0.107</td> <td>   -0.142</td> <td> 0.887</td> <td>   -0.225</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p76</th>  <td>   -0.0962</td> <td>    0.369</td> <td>   -0.261</td> <td> 0.794</td> <td>   -0.820</td> <td>    0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p77</th>  <td>    0.1223</td> <td>    0.404</td> <td>    0.303</td> <td> 0.762</td> <td>   -0.669</td> <td>    0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p78</th>  <td>    0.2174</td> <td>    0.481</td> <td>    0.452</td> <td> 0.651</td> <td>   -0.726</td> <td>    1.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p79</th>  <td>   -0.0461</td> <td>    0.024</td> <td>   -1.886</td> <td> 0.059</td> <td>   -0.094</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p80</th>  <td>    0.2631</td> <td>    0.060</td> <td>    4.395</td> <td> 0.000</td> <td>    0.146</td> <td>    0.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p81</th>  <td>    0.0744</td> <td>    0.050</td> <td>    1.481</td> <td> 0.139</td> <td>   -0.024</td> <td>    0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p82</th>  <td>   -0.0248</td> <td>    0.048</td> <td>   -0.517</td> <td> 0.605</td> <td>   -0.119</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p83</th>  <td>    0.2877</td> <td>    0.212</td> <td>    1.357</td> <td> 0.175</td> <td>   -0.128</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p84</th>  <td>   -0.1643</td> <td>    0.158</td> <td>   -1.041</td> <td> 0.298</td> <td>   -0.474</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p85</th>  <td>   -0.8898</td> <td>    0.461</td> <td>   -1.929</td> <td> 0.054</td> <td>   -1.794</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p86</th>  <td>   -0.5137</td> <td>    0.499</td> <td>   -1.029</td> <td> 0.303</td> <td>   -1.492</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p87</th>  <td>    0.7095</td> <td>    0.900</td> <td>    0.789</td> <td> 0.430</td> <td>   -1.054</td> <td>    2.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p88</th>  <td>    0.0567</td> <td>    0.044</td> <td>    1.284</td> <td> 0.199</td> <td>   -0.030</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p89</th>  <td>   -0.3486</td> <td>    0.114</td> <td>   -3.053</td> <td> 0.002</td> <td>   -0.572</td> <td>   -0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p90</th>  <td>    0.2159</td> <td>    0.094</td> <td>    2.295</td> <td> 0.022</td> <td>    0.031</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p91</th>  <td>   -0.1927</td> <td>    0.102</td> <td>   -1.892</td> <td> 0.059</td> <td>   -0.392</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p92</th>  <td>    0.6271</td> <td>    0.282</td> <td>    2.225</td> <td> 0.026</td> <td>    0.075</td> <td>    1.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p93</th>  <td>   -0.0760</td> <td>    0.101</td> <td>   -0.749</td> <td> 0.454</td> <td>   -0.275</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p94</th>  <td>   -0.9138</td> <td>    0.431</td> <td>   -2.122</td> <td> 0.034</td> <td>   -1.758</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p95</th>  <td>   -0.1929</td> <td>    0.466</td> <td>   -0.414</td> <td> 0.679</td> <td>   -1.106</td> <td>    0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p96</th>  <td>    0.3278</td> <td>    0.620</td> <td>    0.529</td> <td> 0.597</td> <td>   -0.887</td> <td>    1.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p97</th>  <td>   -0.0049</td> <td>    0.042</td> <td>   -0.115</td> <td> 0.908</td> <td>   -0.088</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p98</th>  <td>   -0.0573</td> <td>    0.097</td> <td>   -0.594</td> <td> 0.553</td> <td>   -0.247</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p99</th>  <td>    0.0010</td> <td>    0.085</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.167</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p100</th> <td>    0.0768</td> <td>    0.074</td> <td>    1.040</td> <td> 0.298</td> <td>   -0.068</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p101</th> <td>    0.3028</td> <td>    0.273</td> <td>    1.108</td> <td> 0.268</td> <td>   -0.233</td> <td>    0.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p102</th> <td>    0.0022</td> <td>    0.025</td> <td>    0.090</td> <td> 0.928</td> <td>   -0.047</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p103</th> <td>    0.1456</td> <td>    0.080</td> <td>    1.812</td> <td> 0.070</td> <td>   -0.012</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p104</th> <td>   -0.0097</td> <td>    0.074</td> <td>   -0.132</td> <td> 0.895</td> <td>   -0.155</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p105</th> <td>    0.0387</td> <td>    0.134</td> <td>    0.288</td> <td> 0.774</td> <td>   -0.225</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p106</th> <td>   -0.0075</td> <td>    0.007</td> <td>   -1.060</td> <td> 0.289</td> <td>   -0.021</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p107</th> <td>   -0.0127</td> <td>    0.016</td> <td>   -0.802</td> <td> 0.422</td> <td>   -0.044</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p108</th> <td>   -0.0295</td> <td>    0.015</td> <td>   -1.940</td> <td> 0.052</td> <td>   -0.059</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p109</th> <td>    0.0278</td> <td>    0.015</td> <td>    1.854</td> <td> 0.064</td> <td>   -0.002</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p110</th> <td>    0.1321</td> <td>    0.065</td> <td>    2.047</td> <td> 0.041</td> <td>    0.006</td> <td>    0.258</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 var Results                                  \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   Hansen J:                    1.606e-18\n",
       "Model:                            var   Prob (Hansen J):                   nan\n",
       "Method:                           GMM                                         \n",
       "Date:                Wed, 10 May 2023                                         \n",
       "Time:                        18:07:10                                         \n",
       "No. Observations:                 400                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "p 0            0.0256      0.007      3.825      0.000       0.013       0.039\n",
       "p 1            0.1195      0.065      1.833      0.067      -0.008       0.247\n",
       "p 2           -0.4215      0.193     -2.182      0.029      -0.800      -0.043\n",
       "p 3            0.4659      0.315      1.479      0.139      -0.152       1.083\n",
       "p 4           -0.0443      0.032     -1.374      0.170      -0.108       0.019\n",
       "p 5           -0.0657      0.087     -0.759      0.448      -0.235       0.104\n",
       "p 6            0.0202      0.099      0.205      0.837      -0.173       0.213\n",
       "p 7           -9.2779      1.946     -4.768      0.000     -13.092      -5.464\n",
       "p 8           -0.0905      0.247     -0.367      0.714      -0.574       0.393\n",
       "p 9            1.6812      0.471      3.567      0.000       0.757       2.605\n",
       "p10           -0.2899      0.771     -0.376      0.707      -1.801       1.222\n",
       "p11            1.7601      0.286      6.148      0.000       1.199       2.321\n",
       "p12            0.1327      0.051      2.596      0.009       0.033       0.233\n",
       "p13           -0.5132      0.111     -4.610      0.000      -0.731      -0.295\n",
       "p14            0.1029      0.010     10.378      0.000       0.083       0.122\n",
       "p15            0.1238      0.064      1.940      0.052      -0.001       0.249\n",
       "p16           -0.1422      0.189     -0.752      0.452      -0.513       0.228\n",
       "p17           -0.0317      0.279     -0.114      0.910      -0.579       0.515\n",
       "p18           -0.0562      0.029     -1.925      0.054      -0.113       0.001\n",
       "p19           -0.1168      0.082     -1.423      0.155      -0.278       0.044\n",
       "p20           -0.1198      0.095     -1.262      0.207      -0.306       0.066\n",
       "p21            0.0009      0.001      1.538      0.124      -0.000       0.002\n",
       "p22            0.0034      0.002      2.132      0.033       0.000       0.007\n",
       "p23            0.0026      0.002      1.544      0.123      -0.001       0.006\n",
       "p24            0.0101      0.002      4.457      0.000       0.006       0.014\n",
       "p25           -0.0002      0.000     -1.182      0.237      -0.000       0.000\n",
       "p26         7.898e-06      0.000      0.025      0.980      -0.001       0.001\n",
       "p27           -0.0002      0.000     -0.688      0.491      -0.001       0.000\n",
       "p28            0.0001      0.000      0.448      0.654      -0.000       0.001\n",
       "p29           -0.0004      0.001     -0.366      0.714      -0.002       0.002\n",
       "p30           -0.0726      0.111     -0.653      0.514      -0.291       0.145\n",
       "p31            0.4242      0.375      1.130      0.259      -0.312       1.160\n",
       "p32            0.5111      0.469      1.089      0.276      -0.408       1.431\n",
       "p33           -1.1493      0.566     -2.030      0.042      -2.259      -0.039\n",
       "p34            0.0614      0.031      1.951      0.051      -0.000       0.123\n",
       "p35            0.0980      0.071      1.384      0.166      -0.041       0.237\n",
       "p36            0.0339      0.072      0.470      0.639      -0.108       0.175\n",
       "p37           -0.0193      0.063     -0.309      0.757      -0.142       0.103\n",
       "p38            0.0891      0.213      0.418      0.676      -0.329       0.507\n",
       "p39           -0.0221      0.017     -1.295      0.195      -0.055       0.011\n",
       "p40            0.1980      0.078      2.554      0.011       0.046       0.350\n",
       "p41           -0.0322      0.069     -0.464      0.643      -0.168       0.104\n",
       "p42           -0.1287      0.127     -1.017      0.309      -0.377       0.119\n",
       "p43            0.0052      0.005      1.064      0.287      -0.004       0.015\n",
       "p44           -0.0026      0.012     -0.210      0.834      -0.027       0.021\n",
       "p45           -0.0027      0.010     -0.260      0.795      -0.023       0.017\n",
       "p46           -0.0006      0.010     -0.062      0.950      -0.020       0.019\n",
       "p47           -0.0165      0.040     -0.417      0.677      -0.094       0.061\n",
       "p48            0.0145      0.012      1.163      0.245      -0.010       0.039\n",
       "p49            0.1020      0.083      1.230      0.219      -0.060       0.264\n",
       "p50           -0.0214      0.120     -0.178      0.859      -0.258       0.215\n",
       "p51            0.1052      0.107      0.980      0.327      -0.105       0.316\n",
       "p52            0.0101      0.005      2.191      0.028       0.001       0.019\n",
       "p53            0.0238      0.011      2.122      0.034       0.002       0.046\n",
       "p54            0.0062      0.006      1.081      0.280      -0.005       0.017\n",
       "p55           -0.0058      0.007     -0.841      0.400      -0.019       0.008\n",
       "p56           -0.0172      0.033     -0.515      0.607      -0.083       0.048\n",
       "p57           -0.0149      0.014     -1.057      0.291      -0.043       0.013\n",
       "p58           -0.0766      0.061     -1.247      0.212      -0.197       0.044\n",
       "p59           -0.0873      0.052     -1.688      0.091      -0.189       0.014\n",
       "p60           -0.0483      0.089     -0.541      0.589      -0.224       0.127\n",
       "p61            0.0061      0.003      1.856      0.063      -0.000       0.013\n",
       "p62           -0.0082      0.009     -0.876      0.381      -0.026       0.010\n",
       "p63            0.0042      0.008      0.555      0.579      -0.011       0.019\n",
       "p64           -0.0062      0.007     -0.923      0.356      -0.019       0.007\n",
       "p65           -0.0290      0.027     -1.058      0.290      -0.083       0.025\n",
       "p66            0.0085      0.129      0.066      0.947      -0.244       0.261\n",
       "p67            0.5963      0.495      1.205      0.228      -0.374       1.566\n",
       "p68           -1.2846      0.546     -2.352      0.019      -2.355      -0.214\n",
       "p69           -0.1152      0.716     -0.161      0.872      -1.519       1.288\n",
       "p70            0.3619      0.054      6.743      0.000       0.257       0.467\n",
       "p71           -0.1501      0.103     -1.459      0.145      -0.352       0.052\n",
       "p72            0.0629      0.084      0.752      0.452      -0.101       0.227\n",
       "p73            0.1135      0.080      1.427      0.154      -0.042       0.269\n",
       "p74           -0.0109      0.343     -0.032      0.975      -0.684       0.662\n",
       "p75           -0.0152      0.107     -0.142      0.887      -0.225       0.195\n",
       "p76           -0.0962      0.369     -0.261      0.794      -0.820       0.628\n",
       "p77            0.1223      0.404      0.303      0.762      -0.669       0.913\n",
       "p78            0.2174      0.481      0.452      0.651      -0.726       1.161\n",
       "p79           -0.0461      0.024     -1.886      0.059      -0.094       0.002\n",
       "p80            0.2631      0.060      4.395      0.000       0.146       0.380\n",
       "p81            0.0744      0.050      1.481      0.139      -0.024       0.173\n",
       "p82           -0.0248      0.048     -0.517      0.605      -0.119       0.069\n",
       "p83            0.2877      0.212      1.357      0.175      -0.128       0.703\n",
       "p84           -0.1643      0.158     -1.041      0.298      -0.474       0.145\n",
       "p85           -0.8898      0.461     -1.929      0.054      -1.794       0.014\n",
       "p86           -0.5137      0.499     -1.029      0.303      -1.492       0.464\n",
       "p87            0.7095      0.900      0.789      0.430      -1.054       2.473\n",
       "p88            0.0567      0.044      1.284      0.199      -0.030       0.143\n",
       "p89           -0.3486      0.114     -3.053      0.002      -0.572      -0.125\n",
       "p90            0.2159      0.094      2.295      0.022       0.031       0.400\n",
       "p91           -0.1927      0.102     -1.892      0.059      -0.392       0.007\n",
       "p92            0.6271      0.282      2.225      0.026       0.075       1.179\n",
       "p93           -0.0760      0.101     -0.749      0.454      -0.275       0.123\n",
       "p94           -0.9138      0.431     -2.122      0.034      -1.758      -0.070\n",
       "p95           -0.1929      0.466     -0.414      0.679      -1.106       0.720\n",
       "p96            0.3278      0.620      0.529      0.597      -0.887       1.543\n",
       "p97           -0.0049      0.042     -0.115      0.908      -0.088       0.078\n",
       "p98           -0.0573      0.097     -0.594      0.553      -0.247       0.132\n",
       "p99            0.0010      0.085      0.011      0.991      -0.167       0.168\n",
       "p100           0.0768      0.074      1.040      0.298      -0.068       0.222\n",
       "p101           0.3028      0.273      1.108      0.268      -0.233       0.838\n",
       "p102           0.0022      0.025      0.090      0.928      -0.047       0.051\n",
       "p103           0.1456      0.080      1.812      0.070      -0.012       0.303\n",
       "p104          -0.0097      0.074     -0.132      0.895      -0.155       0.135\n",
       "p105           0.0387      0.134      0.288      0.774      -0.225       0.302\n",
       "p106          -0.0075      0.007     -1.060      0.289      -0.021       0.006\n",
       "p107          -0.0127      0.016     -0.802      0.422      -0.044       0.018\n",
       "p108          -0.0295      0.015     -1.940      0.052      -0.059       0.000\n",
       "p109           0.0278      0.015      1.854      0.064      -0.002       0.057\n",
       "p110           0.1321      0.065      2.047      0.041       0.006       0.258\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimick = ts2[['log_indprod_growth_nextyear',\n",
    "                    'ex_mkt',\n",
    "                   'ex_b10ret',\n",
    "                   'ex_b5ret',\n",
    "                #    'ex_high_yd_bd_ret',\n",
    "                   'ex_gold_ret', \n",
    "                   'slope_ex_mkt_87', \n",
    "                   'slope_ex_mkt_9602', \n",
    "                   'rf', \n",
    "                   'lag_10y_3m_gov_bd_yd',\n",
    "                   'lag_1y_3m_gov_bd_yd',\n",
    "                   'lag_Baa_Aaa_bd_yd',\n",
    "                   'lag_sp_div_yd',\n",
    "                   'log_indprod_growth_lastyear',\n",
    "                   'infl_lastyear',\n",
    "                   'ex_mkt_lastyear',\n",
    "                   'lag_ex_mkt',\n",
    "                   'lag_ex_b10ret',\n",
    "                   'lag_ex_b5ret',\n",
    "                   'lag_ex_gold_ret',\n",
    "                   'lag_slope_ex_mkt_87',\n",
    "                   'lag_slope_ex_mkt_9602'\n",
    "                   ]].values\n",
    "T,K = mimick.shape\n",
    "# mimick_endog = ts2[['log_indprod_growth_nextyear']].values\n",
    "var_exog = ts2[['hml','smb','mom','ui','dsv','ats','sts','fx']].values\n",
    "T,M = var_exog.shape\n",
    "\n",
    "\n",
    "class var(GMM):\n",
    "    def momcond(self, params):\n",
    "        var_exog = self.exog\n",
    "        y = self.endog\n",
    "        mimick = self.instrument\n",
    "        coeff = squeeze(array(params)) \n",
    "        \n",
    "        # first stage\n",
    "        mimick_endog = mimick[:,0]\n",
    "        mimick_exog = mimick[:,1:]\n",
    "        mimick_exog = sm.add_constant(mimick_exog)\n",
    "        T,K = mimick_exog.shape\n",
    "        mimick_exog = reshape(mimick_exog,(T,K))\n",
    "        mimick_endog = reshape(mimick_endog,(T,1))\n",
    "        mimick_coeff = coeff[:K]\n",
    "        mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "        mimick_err = (mimick_endog - mimick_exog @ mimick_coeff)\n",
    "        moments1 = mimick_exog * kron(mimick_err,ones((1,K)))\n",
    "        moments1 = moments1[1:] # delete the first row\n",
    "\n",
    "        # compute myp\n",
    "        myp = mimick[:, 1:7] @ mimick_coeff[1:7] \n",
    "\n",
    "        # var\n",
    "        var_exog = column_stack((myp, var_exog))\n",
    "        var_lag = var_exog[:-1]\n",
    "        var_lag = sm.add_constant(var_lag)\n",
    "        var_exog = var_exog[1:]\n",
    "        T,M = var_exog.shape\n",
    "        T,N = var_lag.shape\n",
    "        var_exog = reshape(var_exog,(T,M))\n",
    "        var_lag = reshape(var_lag,(T,N))\n",
    "        var_coeff = params[K:]\n",
    "        var_coeff = reshape(var_coeff,(N,M))\n",
    "        var_err = (var_exog - var_lag @ var_coeff)\n",
    "        moments2 = tile(var_lag, (1, M)) * kron(var_err,ones((1,N)))\n",
    "        return column_stack((moments1,moments2))\n",
    "    \n",
    "var_mod = var(endog=zeros(var_exog.shape[0]), exog=var_exog, instrument=mimick)\n",
    "var_fit = var_mod.fit(start_params=zeros(K+(M+1)*(M+2)), maxiter=1, inv_weights=eye(K+(M+1)*(M+2)), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 10000})\n",
    "var_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60394e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 2857\n",
      "         Function evaluations: 2914\n",
      "         Gradient evaluations: 2914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>var Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8']</td> <th>  Hansen J:          </th> <td>1.606e-18</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                                   <td>var</td>                       <th>  Prob (Hansen J):   </th>  <td>   nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                                  <td>GMM</td>                       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                             <td>Tue, 09 May 2023</td>                 <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                                 <td>19:17:05</td>                     <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                      <td>   400</td>                      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 0</th>  <td>    0.0256</td> <td>    0.007</td> <td>    3.825</td> <td> 0.000</td> <td>    0.013</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 1</th>  <td>    0.1195</td> <td>    0.065</td> <td>    1.833</td> <td> 0.067</td> <td>   -0.008</td> <td>    0.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 2</th>  <td>   -0.4215</td> <td>    0.193</td> <td>   -2.182</td> <td> 0.029</td> <td>   -0.800</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 3</th>  <td>    0.4659</td> <td>    0.315</td> <td>    1.479</td> <td> 0.139</td> <td>   -0.152</td> <td>    1.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 4</th>  <td>   -0.0443</td> <td>    0.032</td> <td>   -1.374</td> <td> 0.170</td> <td>   -0.108</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 5</th>  <td>   -0.0657</td> <td>    0.087</td> <td>   -0.759</td> <td> 0.448</td> <td>   -0.235</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 6</th>  <td>    0.0202</td> <td>    0.099</td> <td>    0.205</td> <td> 0.837</td> <td>   -0.173</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 7</th>  <td>   -9.2779</td> <td>    1.946</td> <td>   -4.768</td> <td> 0.000</td> <td>  -13.092</td> <td>   -5.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 8</th>  <td>   -0.0905</td> <td>    0.247</td> <td>   -0.367</td> <td> 0.714</td> <td>   -0.574</td> <td>    0.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 9</th>  <td>    1.6812</td> <td>    0.471</td> <td>    3.567</td> <td> 0.000</td> <td>    0.757</td> <td>    2.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p10</th>  <td>   -0.2899</td> <td>    0.771</td> <td>   -0.376</td> <td> 0.707</td> <td>   -1.801</td> <td>    1.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p11</th>  <td>    1.7601</td> <td>    0.286</td> <td>    6.148</td> <td> 0.000</td> <td>    1.199</td> <td>    2.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p12</th>  <td>    0.1327</td> <td>    0.051</td> <td>    2.596</td> <td> 0.009</td> <td>    0.033</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p13</th>  <td>   -0.5132</td> <td>    0.111</td> <td>   -4.610</td> <td> 0.000</td> <td>   -0.731</td> <td>   -0.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p14</th>  <td>    0.1029</td> <td>    0.010</td> <td>   10.378</td> <td> 0.000</td> <td>    0.083</td> <td>    0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p15</th>  <td>    0.1238</td> <td>    0.064</td> <td>    1.940</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p16</th>  <td>   -0.1422</td> <td>    0.189</td> <td>   -0.752</td> <td> 0.452</td> <td>   -0.513</td> <td>    0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p17</th>  <td>   -0.0317</td> <td>    0.279</td> <td>   -0.114</td> <td> 0.910</td> <td>   -0.579</td> <td>    0.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p18</th>  <td>   -0.0562</td> <td>    0.029</td> <td>   -1.925</td> <td> 0.054</td> <td>   -0.113</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p19</th>  <td>   -0.1168</td> <td>    0.082</td> <td>   -1.423</td> <td> 0.155</td> <td>   -0.278</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p20</th>  <td>   -0.1198</td> <td>    0.095</td> <td>   -1.262</td> <td> 0.207</td> <td>   -0.306</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p21</th>  <td>    0.0009</td> <td>    0.001</td> <td>    1.538</td> <td> 0.124</td> <td>   -0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p22</th>  <td>    0.0034</td> <td>    0.002</td> <td>    2.132</td> <td> 0.033</td> <td>    0.000</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p23</th>  <td>    0.0026</td> <td>    0.002</td> <td>    1.544</td> <td> 0.123</td> <td>   -0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p24</th>  <td>    0.0101</td> <td>    0.002</td> <td>    4.457</td> <td> 0.000</td> <td>    0.006</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p25</th>  <td>   -0.0002</td> <td>    0.000</td> <td>   -1.182</td> <td> 0.237</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p26</th>  <td> 7.898e-06</td> <td>    0.000</td> <td>    0.025</td> <td> 0.980</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p27</th>  <td>   -0.0002</td> <td>    0.000</td> <td>   -0.688</td> <td> 0.491</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p28</th>  <td>    0.0001</td> <td>    0.000</td> <td>    0.448</td> <td> 0.654</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p29</th>  <td>   -0.0004</td> <td>    0.001</td> <td>   -0.366</td> <td> 0.714</td> <td>   -0.002</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p30</th>  <td>   -0.0726</td> <td>    0.111</td> <td>   -0.653</td> <td> 0.514</td> <td>   -0.291</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p31</th>  <td>    0.4242</td> <td>    0.375</td> <td>    1.130</td> <td> 0.259</td> <td>   -0.312</td> <td>    1.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p32</th>  <td>    0.5111</td> <td>    0.469</td> <td>    1.089</td> <td> 0.276</td> <td>   -0.408</td> <td>    1.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p33</th>  <td>   -1.1493</td> <td>    0.566</td> <td>   -2.030</td> <td> 0.042</td> <td>   -2.259</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p34</th>  <td>    0.0614</td> <td>    0.031</td> <td>    1.951</td> <td> 0.051</td> <td>   -0.000</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p35</th>  <td>    0.0980</td> <td>    0.071</td> <td>    1.384</td> <td> 0.166</td> <td>   -0.041</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p36</th>  <td>    0.0339</td> <td>    0.072</td> <td>    0.470</td> <td> 0.639</td> <td>   -0.108</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p37</th>  <td>   -0.0193</td> <td>    0.063</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.142</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p38</th>  <td>    0.0891</td> <td>    0.213</td> <td>    0.418</td> <td> 0.676</td> <td>   -0.329</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p39</th>  <td>   -0.0221</td> <td>    0.017</td> <td>   -1.295</td> <td> 0.195</td> <td>   -0.055</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p40</th>  <td>    0.1980</td> <td>    0.078</td> <td>    2.554</td> <td> 0.011</td> <td>    0.046</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p41</th>  <td>   -0.0322</td> <td>    0.069</td> <td>   -0.464</td> <td> 0.643</td> <td>   -0.168</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p42</th>  <td>   -0.1287</td> <td>    0.127</td> <td>   -1.017</td> <td> 0.309</td> <td>   -0.377</td> <td>    0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p43</th>  <td>    0.0052</td> <td>    0.005</td> <td>    1.064</td> <td> 0.287</td> <td>   -0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p44</th>  <td>   -0.0026</td> <td>    0.012</td> <td>   -0.210</td> <td> 0.834</td> <td>   -0.027</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p45</th>  <td>   -0.0027</td> <td>    0.010</td> <td>   -0.260</td> <td> 0.795</td> <td>   -0.023</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p46</th>  <td>   -0.0006</td> <td>    0.010</td> <td>   -0.062</td> <td> 0.950</td> <td>   -0.020</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p47</th>  <td>   -0.0165</td> <td>    0.040</td> <td>   -0.417</td> <td> 0.677</td> <td>   -0.094</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p48</th>  <td>    0.0145</td> <td>    0.012</td> <td>    1.163</td> <td> 0.245</td> <td>   -0.010</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p49</th>  <td>    0.1020</td> <td>    0.083</td> <td>    1.230</td> <td> 0.219</td> <td>   -0.060</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p50</th>  <td>   -0.0214</td> <td>    0.120</td> <td>   -0.178</td> <td> 0.859</td> <td>   -0.258</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p51</th>  <td>    0.1052</td> <td>    0.107</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.105</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p52</th>  <td>    0.0101</td> <td>    0.005</td> <td>    2.191</td> <td> 0.028</td> <td>    0.001</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p53</th>  <td>    0.0238</td> <td>    0.011</td> <td>    2.122</td> <td> 0.034</td> <td>    0.002</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p54</th>  <td>    0.0062</td> <td>    0.006</td> <td>    1.081</td> <td> 0.280</td> <td>   -0.005</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p55</th>  <td>   -0.0058</td> <td>    0.007</td> <td>   -0.841</td> <td> 0.400</td> <td>   -0.019</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p56</th>  <td>   -0.0172</td> <td>    0.033</td> <td>   -0.515</td> <td> 0.607</td> <td>   -0.083</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p57</th>  <td>   -0.0149</td> <td>    0.014</td> <td>   -1.057</td> <td> 0.291</td> <td>   -0.043</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p58</th>  <td>   -0.0766</td> <td>    0.061</td> <td>   -1.247</td> <td> 0.212</td> <td>   -0.197</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p59</th>  <td>   -0.0873</td> <td>    0.052</td> <td>   -1.688</td> <td> 0.091</td> <td>   -0.189</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p60</th>  <td>   -0.0483</td> <td>    0.089</td> <td>   -0.541</td> <td> 0.589</td> <td>   -0.224</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p61</th>  <td>    0.0061</td> <td>    0.003</td> <td>    1.856</td> <td> 0.063</td> <td>   -0.000</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p62</th>  <td>   -0.0082</td> <td>    0.009</td> <td>   -0.876</td> <td> 0.381</td> <td>   -0.026</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p63</th>  <td>    0.0042</td> <td>    0.008</td> <td>    0.555</td> <td> 0.579</td> <td>   -0.011</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p64</th>  <td>   -0.0062</td> <td>    0.007</td> <td>   -0.923</td> <td> 0.356</td> <td>   -0.019</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p65</th>  <td>   -0.0290</td> <td>    0.027</td> <td>   -1.058</td> <td> 0.290</td> <td>   -0.083</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p66</th>  <td>    0.0085</td> <td>    0.129</td> <td>    0.066</td> <td> 0.947</td> <td>   -0.244</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p67</th>  <td>    0.5963</td> <td>    0.495</td> <td>    1.205</td> <td> 0.228</td> <td>   -0.374</td> <td>    1.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p68</th>  <td>   -1.2846</td> <td>    0.546</td> <td>   -2.352</td> <td> 0.019</td> <td>   -2.355</td> <td>   -0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p69</th>  <td>   -0.1152</td> <td>    0.716</td> <td>   -0.161</td> <td> 0.872</td> <td>   -1.519</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p70</th>  <td>    0.3619</td> <td>    0.054</td> <td>    6.743</td> <td> 0.000</td> <td>    0.257</td> <td>    0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p71</th>  <td>   -0.1501</td> <td>    0.103</td> <td>   -1.459</td> <td> 0.145</td> <td>   -0.352</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p72</th>  <td>    0.0629</td> <td>    0.084</td> <td>    0.752</td> <td> 0.452</td> <td>   -0.101</td> <td>    0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p73</th>  <td>    0.1135</td> <td>    0.080</td> <td>    1.427</td> <td> 0.154</td> <td>   -0.042</td> <td>    0.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p74</th>  <td>   -0.0109</td> <td>    0.343</td> <td>   -0.032</td> <td> 0.975</td> <td>   -0.684</td> <td>    0.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p75</th>  <td>   -0.0152</td> <td>    0.107</td> <td>   -0.142</td> <td> 0.887</td> <td>   -0.225</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p76</th>  <td>   -0.0962</td> <td>    0.369</td> <td>   -0.261</td> <td> 0.794</td> <td>   -0.820</td> <td>    0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p77</th>  <td>    0.1223</td> <td>    0.404</td> <td>    0.303</td> <td> 0.762</td> <td>   -0.669</td> <td>    0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p78</th>  <td>    0.2174</td> <td>    0.481</td> <td>    0.452</td> <td> 0.651</td> <td>   -0.726</td> <td>    1.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p79</th>  <td>   -0.0461</td> <td>    0.024</td> <td>   -1.886</td> <td> 0.059</td> <td>   -0.094</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p80</th>  <td>    0.2631</td> <td>    0.060</td> <td>    4.395</td> <td> 0.000</td> <td>    0.146</td> <td>    0.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p81</th>  <td>    0.0744</td> <td>    0.050</td> <td>    1.481</td> <td> 0.139</td> <td>   -0.024</td> <td>    0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p82</th>  <td>   -0.0248</td> <td>    0.048</td> <td>   -0.517</td> <td> 0.605</td> <td>   -0.119</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p83</th>  <td>    0.2877</td> <td>    0.212</td> <td>    1.357</td> <td> 0.175</td> <td>   -0.128</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p84</th>  <td>   -0.1643</td> <td>    0.158</td> <td>   -1.041</td> <td> 0.298</td> <td>   -0.474</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p85</th>  <td>   -0.8898</td> <td>    0.461</td> <td>   -1.929</td> <td> 0.054</td> <td>   -1.794</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p86</th>  <td>   -0.5137</td> <td>    0.499</td> <td>   -1.029</td> <td> 0.303</td> <td>   -1.492</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p87</th>  <td>    0.7095</td> <td>    0.900</td> <td>    0.789</td> <td> 0.430</td> <td>   -1.054</td> <td>    2.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p88</th>  <td>    0.0567</td> <td>    0.044</td> <td>    1.284</td> <td> 0.199</td> <td>   -0.030</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p89</th>  <td>   -0.3486</td> <td>    0.114</td> <td>   -3.053</td> <td> 0.002</td> <td>   -0.572</td> <td>   -0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p90</th>  <td>    0.2159</td> <td>    0.094</td> <td>    2.295</td> <td> 0.022</td> <td>    0.031</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p91</th>  <td>   -0.1927</td> <td>    0.102</td> <td>   -1.892</td> <td> 0.059</td> <td>   -0.392</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p92</th>  <td>    0.6271</td> <td>    0.282</td> <td>    2.225</td> <td> 0.026</td> <td>    0.075</td> <td>    1.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p93</th>  <td>   -0.0760</td> <td>    0.101</td> <td>   -0.749</td> <td> 0.454</td> <td>   -0.275</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p94</th>  <td>   -0.9138</td> <td>    0.431</td> <td>   -2.122</td> <td> 0.034</td> <td>   -1.758</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p95</th>  <td>   -0.1929</td> <td>    0.466</td> <td>   -0.414</td> <td> 0.679</td> <td>   -1.106</td> <td>    0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p96</th>  <td>    0.3278</td> <td>    0.620</td> <td>    0.529</td> <td> 0.597</td> <td>   -0.887</td> <td>    1.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p97</th>  <td>   -0.0049</td> <td>    0.042</td> <td>   -0.115</td> <td> 0.908</td> <td>   -0.088</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p98</th>  <td>   -0.0573</td> <td>    0.097</td> <td>   -0.594</td> <td> 0.553</td> <td>   -0.247</td> <td>    0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p99</th>  <td>    0.0010</td> <td>    0.085</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.167</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p100</th> <td>    0.0768</td> <td>    0.074</td> <td>    1.040</td> <td> 0.298</td> <td>   -0.068</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p101</th> <td>    0.3028</td> <td>    0.273</td> <td>    1.108</td> <td> 0.268</td> <td>   -0.233</td> <td>    0.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p102</th> <td>    0.0022</td> <td>    0.025</td> <td>    0.090</td> <td> 0.928</td> <td>   -0.047</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p103</th> <td>    0.1456</td> <td>    0.080</td> <td>    1.812</td> <td> 0.070</td> <td>   -0.012</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p104</th> <td>   -0.0097</td> <td>    0.074</td> <td>   -0.132</td> <td> 0.895</td> <td>   -0.155</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p105</th> <td>    0.0387</td> <td>    0.134</td> <td>    0.288</td> <td> 0.774</td> <td>   -0.225</td> <td>    0.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p106</th> <td>   -0.0075</td> <td>    0.007</td> <td>   -1.060</td> <td> 0.289</td> <td>   -0.021</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p107</th> <td>   -0.0127</td> <td>    0.016</td> <td>   -0.802</td> <td> 0.422</td> <td>   -0.044</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p108</th> <td>   -0.0295</td> <td>    0.015</td> <td>   -1.940</td> <td> 0.052</td> <td>   -0.059</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p109</th> <td>    0.0278</td> <td>    0.015</td> <td>    1.854</td> <td> 0.064</td> <td>   -0.002</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p110</th> <td>    0.1321</td> <td>    0.065</td> <td>    2.047</td> <td> 0.041</td> <td>    0.006</td> <td>    0.258</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                                var Results                                                 \n",
       "============================================================================================================\n",
       "Dep. Variable:     ['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8']   Hansen J:                    1.606e-18\n",
       "Model:                                                          var   Prob (Hansen J):                   nan\n",
       "Method:                                                         GMM                                         \n",
       "Date:                                              Tue, 09 May 2023                                         \n",
       "Time:                                                      19:17:05                                         \n",
       "No. Observations:                                               400                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "p 0            0.0256      0.007      3.825      0.000       0.013       0.039\n",
       "p 1            0.1195      0.065      1.833      0.067      -0.008       0.247\n",
       "p 2           -0.4215      0.193     -2.182      0.029      -0.800      -0.043\n",
       "p 3            0.4659      0.315      1.479      0.139      -0.152       1.083\n",
       "p 4           -0.0443      0.032     -1.374      0.170      -0.108       0.019\n",
       "p 5           -0.0657      0.087     -0.759      0.448      -0.235       0.104\n",
       "p 6            0.0202      0.099      0.205      0.837      -0.173       0.213\n",
       "p 7           -9.2779      1.946     -4.768      0.000     -13.092      -5.464\n",
       "p 8           -0.0905      0.247     -0.367      0.714      -0.574       0.393\n",
       "p 9            1.6812      0.471      3.567      0.000       0.757       2.605\n",
       "p10           -0.2899      0.771     -0.376      0.707      -1.801       1.222\n",
       "p11            1.7601      0.286      6.148      0.000       1.199       2.321\n",
       "p12            0.1327      0.051      2.596      0.009       0.033       0.233\n",
       "p13           -0.5132      0.111     -4.610      0.000      -0.731      -0.295\n",
       "p14            0.1029      0.010     10.378      0.000       0.083       0.122\n",
       "p15            0.1238      0.064      1.940      0.052      -0.001       0.249\n",
       "p16           -0.1422      0.189     -0.752      0.452      -0.513       0.228\n",
       "p17           -0.0317      0.279     -0.114      0.910      -0.579       0.515\n",
       "p18           -0.0562      0.029     -1.925      0.054      -0.113       0.001\n",
       "p19           -0.1168      0.082     -1.423      0.155      -0.278       0.044\n",
       "p20           -0.1198      0.095     -1.262      0.207      -0.306       0.066\n",
       "p21            0.0009      0.001      1.538      0.124      -0.000       0.002\n",
       "p22            0.0034      0.002      2.132      0.033       0.000       0.007\n",
       "p23            0.0026      0.002      1.544      0.123      -0.001       0.006\n",
       "p24            0.0101      0.002      4.457      0.000       0.006       0.014\n",
       "p25           -0.0002      0.000     -1.182      0.237      -0.000       0.000\n",
       "p26         7.898e-06      0.000      0.025      0.980      -0.001       0.001\n",
       "p27           -0.0002      0.000     -0.688      0.491      -0.001       0.000\n",
       "p28            0.0001      0.000      0.448      0.654      -0.000       0.001\n",
       "p29           -0.0004      0.001     -0.366      0.714      -0.002       0.002\n",
       "p30           -0.0726      0.111     -0.653      0.514      -0.291       0.145\n",
       "p31            0.4242      0.375      1.130      0.259      -0.312       1.160\n",
       "p32            0.5111      0.469      1.089      0.276      -0.408       1.431\n",
       "p33           -1.1493      0.566     -2.030      0.042      -2.259      -0.039\n",
       "p34            0.0614      0.031      1.951      0.051      -0.000       0.123\n",
       "p35            0.0980      0.071      1.384      0.166      -0.041       0.237\n",
       "p36            0.0339      0.072      0.470      0.639      -0.108       0.175\n",
       "p37           -0.0193      0.063     -0.309      0.757      -0.142       0.103\n",
       "p38            0.0891      0.213      0.418      0.676      -0.329       0.507\n",
       "p39           -0.0221      0.017     -1.295      0.195      -0.055       0.011\n",
       "p40            0.1980      0.078      2.554      0.011       0.046       0.350\n",
       "p41           -0.0322      0.069     -0.464      0.643      -0.168       0.104\n",
       "p42           -0.1287      0.127     -1.017      0.309      -0.377       0.119\n",
       "p43            0.0052      0.005      1.064      0.287      -0.004       0.015\n",
       "p44           -0.0026      0.012     -0.210      0.834      -0.027       0.021\n",
       "p45           -0.0027      0.010     -0.260      0.795      -0.023       0.017\n",
       "p46           -0.0006      0.010     -0.062      0.950      -0.020       0.019\n",
       "p47           -0.0165      0.040     -0.417      0.677      -0.094       0.061\n",
       "p48            0.0145      0.012      1.163      0.245      -0.010       0.039\n",
       "p49            0.1020      0.083      1.230      0.219      -0.060       0.264\n",
       "p50           -0.0214      0.120     -0.178      0.859      -0.258       0.215\n",
       "p51            0.1052      0.107      0.980      0.327      -0.105       0.316\n",
       "p52            0.0101      0.005      2.191      0.028       0.001       0.019\n",
       "p53            0.0238      0.011      2.122      0.034       0.002       0.046\n",
       "p54            0.0062      0.006      1.081      0.280      -0.005       0.017\n",
       "p55           -0.0058      0.007     -0.841      0.400      -0.019       0.008\n",
       "p56           -0.0172      0.033     -0.515      0.607      -0.083       0.048\n",
       "p57           -0.0149      0.014     -1.057      0.291      -0.043       0.013\n",
       "p58           -0.0766      0.061     -1.247      0.212      -0.197       0.044\n",
       "p59           -0.0873      0.052     -1.688      0.091      -0.189       0.014\n",
       "p60           -0.0483      0.089     -0.541      0.589      -0.224       0.127\n",
       "p61            0.0061      0.003      1.856      0.063      -0.000       0.013\n",
       "p62           -0.0082      0.009     -0.876      0.381      -0.026       0.010\n",
       "p63            0.0042      0.008      0.555      0.579      -0.011       0.019\n",
       "p64           -0.0062      0.007     -0.923      0.356      -0.019       0.007\n",
       "p65           -0.0290      0.027     -1.058      0.290      -0.083       0.025\n",
       "p66            0.0085      0.129      0.066      0.947      -0.244       0.261\n",
       "p67            0.5963      0.495      1.205      0.228      -0.374       1.566\n",
       "p68           -1.2846      0.546     -2.352      0.019      -2.355      -0.214\n",
       "p69           -0.1152      0.716     -0.161      0.872      -1.519       1.288\n",
       "p70            0.3619      0.054      6.743      0.000       0.257       0.467\n",
       "p71           -0.1501      0.103     -1.459      0.145      -0.352       0.052\n",
       "p72            0.0629      0.084      0.752      0.452      -0.101       0.227\n",
       "p73            0.1135      0.080      1.427      0.154      -0.042       0.269\n",
       "p74           -0.0109      0.343     -0.032      0.975      -0.684       0.662\n",
       "p75           -0.0152      0.107     -0.142      0.887      -0.225       0.195\n",
       "p76           -0.0962      0.369     -0.261      0.794      -0.820       0.628\n",
       "p77            0.1223      0.404      0.303      0.762      -0.669       0.913\n",
       "p78            0.2174      0.481      0.452      0.651      -0.726       1.161\n",
       "p79           -0.0461      0.024     -1.886      0.059      -0.094       0.002\n",
       "p80            0.2631      0.060      4.395      0.000       0.146       0.380\n",
       "p81            0.0744      0.050      1.481      0.139      -0.024       0.173\n",
       "p82           -0.0248      0.048     -0.517      0.605      -0.119       0.069\n",
       "p83            0.2877      0.212      1.357      0.175      -0.128       0.703\n",
       "p84           -0.1643      0.158     -1.041      0.298      -0.474       0.145\n",
       "p85           -0.8898      0.461     -1.929      0.054      -1.794       0.014\n",
       "p86           -0.5137      0.499     -1.029      0.303      -1.492       0.464\n",
       "p87            0.7095      0.900      0.789      0.430      -1.054       2.473\n",
       "p88            0.0567      0.044      1.284      0.199      -0.030       0.143\n",
       "p89           -0.3486      0.114     -3.053      0.002      -0.572      -0.125\n",
       "p90            0.2159      0.094      2.295      0.022       0.031       0.400\n",
       "p91           -0.1927      0.102     -1.892      0.059      -0.392       0.007\n",
       "p92            0.6271      0.282      2.225      0.026       0.075       1.179\n",
       "p93           -0.0760      0.101     -0.749      0.454      -0.275       0.123\n",
       "p94           -0.9138      0.431     -2.122      0.034      -1.758      -0.070\n",
       "p95           -0.1929      0.466     -0.414      0.679      -1.106       0.720\n",
       "p96            0.3278      0.620      0.529      0.597      -0.887       1.543\n",
       "p97           -0.0049      0.042     -0.115      0.908      -0.088       0.078\n",
       "p98           -0.0573      0.097     -0.594      0.553      -0.247       0.132\n",
       "p99            0.0010      0.085      0.011      0.991      -0.167       0.168\n",
       "p100           0.0768      0.074      1.040      0.298      -0.068       0.222\n",
       "p101           0.3028      0.273      1.108      0.268      -0.233       0.838\n",
       "p102           0.0022      0.025      0.090      0.928      -0.047       0.051\n",
       "p103           0.1456      0.080      1.812      0.070      -0.012       0.303\n",
       "p104          -0.0097      0.074     -0.132      0.895      -0.155       0.135\n",
       "p105           0.0387      0.134      0.288      0.774      -0.225       0.302\n",
       "p106          -0.0075      0.007     -1.060      0.289      -0.021       0.006\n",
       "p107          -0.0127      0.016     -0.802      0.422      -0.044       0.018\n",
       "p108          -0.0295      0.015     -1.940      0.052      -0.059       0.000\n",
       "p109           0.0278      0.015      1.854      0.064      -0.002       0.057\n",
       "p110           0.1321      0.065      2.047      0.041       0.006       0.258\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimick_exog = ts2[['ex_mkt',\n",
    "                   'ex_b10ret',\n",
    "                   'ex_b5ret',\n",
    "                #    'ex_high_yd_bd_ret',\n",
    "                   'ex_gold_ret', \n",
    "                   'slope_ex_mkt_87', \n",
    "                   'slope_ex_mkt_9602', \n",
    "                   'rf', \n",
    "                   'lag_10y_3m_gov_bd_yd',\n",
    "                   'lag_1y_3m_gov_bd_yd',\n",
    "                   'lag_Baa_Aaa_bd_yd',\n",
    "                   'lag_sp_div_yd',\n",
    "                   'log_indprod_growth_lastyear',\n",
    "                   'infl_lastyear',\n",
    "                   'ex_mkt_lastyear',\n",
    "                   'lag_ex_mkt',\n",
    "                   'lag_ex_b10ret',\n",
    "                   'lag_ex_b5ret',\n",
    "                   'lag_ex_gold_ret',\n",
    "                   'lag_slope_ex_mkt_87',\n",
    "                   'lag_slope_ex_mkt_9602'\n",
    "                   ]].values\n",
    "T,K = mimick_exog.shape\n",
    "mimick_endog = ts2[['log_indprod_growth_nextyear']].values\n",
    "var_endog = ts2[['hml','smb','mom','ui','dsv','ats','sts','fx']].values\n",
    "T,M = var_endog.shape\n",
    "\n",
    "\n",
    "class var(GMM):\n",
    "    def momcond(self, params):\n",
    "        x = self.exog\n",
    "        y = self.endog\n",
    "        z = self.instrument\n",
    "        coeff = squeeze(array(params)) \n",
    "        \n",
    "        # first stage\n",
    "        z = sm.add_constant(z)\n",
    "        T,K = z.shape\n",
    "        z = reshape(z,(T,K))\n",
    "        x = reshape(x,(T,1))\n",
    "        mimick_coeff = coeff[:K]\n",
    "        mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "        mimick_err = (x - z @ mimick_coeff)\n",
    "        moments1 = z * kron(mimick_err,ones((1,K)))\n",
    "        moments1 = moments1[1:] # delete the first row\n",
    "\n",
    "        # compute myp\n",
    "        myp = z[:, 1:7] @ mimick_coeff[1:7] \n",
    "\n",
    "        # var\n",
    "        y = column_stack((myp, y))\n",
    "        y_lag = y[:-1]\n",
    "        y_lag = sm.add_constant(y_lag)\n",
    "        y = y[1:]\n",
    "        T,M = y.shape\n",
    "        T,N = y_lag.shape\n",
    "        y = reshape(y,(T,M))\n",
    "        y_lag = reshape(y_lag,(T,N))\n",
    "        var_coeff = params[K:]\n",
    "        var_coeff = reshape(var_coeff,(N,M))\n",
    "        var_err = (y - y_lag @ var_coeff)\n",
    "        moments2 = tile(y_lag, (1, M)) * kron(var_err,ones((1,N)))\n",
    "        return column_stack((moments1,moments2))\n",
    "    \n",
    "var_mod = var(endog=var_endog, exog=mimick_endog, instrument=mimick_exog)\n",
    "var_fit = var_mod.fit(start_params=zeros((K+1)+(M+1)*(M+2)), maxiter=1, inv_weights=eye((K+1)+(M+1)*(M+2)), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 10000})\n",
    "var_fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'coeff_var' (ndarray)\n",
      "Stored 'tvalues_var' (ndarray)\n",
      "Stored 'pvalues_var' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "def reshape_varfit(item, nmimick, nvar):\n",
    "    mimick = item[:nmimick]\n",
    "    var = reshape(item[nmimick:],((nvar+2),(nvar+1)))\n",
    "    row_2 = var[1]\n",
    "    var = np.delete(var, 1, axis=0)\n",
    "    var = np.insert(var, 4, row_2, axis=0)\n",
    "    var = np.concatenate((var[:, 1:4], var[:, 0].reshape(-1, 1), var[:, 4:]), axis=1)\n",
    "    return mimick, var\n",
    "\n",
    "coeff_mimick, coeff_var = reshape_varfit(var_fit.params, K+1, M)\n",
    "tvalues_mimick, tvalues_var = reshape_varfit(var_fit.tvalues, K+1, M)\n",
    "pvalues_mimick, pvalues_var = reshape_varfit(var_fit.pvalues, K+1, M)\n",
    "\n",
    "%store coeff_var\n",
    "%store tvalues_var\n",
    "%store pvalues_var"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e27c2d8",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7654766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 11 6\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 61373\n",
      "         Function evaluations: 64787\n",
      "         Gradient evaluations: 64775\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>gmm_lambeta Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10']</td> <th>  Hansen J:          </th> <td>1.158e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                                     <td>gmm_lambeta</td>                          <th>  Prob (Hansen J):   </th>  <td>   nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                                        <td>GMM</td>                              <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                                   <td>Thu, 11 May 2023</td>                        <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                                       <td>14:12:54</td>                            <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>                            <td>   400</td>                             <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 0</th> <td>    0.0285</td> <td>    0.007</td> <td>    4.141</td> <td> 0.000</td> <td>    0.015</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 1</th> <td>    0.1566</td> <td>    0.067</td> <td>    2.342</td> <td> 0.019</td> <td>    0.026</td> <td>    0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 2</th> <td>   -1.0335</td> <td>    0.207</td> <td>   -4.994</td> <td> 0.000</td> <td>   -1.439</td> <td>   -0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 3</th> <td>    1.3332</td> <td>    0.310</td> <td>    4.307</td> <td> 0.000</td> <td>    0.727</td> <td>    1.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 4</th> <td>   -0.0616</td> <td>    0.034</td> <td>   -1.814</td> <td> 0.070</td> <td>   -0.128</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 5</th> <td>   -0.1505</td> <td>    0.093</td> <td>   -1.612</td> <td> 0.107</td> <td>   -0.333</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 6</th> <td>   -0.0139</td> <td>    0.097</td> <td>   -0.143</td> <td> 0.886</td> <td>   -0.205</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 7</th> <td>   -7.2378</td> <td>    1.955</td> <td>   -3.702</td> <td> 0.000</td> <td>  -11.070</td> <td>   -3.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 8</th> <td>    0.2057</td> <td>    0.263</td> <td>    0.781</td> <td> 0.435</td> <td>   -0.311</td> <td>    0.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p 9</th> <td>    1.0658</td> <td>    0.510</td> <td>    2.088</td> <td> 0.037</td> <td>    0.065</td> <td>    2.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p10</th> <td>   -2.0867</td> <td>    0.799</td> <td>   -2.613</td> <td> 0.009</td> <td>   -3.652</td> <td>   -0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p11</th> <td>    2.0114</td> <td>    0.292</td> <td>    6.887</td> <td> 0.000</td> <td>    1.439</td> <td>    2.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p12</th> <td>    0.1970</td> <td>    0.055</td> <td>    3.581</td> <td> 0.000</td> <td>    0.089</td> <td>    0.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p13</th> <td>   -0.5289</td> <td>    0.115</td> <td>   -4.584</td> <td> 0.000</td> <td>   -0.755</td> <td>   -0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p14</th> <td>    0.0972</td> <td>    0.010</td> <td>    9.749</td> <td> 0.000</td> <td>    0.078</td> <td>    0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p15</th> <td>    0.1694</td> <td>    0.063</td> <td>    2.690</td> <td> 0.007</td> <td>    0.046</td> <td>    0.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p16</th> <td>   -0.4603</td> <td>    0.187</td> <td>   -2.460</td> <td> 0.014</td> <td>   -0.827</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p17</th> <td>    0.3688</td> <td>    0.270</td> <td>    1.366</td> <td> 0.172</td> <td>   -0.160</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p18</th> <td>   -0.0522</td> <td>    0.030</td> <td>   -1.757</td> <td> 0.079</td> <td>   -0.110</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p19</th> <td>   -0.1425</td> <td>    0.085</td> <td>   -1.681</td> <td> 0.093</td> <td>   -0.309</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p20</th> <td>   -0.1603</td> <td>    0.096</td> <td>   -1.662</td> <td> 0.097</td> <td>   -0.349</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p21</th> <td>    1.5367</td> <td>    0.669</td> <td>    2.298</td> <td> 0.022</td> <td>    0.226</td> <td>    2.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p22</th> <td>   -2.5200</td> <td>    0.677</td> <td>   -3.722</td> <td> 0.000</td> <td>   -3.847</td> <td>   -1.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p23</th> <td>    2.8601</td> <td>    0.608</td> <td>    4.702</td> <td> 0.000</td> <td>    1.668</td> <td>    4.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p24</th> <td>   -1.8361</td> <td>    0.953</td> <td>   -1.926</td> <td> 0.054</td> <td>   -3.705</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p25</th> <td>   -2.0637</td> <td>    0.898</td> <td>   -2.299</td> <td> 0.022</td> <td>   -3.823</td> <td>   -0.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p26</th> <td>   -0.0294</td> <td>    0.132</td> <td>   -0.223</td> <td> 0.823</td> <td>   -0.287</td> <td>    0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p27</th> <td>    1.4306</td> <td>    0.606</td> <td>    2.359</td> <td> 0.018</td> <td>    0.242</td> <td>    2.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p28</th> <td>   -1.9528</td> <td>    0.611</td> <td>   -3.197</td> <td> 0.001</td> <td>   -3.150</td> <td>   -0.756</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p29</th> <td>    2.8642</td> <td>    0.581</td> <td>    4.929</td> <td> 0.000</td> <td>    1.725</td> <td>    4.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p30</th> <td>   -1.8990</td> <td>    0.895</td> <td>   -2.123</td> <td> 0.034</td> <td>   -3.652</td> <td>   -0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p31</th> <td>   -1.8604</td> <td>    0.806</td> <td>   -2.308</td> <td> 0.021</td> <td>   -3.440</td> <td>   -0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p32</th> <td>   -0.0212</td> <td>    0.124</td> <td>   -0.171</td> <td> 0.864</td> <td>   -0.263</td> <td>    0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p33</th> <td>    1.4515</td> <td>    0.590</td> <td>    2.459</td> <td> 0.014</td> <td>    0.295</td> <td>    2.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p34</th> <td>   -1.4879</td> <td>    0.669</td> <td>   -2.224</td> <td> 0.026</td> <td>   -2.799</td> <td>   -0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p35</th> <td>    3.1487</td> <td>    0.567</td> <td>    5.554</td> <td> 0.000</td> <td>    2.038</td> <td>    4.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p36</th> <td>   -1.7585</td> <td>    0.926</td> <td>   -1.899</td> <td> 0.058</td> <td>   -3.573</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p37</th> <td>   -2.0649</td> <td>    0.806</td> <td>   -2.563</td> <td> 0.010</td> <td>   -3.644</td> <td>   -0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p38</th> <td>   -0.0627</td> <td>    0.122</td> <td>   -0.514</td> <td> 0.607</td> <td>   -0.302</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p39</th> <td>    1.3845</td> <td>    0.587</td> <td>    2.359</td> <td> 0.018</td> <td>    0.234</td> <td>    2.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p40</th> <td>   -1.0258</td> <td>    0.646</td> <td>   -1.589</td> <td> 0.112</td> <td>   -2.291</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p41</th> <td>    3.2480</td> <td>    0.561</td> <td>    5.793</td> <td> 0.000</td> <td>    2.149</td> <td>    4.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p42</th> <td>   -1.4985</td> <td>    0.895</td> <td>   -1.673</td> <td> 0.094</td> <td>   -3.253</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p43</th> <td>   -2.2258</td> <td>    0.815</td> <td>   -2.733</td> <td> 0.006</td> <td>   -3.822</td> <td>   -0.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p44</th> <td>   -0.0973</td> <td>    0.116</td> <td>   -0.838</td> <td> 0.402</td> <td>   -0.325</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p45</th> <td>    1.2173</td> <td>    0.517</td> <td>    2.355</td> <td> 0.019</td> <td>    0.204</td> <td>    2.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p46</th> <td>   -1.0197</td> <td>    0.629</td> <td>   -1.620</td> <td> 0.105</td> <td>   -2.253</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p47</th> <td>    2.8663</td> <td>    0.522</td> <td>    5.491</td> <td> 0.000</td> <td>    1.843</td> <td>    3.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p48</th> <td>   -1.5316</td> <td>    0.812</td> <td>   -1.886</td> <td> 0.059</td> <td>   -3.123</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p49</th> <td>   -1.9046</td> <td>    0.722</td> <td>   -2.637</td> <td> 0.008</td> <td>   -3.320</td> <td>   -0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p50</th> <td>   -0.0388</td> <td>    0.107</td> <td>   -0.363</td> <td> 0.717</td> <td>   -0.248</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p51</th> <td>    1.1241</td> <td>    0.526</td> <td>    2.137</td> <td> 0.033</td> <td>    0.093</td> <td>    2.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p52</th> <td>   -1.2032</td> <td>    0.634</td> <td>   -1.898</td> <td> 0.058</td> <td>   -2.445</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p53</th> <td>    2.8915</td> <td>    0.528</td> <td>    5.479</td> <td> 0.000</td> <td>    1.857</td> <td>    3.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p54</th> <td>   -1.7727</td> <td>    0.735</td> <td>   -2.412</td> <td> 0.016</td> <td>   -3.213</td> <td>   -0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p55</th> <td>   -2.1919</td> <td>    0.742</td> <td>   -2.955</td> <td> 0.003</td> <td>   -3.646</td> <td>   -0.738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p56</th> <td>   -0.0623</td> <td>    0.109</td> <td>   -0.570</td> <td> 0.568</td> <td>   -0.276</td> <td>    0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p57</th> <td>    1.1997</td> <td>    0.498</td> <td>    2.407</td> <td> 0.016</td> <td>    0.223</td> <td>    2.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p58</th> <td>   -1.1248</td> <td>    0.648</td> <td>   -1.735</td> <td> 0.083</td> <td>   -2.396</td> <td>    0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p59</th> <td>    3.1057</td> <td>    0.477</td> <td>    6.505</td> <td> 0.000</td> <td>    2.170</td> <td>    4.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p60</th> <td>   -2.0184</td> <td>    0.761</td> <td>   -2.654</td> <td> 0.008</td> <td>   -3.509</td> <td>   -0.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p61</th> <td>   -2.1581</td> <td>    0.720</td> <td>   -2.998</td> <td> 0.003</td> <td>   -3.569</td> <td>   -0.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p62</th> <td>   -0.0316</td> <td>    0.107</td> <td>   -0.296</td> <td> 0.767</td> <td>   -0.241</td> <td>    0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p63</th> <td>    1.1771</td> <td>    0.513</td> <td>    2.296</td> <td> 0.022</td> <td>    0.172</td> <td>    2.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p64</th> <td>   -1.3133</td> <td>    0.666</td> <td>   -1.971</td> <td> 0.049</td> <td>   -2.620</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p65</th> <td>    2.8298</td> <td>    0.501</td> <td>    5.654</td> <td> 0.000</td> <td>    1.849</td> <td>    3.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p66</th> <td>   -1.8962</td> <td>    0.754</td> <td>   -2.515</td> <td> 0.012</td> <td>   -3.374</td> <td>   -0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p67</th> <td>   -1.8676</td> <td>    0.698</td> <td>   -2.674</td> <td> 0.007</td> <td>   -3.236</td> <td>   -0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p68</th> <td>    0.0637</td> <td>    0.108</td> <td>    0.589</td> <td> 0.556</td> <td>   -0.148</td> <td>    0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p69</th> <td>    1.2327</td> <td>    0.547</td> <td>    2.253</td> <td> 0.024</td> <td>    0.161</td> <td>    2.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p70</th> <td>   -0.9935</td> <td>    0.707</td> <td>   -1.405</td> <td> 0.160</td> <td>   -2.379</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p71</th> <td>    2.9652</td> <td>    0.523</td> <td>    5.675</td> <td> 0.000</td> <td>    1.941</td> <td>    3.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p72</th> <td>   -1.7842</td> <td>    0.806</td> <td>   -2.214</td> <td> 0.027</td> <td>   -3.364</td> <td>   -0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p73</th> <td>   -1.8264</td> <td>    0.742</td> <td>   -2.460</td> <td> 0.014</td> <td>   -3.282</td> <td>   -0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p74</th> <td>    0.0169</td> <td>    0.120</td> <td>    0.140</td> <td> 0.889</td> <td>   -0.219</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p75</th> <td>    1.4738</td> <td>    0.590</td> <td>    2.498</td> <td> 0.012</td> <td>    0.318</td> <td>    2.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p76</th> <td>    0.2012</td> <td>    0.737</td> <td>    0.273</td> <td> 0.785</td> <td>   -1.244</td> <td>    1.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p77</th> <td>    4.1884</td> <td>    0.599</td> <td>    6.987</td> <td> 0.000</td> <td>    3.013</td> <td>    5.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p78</th> <td>   -1.6179</td> <td>    0.943</td> <td>   -1.716</td> <td> 0.086</td> <td>   -3.466</td> <td>    0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p79</th> <td>   -2.6333</td> <td>    0.871</td> <td>   -3.025</td> <td> 0.002</td> <td>   -4.340</td> <td>   -0.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p80</th> <td>   -0.0878</td> <td>    0.126</td> <td>   -0.698</td> <td> 0.485</td> <td>   -0.334</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p81</th> <td>   -1.2395</td> <td>  163.963</td> <td>   -0.008</td> <td> 0.994</td> <td> -322.600</td> <td>  320.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p82</th> <td>   -0.3805</td> <td>   50.683</td> <td>   -0.008</td> <td> 0.994</td> <td>  -99.718</td> <td>   98.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p83</th> <td>    0.8822</td> <td>  116.597</td> <td>    0.008</td> <td> 0.994</td> <td> -227.643</td> <td>  229.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p84</th> <td>    0.2844</td> <td>   38.099</td> <td>    0.007</td> <td> 0.994</td> <td>  -74.388</td> <td>   74.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p85</th> <td>    0.5151</td> <td>   68.003</td> <td>    0.008</td> <td> 0.994</td> <td> -132.768</td> <td>  133.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>p86</th> <td>   -0.4254</td> <td>   56.749</td> <td>   -0.007</td> <td> 0.994</td> <td> -111.652</td> <td>  110.801</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                                   gmm_lambeta Results                                                   \n",
       "=========================================================================================================================\n",
       "Dep. Variable:     ['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10']   Hansen J:                    1.158e-06\n",
       "Model:                                                               gmm_lambeta   Prob (Hansen J):                   nan\n",
       "Method:                                                                      GMM                                         \n",
       "Date:                                                           Thu, 11 May 2023                                         \n",
       "Time:                                                                   14:12:54                                         \n",
       "No. Observations:                                                            400                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "p 0            0.0285      0.007      4.141      0.000       0.015       0.042\n",
       "p 1            0.1566      0.067      2.342      0.019       0.026       0.288\n",
       "p 2           -1.0335      0.207     -4.994      0.000      -1.439      -0.628\n",
       "p 3            1.3332      0.310      4.307      0.000       0.727       1.940\n",
       "p 4           -0.0616      0.034     -1.814      0.070      -0.128       0.005\n",
       "p 5           -0.1505      0.093     -1.612      0.107      -0.333       0.033\n",
       "p 6           -0.0139      0.097     -0.143      0.886      -0.205       0.177\n",
       "p 7           -7.2378      1.955     -3.702      0.000     -11.070      -3.406\n",
       "p 8            0.2057      0.263      0.781      0.435      -0.311       0.722\n",
       "p 9            1.0658      0.510      2.088      0.037       0.065       2.066\n",
       "p10           -2.0867      0.799     -2.613      0.009      -3.652      -0.521\n",
       "p11            2.0114      0.292      6.887      0.000       1.439       2.584\n",
       "p12            0.1970      0.055      3.581      0.000       0.089       0.305\n",
       "p13           -0.5289      0.115     -4.584      0.000      -0.755      -0.303\n",
       "p14            0.0972      0.010      9.749      0.000       0.078       0.117\n",
       "p15            0.1694      0.063      2.690      0.007       0.046       0.293\n",
       "p16           -0.4603      0.187     -2.460      0.014      -0.827      -0.094\n",
       "p17            0.3688      0.270      1.366      0.172      -0.160       0.898\n",
       "p18           -0.0522      0.030     -1.757      0.079      -0.110       0.006\n",
       "p19           -0.1425      0.085     -1.681      0.093      -0.309       0.024\n",
       "p20           -0.1603      0.096     -1.662      0.097      -0.349       0.029\n",
       "p21            1.5367      0.669      2.298      0.022       0.226       2.848\n",
       "p22           -2.5200      0.677     -3.722      0.000      -3.847      -1.193\n",
       "p23            2.8601      0.608      4.702      0.000       1.668       4.052\n",
       "p24           -1.8361      0.953     -1.926      0.054      -3.705       0.033\n",
       "p25           -2.0637      0.898     -2.299      0.022      -3.823      -0.304\n",
       "p26           -0.0294      0.132     -0.223      0.823      -0.287       0.228\n",
       "p27            1.4306      0.606      2.359      0.018       0.242       2.619\n",
       "p28           -1.9528      0.611     -3.197      0.001      -3.150      -0.756\n",
       "p29            2.8642      0.581      4.929      0.000       1.725       4.003\n",
       "p30           -1.8990      0.895     -2.123      0.034      -3.652      -0.146\n",
       "p31           -1.8604      0.806     -2.308      0.021      -3.440      -0.281\n",
       "p32           -0.0212      0.124     -0.171      0.864      -0.263       0.221\n",
       "p33            1.4515      0.590      2.459      0.014       0.295       2.608\n",
       "p34           -1.4879      0.669     -2.224      0.026      -2.799      -0.177\n",
       "p35            3.1487      0.567      5.554      0.000       2.038       4.260\n",
       "p36           -1.7585      0.926     -1.899      0.058      -3.573       0.056\n",
       "p37           -2.0649      0.806     -2.563      0.010      -3.644      -0.486\n",
       "p38           -0.0627      0.122     -0.514      0.607      -0.302       0.177\n",
       "p39            1.3845      0.587      2.359      0.018       0.234       2.535\n",
       "p40           -1.0258      0.646     -1.589      0.112      -2.291       0.239\n",
       "p41            3.2480      0.561      5.793      0.000       2.149       4.347\n",
       "p42           -1.4985      0.895     -1.673      0.094      -3.253       0.257\n",
       "p43           -2.2258      0.815     -2.733      0.006      -3.822      -0.629\n",
       "p44           -0.0973      0.116     -0.838      0.402      -0.325       0.130\n",
       "p45            1.2173      0.517      2.355      0.019       0.204       2.230\n",
       "p46           -1.0197      0.629     -1.620      0.105      -2.253       0.214\n",
       "p47            2.8663      0.522      5.491      0.000       1.843       3.889\n",
       "p48           -1.5316      0.812     -1.886      0.059      -3.123       0.060\n",
       "p49           -1.9046      0.722     -2.637      0.008      -3.320      -0.489\n",
       "p50           -0.0388      0.107     -0.363      0.717      -0.248       0.171\n",
       "p51            1.1241      0.526      2.137      0.033       0.093       2.155\n",
       "p52           -1.2032      0.634     -1.898      0.058      -2.445       0.039\n",
       "p53            2.8915      0.528      5.479      0.000       1.857       3.926\n",
       "p54           -1.7727      0.735     -2.412      0.016      -3.213      -0.332\n",
       "p55           -2.1919      0.742     -2.955      0.003      -3.646      -0.738\n",
       "p56           -0.0623      0.109     -0.570      0.568      -0.276       0.152\n",
       "p57            1.1997      0.498      2.407      0.016       0.223       2.177\n",
       "p58           -1.1248      0.648     -1.735      0.083      -2.396       0.146\n",
       "p59            3.1057      0.477      6.505      0.000       2.170       4.041\n",
       "p60           -2.0184      0.761     -2.654      0.008      -3.509      -0.528\n",
       "p61           -2.1581      0.720     -2.998      0.003      -3.569      -0.747\n",
       "p62           -0.0316      0.107     -0.296      0.767      -0.241       0.177\n",
       "p63            1.1771      0.513      2.296      0.022       0.172       2.182\n",
       "p64           -1.3133      0.666     -1.971      0.049      -2.620      -0.007\n",
       "p65            2.8298      0.501      5.654      0.000       1.849       3.811\n",
       "p66           -1.8962      0.754     -2.515      0.012      -3.374      -0.418\n",
       "p67           -1.8676      0.698     -2.674      0.007      -3.236      -0.499\n",
       "p68            0.0637      0.108      0.589      0.556      -0.148       0.276\n",
       "p69            1.2327      0.547      2.253      0.024       0.161       2.305\n",
       "p70           -0.9935      0.707     -1.405      0.160      -2.379       0.392\n",
       "p71            2.9652      0.523      5.675      0.000       1.941       3.989\n",
       "p72           -1.7842      0.806     -2.214      0.027      -3.364      -0.205\n",
       "p73           -1.8264      0.742     -2.460      0.014      -3.282      -0.371\n",
       "p74            0.0169      0.120      0.140      0.889      -0.219       0.253\n",
       "p75            1.4738      0.590      2.498      0.012       0.318       2.630\n",
       "p76            0.2012      0.737      0.273      0.785      -1.244       1.646\n",
       "p77            4.1884      0.599      6.987      0.000       3.013       5.363\n",
       "p78           -1.6179      0.943     -1.716      0.086      -3.466       0.230\n",
       "p79           -2.6333      0.871     -3.025      0.002      -4.340      -0.927\n",
       "p80           -0.0878      0.126     -0.698      0.485      -0.334       0.159\n",
       "p81           -1.2395    163.963     -0.008      0.994    -322.600     320.121\n",
       "p82           -0.3805     50.683     -0.008      0.994     -99.718      98.957\n",
       "p83            0.8822    116.597      0.008      0.994    -227.643     229.408\n",
       "p84            0.2844     38.099      0.007      0.994     -74.388      74.957\n",
       "p85            0.5151     68.003      0.008      0.994    -132.768     133.798\n",
       "p86           -0.4254     56.749     -0.007      0.994    -111.652     110.801\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port = 'bm'\n",
    "ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts3 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "# Set the start and end dates of the analysis period\n",
    "# start_date = pd.to_datetime('1984-01-01') ##############\n",
    "# end_date = pd.to_datetime('1999-12-31') ##############\n",
    "ts3 = ts3.loc[(ts3.index >= start_date) & (ts3.index <= end_date)]\n",
    "\n",
    "\n",
    "mimick = ts3[['log_indprod_growth_nextyear',\n",
    "                'ex_mkt',\n",
    "                   'ex_b10ret',\n",
    "                   'ex_b5ret',\n",
    "                #    'ex_high_yd_bd_ret',\n",
    "                   'ex_gold_ret', \n",
    "                   'slope_ex_mkt_87', \n",
    "                   'slope_ex_mkt_9602', \n",
    "                   'rf', \n",
    "                   'lag_10y_3m_gov_bd_yd',\n",
    "                   'lag_1y_3m_gov_bd_yd',\n",
    "                   'lag_Baa_Aaa_bd_yd',\n",
    "                   'lag_sp_div_yd',\n",
    "                   'log_indprod_growth_lastyear',\n",
    "                   'infl_lastyear',\n",
    "                   'ex_mkt_lastyear',\n",
    "                   'lag_ex_mkt',\n",
    "                   'lag_ex_b10ret',\n",
    "                   'lag_ex_b5ret',\n",
    "                   'lag_ex_gold_ret',\n",
    "                   'lag_slope_ex_mkt_87',\n",
    "                   'lag_slope_ex_mkt_9602'\n",
    "                   ]].values\n",
    "T,K = mimick.shape\n",
    "exog_macro_factors = ts3[['ui','dsv','ats','sts','fx']].values\n",
    "T,M = exog_macro_factors.shape\n",
    "riskfree = ts3['rf'].values\n",
    "portfolios = ts3[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "T,P = portfolios.shape\n",
    "excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "\n",
    "class gmm_lambeta(GMM):\n",
    "    def momcond(self, params):\n",
    "        fRets = self.exog\n",
    "        pRets = self.endog\n",
    "        mimick = self.instrument\n",
    "        coeff = squeeze(array(params)) \n",
    "        \n",
    "        # first stage\n",
    "        mimick_endog = mimick[:,0]\n",
    "        mimick_exog = mimick[:,1:]\n",
    "        mimick_exog = sm.add_constant(mimick_exog)\n",
    "        T,K = mimick_exog.shape\n",
    "        mimick_exog = reshape(mimick_exog,(T,K))\n",
    "        mimick_endog = reshape(mimick_endog,(T,1))\n",
    "        mimick_coeff = coeff[:K]\n",
    "        mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "        mimick_err = (mimick_endog - mimick_exog @ mimick_coeff)\n",
    "        moments1 = mimick_exog * kron(mimick_err,ones((1,K)))\n",
    "      #   moments1 = moments1[1:] # delete the first row\n",
    "\n",
    "        # compute myp\n",
    "        myp = mimick_exog[:, 1:7] @ mimick_coeff[1:7] \n",
    "\n",
    "        # gmm\n",
    "        full_fRets = column_stack((myp, fRets))\n",
    "        T,P = pRets.shape\n",
    "        T,M = full_fRets.shape\n",
    "        betalam_params = params[K:]\n",
    "        # var_coeff = reshape(var_coeff,(N,M))\n",
    "      #   print(P, M)\n",
    "        beta = squeeze(array(betalam_params[:(P*M)]))\n",
    "        lam = squeeze(array(betalam_params[(P*M):]))\n",
    "        beta = reshape(beta,(P,M))\n",
    "        lam = reshape(lam,(M,1))\n",
    "        betalam = beta @ lam\n",
    "        expectedRet = full_fRets @ beta.T\n",
    "        e = pRets - expectedRet\n",
    "        # print(M, P)\n",
    "        moments_beta = kron(e,ones((1,M))) * tile(full_fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "        moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "        moments2 = hstack((moments_beta,moments_lam))\n",
    "        # print(moments2.shape)\n",
    "\n",
    "        return column_stack((moments1,moments2))\n",
    "    \n",
    "lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "lambeta_fit = lambeta_mod.fit(start_params=zeros(K+(P+1)*(M+1)), maxiter=1, inv_weights=eye(K+P*(M+2)), optim_method='bfgs', optim_args={'gtol': 1e-15, 'maxiter': 100000})\n",
    "lambeta_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0565d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[39mreturn\u001b[39;00m column_stack((moments1,moments2))\n\u001b[0;32m     88\u001b[0m lambeta_mod \u001b[39m=\u001b[39m gmm_lambeta(endog\u001b[39m=\u001b[39mexcessRet, exog\u001b[39m=\u001b[39mexog_macro_factors, instrument\u001b[39m=\u001b[39mmimick)\n\u001b[1;32m---> 89\u001b[0m lambeta_fit \u001b[39m=\u001b[39m lambeta_mod\u001b[39m.\u001b[39;49mfit(start_params\u001b[39m=\u001b[39;49mzeros(K\u001b[39m+\u001b[39;49m(P\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49m(M\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)), maxiter\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, inv_weights\u001b[39m=\u001b[39;49meye(K\u001b[39m+\u001b[39;49mP\u001b[39m*\u001b[39;49m(M\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m)), weights_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhac\u001b[39;49m\u001b[39m'\u001b[39;49m, wargs\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mmaxlag\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m11\u001b[39;49m}, optim_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbfgs\u001b[39;49m\u001b[39m'\u001b[39;49m, optim_args\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m1e-12\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m100000\u001b[39;49m})\n\u001b[0;32m     90\u001b[0m lambeta_fit\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\sandbox\\regression\\gmm.py:670\u001b[0m, in \u001b[0;36mGMM.fit\u001b[1;34m(self, start_params, maxiter, inv_weights, weights_method, wargs, has_optimal_weights, optim_method, optim_args)\u001b[0m\n\u001b[0;32m    668\u001b[0m     weights_ \u001b[39m=\u001b[39m weights  \u001b[39m# temporary alias used in jval\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     params, weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfititer(start,\n\u001b[0;32m    671\u001b[0m                                    maxiter\u001b[39m=\u001b[39;49mmaxiter,\n\u001b[0;32m    672\u001b[0m                                    start_invweights\u001b[39m=\u001b[39;49minv_weights,\n\u001b[0;32m    673\u001b[0m                                    weights_method\u001b[39m=\u001b[39;49mweights_method,\n\u001b[0;32m    674\u001b[0m                                    wargs\u001b[39m=\u001b[39;49mwargs,\n\u001b[0;32m    675\u001b[0m                                    optim_method\u001b[39m=\u001b[39;49moptim_method,\n\u001b[0;32m    676\u001b[0m                                    optim_args\u001b[39m=\u001b[39;49moptim_args)\n\u001b[0;32m    677\u001b[0m     \u001b[39m# TODO weights returned by fititer is inv_weights - not true anymore\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39m# weights_ currently not necessary and used anymore\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     weights_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mpinv(weights)\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\sandbox\\regression\\gmm.py:916\u001b[0m, in \u001b[0;36mGMM.fititer\u001b[1;34m(self, start, maxiter, start_invweights, weights_method, wargs, optim_method, optim_args)\u001b[0m\n\u001b[0;32m    912\u001b[0m             w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mpinv(winv)\n\u001b[0;32m    913\u001b[0m             \u001b[39m#this is still calling function not method\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \u001b[39m##            resgmm = fitgmm(momcond, (), start, weights=winv, fixed=None,\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39m##                            weightsoptimal=False)\u001b[39;00m\n\u001b[1;32m--> 916\u001b[0m             resgmm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitgmm(start, weights\u001b[39m=\u001b[39;49mw, optim_method\u001b[39m=\u001b[39;49moptim_method,\n\u001b[0;32m    917\u001b[0m                                  optim_args\u001b[39m=\u001b[39;49moptim_args)\n\u001b[0;32m    919\u001b[0m             moms \u001b[39m=\u001b[39m momcond(resgmm)\n\u001b[0;32m    920\u001b[0m             \u001b[39m# the following is S = cov_moments\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\sandbox\\regression\\gmm.py:768\u001b[0m, in \u001b[0;36mGMM.fitgmm\u001b[1;34m(self, start, weights, optim_method, optim_args)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mdet(weights))\n\u001b[0;32m    767\u001b[0m \u001b[39m#TODO: add other optimization options and results\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgmmobjective, start, args\u001b[39m=\u001b[39;49m(weights,),\n\u001b[0;32m    769\u001b[0m                  \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptim_args)\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1303\u001b[0m, in \u001b[0;36mfmin_bfgs\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback, xrtol)\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[39mMinimize a function using the BFGS algorithm.\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \n\u001b[0;32m   1295\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m opts \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mgtol\u001b[39m\u001b[39m'\u001b[39m: gtol,\n\u001b[0;32m   1297\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m'\u001b[39m: norm,\n\u001b[0;32m   1298\u001b[0m         \u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m: epsilon,\n\u001b[0;32m   1299\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp,\n\u001b[0;32m   1300\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: maxiter,\n\u001b[0;32m   1301\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mreturn_all\u001b[39m\u001b[39m'\u001b[39m: retall}\n\u001b[1;32m-> 1303\u001b[0m res \u001b[39m=\u001b[39m _minimize_bfgs(f, x0, args, fprime, callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n\u001b[0;32m   1305\u001b[0m \u001b[39mif\u001b[39;00m full_output:\n\u001b[0;32m   1306\u001b[0m     retlist \u001b[39m=\u001b[39m (res[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mjac\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mhess_inv\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   1307\u001b[0m                res[\u001b[39m'\u001b[39m\u001b[39mnfev\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mnjev\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1388\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001b[0m\n\u001b[0;32m   1385\u001b[0m pk \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mdot(Hk, gfk)\n\u001b[0;32m   1386\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1387\u001b[0m     alpha_k, fc, gc, old_fval, old_old_fval, gfkp1 \u001b[39m=\u001b[39m \\\n\u001b[1;32m-> 1388\u001b[0m              _line_search_wolfe12(f, myfprime, xk, pk, gfk,\n\u001b[0;32m   1389\u001b[0m                                   old_fval, old_old_fval, amin\u001b[39m=\u001b[39;49m\u001b[39m1e-100\u001b[39;49m, amax\u001b[39m=\u001b[39;49m\u001b[39m1e100\u001b[39;49m)\n\u001b[0;32m   1390\u001b[0m \u001b[39mexcept\u001b[39;00m _LineSearchError:\n\u001b[0;32m   1391\u001b[0m     \u001b[39m# Line search failed to find a better solution.\u001b[39;00m\n\u001b[0;32m   1392\u001b[0m     warnflag \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1160\u001b[0m, in \u001b[0;36m_line_search_wolfe12\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[39mSame as line_search_wolfe1, but fall back to line_search_wolfe2 if\u001b[39;00m\n\u001b[0;32m   1148\u001b[0m \u001b[39msuitable step length is not found, and raise an exception if a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \n\u001b[0;32m   1156\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m extra_condition \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mextra_condition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1160\u001b[0m ret \u001b[39m=\u001b[39m line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[0;32m   1161\u001b[0m                          old_fval, old_old_fval,\n\u001b[0;32m   1162\u001b[0m                          \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1164\u001b[0m \u001b[39mif\u001b[39;00m ret[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m extra_condition \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     xp1 \u001b[39m=\u001b[39m xk \u001b[39m+\u001b[39m ret[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m pk\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:84\u001b[0m, in \u001b[0;36mline_search_wolfe1\u001b[1;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(gval[\u001b[39m0\u001b[39m], pk)\n\u001b[0;32m     82\u001b[0m derphi0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(gfk, pk)\n\u001b[1;32m---> 84\u001b[0m stp, fval, old_fval \u001b[39m=\u001b[39m scalar_search_wolfe1(\n\u001b[0;32m     85\u001b[0m         phi, derphi, old_fval, old_old_fval, derphi0,\n\u001b[0;32m     86\u001b[0m         c1\u001b[39m=\u001b[39;49mc1, c2\u001b[39m=\u001b[39;49mc2, amax\u001b[39m=\u001b[39;49mamax, amin\u001b[39m=\u001b[39;49mamin, xtol\u001b[39m=\u001b[39;49mxtol)\n\u001b[0;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m stp, fc[\u001b[39m0\u001b[39m], gc[\u001b[39m0\u001b[39m], fval, old_fval, gval[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:161\u001b[0m, in \u001b[0;36mscalar_search_wolfe1\u001b[1;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[0;32m    159\u001b[0m     alpha1 \u001b[39m=\u001b[39m stp\n\u001b[0;32m    160\u001b[0m     phi1 \u001b[39m=\u001b[39m phi(stp)\n\u001b[1;32m--> 161\u001b[0m     derphi1 \u001b[39m=\u001b[39m derphi(stp)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:78\u001b[0m, in \u001b[0;36mline_search_wolfe1.<locals>.derphi\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mderphi\u001b[39m(s):\n\u001b[1;32m---> 78\u001b[0m     gval[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m fprime(xk \u001b[39m+\u001b[39;49m s\u001b[39m*\u001b[39;49mpk, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     79\u001b[0m     gc[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(gval[\u001b[39m0\u001b[39m], pk)\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:273\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    272\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 273\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[0;32m    274\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[1;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[0;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:167\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_grad\u001b[39m():\n\u001b[1;32m--> 167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m grad_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:164\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.grad_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad_wrapped\u001b[39m(x):\n\u001b[0;32m    163\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(grad(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\sandbox\\regression\\gmm.py:1107\u001b[0m, in \u001b[0;36mGMM.score\u001b[1;34m(self, params, weights, epsilon, centered)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(\u001b[39mself\u001b[39m, params, weights, epsilon\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, centered\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1106\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Score\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1107\u001b[0m     deriv \u001b[39m=\u001b[39m approx_fprime(params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgmmobjective, args\u001b[39m=\u001b[39;49m(weights,),\n\u001b[0;32m   1108\u001b[0m                           centered\u001b[39m=\u001b[39;49mcentered, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[0;32m   1110\u001b[0m     \u001b[39mreturn\u001b[39;00m deriv\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\tools\\numdiff.py:157\u001b[0m, in \u001b[0;36mapprox_fprime\u001b[1;34m(x, f, epsilon, args, kwargs, centered)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n\u001b[0;32m    156\u001b[0m         ei[k] \u001b[39m=\u001b[39m epsilon[k]\n\u001b[1;32m--> 157\u001b[0m         grad[k, :] \u001b[39m=\u001b[39m (f(\u001b[39m*\u001b[39;49m((x\u001b[39m+\u001b[39;49mei,)\u001b[39m+\u001b[39;49margs), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39m-\u001b[39m\n\u001b[0;32m    158\u001b[0m                       f(\u001b[39m*\u001b[39m((x\u001b[39m-\u001b[39mei,)\u001b[39m+\u001b[39margs), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\u001b[39m/\u001b[39m(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m epsilon[k])\n\u001b[0;32m    159\u001b[0m         ei[k] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m grad\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\sandbox\\regression\\gmm.py:832\u001b[0m, in \u001b[0;36mGMM.gmmobjective\u001b[1;34m(self, params, weights)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgmmobjective\u001b[39m(\u001b[39mself\u001b[39m, params, weights):\n\u001b[0;32m    816\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m    objective function for GMM minimization\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    830\u001b[0m \n\u001b[0;32m    831\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 832\u001b[0m     moms \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmomcond_mean(params)\n\u001b[0;32m    833\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(np\u001b[39m.\u001b[39mdot(moms, weights), moms)\n",
      "File \u001b[1;32mc:\\Users\\Huang\\OneDrive - Università Commerciale Luigi Bocconi\\python\\macro-fundamental-factors\\.env\\Lib\\site-packages\\statsmodels\\sandbox\\regression\\gmm.py:1070\u001b[0m, in \u001b[0;36mGMM.momcond_mean\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmomcond_mean\u001b[39m(\u001b[39mself\u001b[39m, params):\n\u001b[0;32m   1065\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[39m    mean of moment conditions,\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \n\u001b[0;32m   1068\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m-> 1070\u001b[0m     momcond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmomcond(params)\n\u001b[0;32m   1071\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnobs_moms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_moms \u001b[39m=\u001b[39m momcond\u001b[39m.\u001b[39mshape\n\u001b[0;32m   1072\u001b[0m     \u001b[39mreturn\u001b[39;00m momcond\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 83\u001b[0m, in \u001b[0;36mgmm_lambeta.momcond\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m     81\u001b[0m moments_beta \u001b[39m=\u001b[39m kron(e,ones((\u001b[39m1\u001b[39m,M))) \u001b[39m*\u001b[39m tile(full_fRets,P)     \u001b[39m# E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \u001b[39;00m\n\u001b[0;32m     82\u001b[0m moments_lam \u001b[39m=\u001b[39m pRets \u001b[39m-\u001b[39m betalam\u001b[39m.\u001b[39mT    \u001b[39m# E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m moments2 \u001b[39m=\u001b[39m hstack((moments_beta,moments_lam))\n\u001b[0;32m     84\u001b[0m \u001b[39m# print(moments2.shape)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m column_stack((moments1,moments2))\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prova\n",
    "port = 'bm'\n",
    "ports_data = pd.read_csv(resource + f'{port}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts3 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "# Set the start and end dates of the analysis period\n",
    "# start_date = pd.to_datetime('1984-01-01') ##############\n",
    "# end_date = pd.to_datetime('1999-12-31') ##############\n",
    "ts3 = ts3.loc[(ts3.index >= start_date) & (ts3.index <= end_date)]\n",
    "\n",
    "\n",
    "mimick = ts3[['log_indprod_growth_nextyear',\n",
    "                'ex_mkt',\n",
    "                   'ex_b10ret',\n",
    "                   'ex_b5ret',\n",
    "                #    'ex_high_yd_bd_ret',\n",
    "                   'ex_gold_ret', \n",
    "                   'slope_ex_mkt_87', \n",
    "                   'slope_ex_mkt_9602', \n",
    "                   'rf', \n",
    "                   'lag_10y_3m_gov_bd_yd',\n",
    "                   'lag_1y_3m_gov_bd_yd',\n",
    "                   'lag_Baa_Aaa_bd_yd',\n",
    "                   'lag_sp_div_yd',\n",
    "                   'log_indprod_growth_lastyear',\n",
    "                   'infl_lastyear',\n",
    "                   'ex_mkt_lastyear',\n",
    "                   'lag_ex_mkt',\n",
    "                   'lag_ex_b10ret',\n",
    "                   'lag_ex_b5ret',\n",
    "                   'lag_ex_gold_ret',\n",
    "                   'lag_slope_ex_mkt_87',\n",
    "                   'lag_slope_ex_mkt_9602'\n",
    "                   ]].values\n",
    "T,K = mimick.shape\n",
    "exog_macro_factors = ts3[['ui','dsv','ats','sts','fx']].values\n",
    "T,M = exog_macro_factors.shape\n",
    "riskfree = ts3['rf'].values\n",
    "portfolios = ts3[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "T,P = portfolios.shape\n",
    "excessRet = portfolios - reshape(riskfree,(T,1))\n",
    "\n",
    "class gmm_lambeta(GMM):\n",
    "    def momcond(self, params):\n",
    "        fRets = self.exog\n",
    "        pRets = self.endog\n",
    "        mimick = self.instrument\n",
    "        coeff = squeeze(array(params)) \n",
    "        \n",
    "        # first stage\n",
    "        mimick_endog = mimick[:,0]\n",
    "        mimick_exog = mimick[:,1:]\n",
    "        mimick_exog = sm.add_constant(mimick_exog)\n",
    "        T,K = mimick_exog.shape\n",
    "        mimick_exog = reshape(mimick_exog,(T,K))\n",
    "        mimick_endog = reshape(mimick_endog,(T,1))\n",
    "        mimick_coeff = coeff[:K]\n",
    "        mimick_coeff = reshape(mimick_coeff,(K,1))\n",
    "        mimick_err = (mimick_endog - mimick_exog @ mimick_coeff)\n",
    "        moments1 = mimick_exog * kron(mimick_err,ones((1,K)))\n",
    "      #   moments1 = moments1[1:] # delete the first row\n",
    "\n",
    "        # compute myp\n",
    "        myp = mimick_exog[:, 1:7] @ mimick_coeff[1:7] \n",
    "\n",
    "        # gmm\n",
    "        full_fRets = column_stack((myp, fRets))\n",
    "        T,P = pRets.shape\n",
    "        T,M = full_fRets.shape\n",
    "        betalam_params = params[K:]\n",
    "        # var_coeff = reshape(var_coeff,(N,M))\n",
    "      #   print(P, M)\n",
    "        beta = squeeze(array(betalam_params[:(P*M)]))\n",
    "        lam = squeeze(array(betalam_params[(P*M):]))\n",
    "        beta = reshape(beta,(P,M))\n",
    "        lam = reshape(lam,(M,1))\n",
    "        betalam = beta @ lam\n",
    "        expectedRet = full_fRets @ beta.T\n",
    "        e = pRets - expectedRet\n",
    "        # print(M, P)\n",
    "        moments_beta = kron(e,ones((1,M))) * tile(full_fRets,P)     # E[(R^{ex,i} - beta^i*FF)*FF]=0 (orthogon. conditions for the time series regression) \n",
    "        moments_lam = pRets - betalam.T    # E[R^{ex,i} – beta^i*lambda] = 0 (pricing equations using the MPR)\n",
    "        moments2 = hstack((moments_beta,moments_lam))\n",
    "        # print(moments2.shape)\n",
    "\n",
    "        return column_stack((moments1,moments2))\n",
    "    \n",
    "lambeta_mod = gmm_lambeta(endog=excessRet, exog=exog_macro_factors, instrument=mimick)\n",
    "lambeta_fit = lambeta_mod.fit(start_params=zeros(K+(P+1)*(M+1)), maxiter=1, inv_weights=eye(K+P*(M+2)), weights_method='hac', wargs={'maxlag':11}, optim_method='bfgs', optim_args={'gtol': 1e-12, 'maxiter': 100000})\n",
    "lambeta_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 65\n",
      "         Function evaluations: 10865\n",
      "         Gradient evaluations: 162\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.900360\n",
      "         Iterations: 210\n",
      "         Function evaluations: 19576\n",
      "         Gradient evaluations: 292\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 14\n",
      "         Function evaluations: 5237\n",
      "         Gradient evaluations: 78\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2.955226\n",
      "         Iterations: 75\n",
      "         Function evaluations: 8118\n",
      "         Gradient evaluations: 121\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 13\n",
      "         Function evaluations: 3695\n",
      "         Gradient evaluations: 55\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.209559\n",
      "         Iterations: 96\n",
      "         Function evaluations: 9391\n",
      "         Gradient evaluations: 140\n",
      "Stored 'beta_port' (list)\n",
      "Stored 'beta_t_stat_port' (list)\n",
      "Stored 'premia_port' (list)\n",
      "Stored 'premia_t_stat_port' (list)\n"
     ]
    }
   ],
   "source": [
    "# premia_port = []\n",
    "# premia_t_stat_port = []\n",
    "# beta_port = []\n",
    "# beta_t_stat_port = []\n",
    "\n",
    "# # For book-to-market portfolios\n",
    "# for index,ports in enumerate(['bm','size','mom']):\n",
    "#     ports_data = pd.read_csv(resource + f'{ports}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "#     ts3 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "#     # Set the start and end dates of the analysis period\n",
    "#     # start_date = pd.to_datetime('1984-01-01') ##############\n",
    "#     # end_date = pd.to_datetime('1999-12-31') ##############\n",
    "#     ts3 = ts3.loc[(ts3.index >= start_date) & (ts3.index <= end_date)]\n",
    "\n",
    "#     macro_factors = ts3[['myp','ui','dsv','ats','sts','fx']].values\n",
    "#     financial_factors = ts3[['ex_mkt','smb','hml','mom']].values\n",
    "#     riskfree = ts3['rf'].values\n",
    "#     portfolios = ts3[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "\n",
    "#     T,N = portfolios.shape\n",
    "#     excessRet = portfolios - np.reshape(riskfree,(T,1))\n",
    "#     K = np.size(macro_factors,1)\n",
    "\n",
    "#     # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "#     betas = []\n",
    "#     for i in range(N):\n",
    "#         res = sm.OLS(excessRet[:,i],sm.add_constant(macro_factors)).fit()\n",
    "#         betas.append(res.params[1:])\n",
    "\n",
    "#     avgReturn = excessRet.mean(axis=0)\n",
    "#     avgReturn.shape = N,1\n",
    "#     betas = array(betas)\n",
    "#     res = sm.OLS(avgReturn, betas).fit()\n",
    "#     riskPremia = res.params\n",
    "\n",
    "#     # The starting values are computed the first step estimates are found using the non-linear optimizer. The initial weighting matrix is just the identify matrix.\n",
    "#     riskPremia.shape = K\n",
    "#     startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "#     Winv = np.eye(N*(K+1))\n",
    "#     args = (excessRet, macro_factors, Winv)\n",
    "#     iteration = 0\n",
    "#     functionCount = 0\n",
    "#     # step1opt = fmin_bfgs(gmm_objective, startingVals, args=args, callback=iter_print)\n",
    "#     step1opt = fmin_bfgs(gmm_objective, startingVals, args=args)\n",
    "\n",
    "#     # Here we look at the risk premia estimates from the first step (inefficient) estimates.\n",
    "#     premia = step1opt[-K:]\n",
    "#     premia = Series(premia,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "#     # print('Annualized Risk Premia (First step)')\n",
    "#     # print(100 * premia)\n",
    "\n",
    "#     # Next the first step estimates are used to estimate the moment conditions which are in-turn used to estimate the optimal weighting matrix for the moment conditions. This is then used as an input for the 2nd-step estimates.\n",
    "#     out = gmm_objective(step1opt, excessRet, macro_factors, Winv, out=True)\n",
    "#     S = np.cov(out[1].T)\n",
    "#     Winv2 = inv(S)\n",
    "#     args = (excessRet, macro_factors, Winv2)\n",
    "\n",
    "#     iteration = 0\n",
    "#     functionCount = 0\n",
    "#     # step2opt = fmin_bfgs(gmm_objective, step1opt, args=args, callback=iter_print)   \n",
    "#     step2opt = fmin_bfgs(gmm_objective, step1opt, args=args)  \n",
    "\n",
    "#     # The annualized risk premia.\n",
    "#     premia = step2opt[-K:]\n",
    "#     # premia = Series(premia,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "#     # print('Annualized Risk Premia')\n",
    "#     # print(100 * premia)\n",
    "\n",
    "#     # Finally the VCV of the parameter estimates is computed.\n",
    "#     out = gmm_objective(step2opt, excessRet, macro_factors, Winv2, out=True)\n",
    "#     G = gmm_G(step2opt, excessRet, macro_factors)\n",
    "#     S = np.cov(out[1].T)\n",
    "#     vcv = inv(G @ inv(S) @ G.T)/T\n",
    "#     premia_vcv = vcv[-K:,-K:]\n",
    "#     premia_stderr = np.diag(premia_vcv)\n",
    "#     # premia_stderr = Series(premia_stderr,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "#     # print('t-stats')\n",
    "#     # print(premia / premia_stderr)\n",
    "#     premia_t_stat = premia / premia_stderr\n",
    "\n",
    "#     beta = reshape(step2opt[:-K],(N,K))\n",
    "#     beta_vcv = vcv[:-K,:-K]\n",
    "#     beta_stderr = np.diag(beta_vcv)\n",
    "#     beta_t_stat = step2opt[:-K] / beta_stderr\n",
    "#     beta_t_stat = reshape(beta_t_stat,(N,K))\n",
    "\n",
    "#     premia_port.append(premia)\n",
    "#     premia_t_stat_port.append(premia_t_stat)\n",
    "#     beta_port.append(beta)\n",
    "#     beta_t_stat_port.append(beta_t_stat)\n",
    "\n",
    "# %store beta_port\n",
    "# %store beta_t_stat_port\n",
    "# %store premia_port\n",
    "# %store premia_t_stat_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b197cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = LinearFactorModelGMM(portfolios, macro_factors)\n",
    "# model = LinearFactorModelGMM(excessRet, macro_factors, risk_free=False)\n",
    "\n",
    "# # Estimate the model parameters\n",
    "# results = model.fit(cov_type='kernel',bandwidth=12)\n",
    "\n",
    "# # Print the summary of results\n",
    "# print(results.full_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.params\n",
    "# results.tstats\n",
    "# results.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# startingVals = np.zeros((1,K))\n",
    "# Winv = np.eye(N)\n",
    "# args = (excessRet, macro_factors, Winv)\n",
    "# iteration = 0\n",
    "# functionCount = 0\n",
    "# opt_b = fmin_bfgs(gmm_objective_b, startingVals, args=args, callback=iter_print)\n",
    "# sdf_loading = Series(opt_b,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "# sdf_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96486fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The GMM objective which needs to be minimized (to get factor loading b)\n",
    "# def moment_b(params, fRets):\n",
    "#     b = params\n",
    "#     error = 1 - fRets @ b\n",
    "#     return error\n",
    "\n",
    "# def moment_consumption1(params, exog):\n",
    "#     beta, gamma = params\n",
    "#     r_forw1, c_forw1, c = exog.T  # unwrap iterable (ndarray)\n",
    "    \n",
    "#     # moment condition without instrument    \n",
    "#     err = 1 - beta * (1 + r_forw1) * np.power(c_forw1 / c, -gamma)\n",
    "#     return -err\n",
    "\n",
    "# endog1 = np.zeros(macro_factors.shape[0])    \n",
    "# mod10 = gmm.NonlinearIVGMM(endog1, macro_factors, excessRet, moment_b)\n",
    "# w0inv = np.eye(N)\n",
    "# res10 = mod10.fit(inv_weights=w0inv, maxiter=100, weights_method='hac', wargs={'maxlag':4}) \n",
    "# print(res10.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
