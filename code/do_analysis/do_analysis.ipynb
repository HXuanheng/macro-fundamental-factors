{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "c8130ac7-479c-4e2a-b49d-960773ada07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"../../data/generated/\"\n",
    "results = \"../../results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "69dba9df-277d-4c64-8e8c-be8345b7e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from linearmodels.panel import PanelOLS, compare\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from gmm import *\n",
    "from linearmodels.asset_pricing import LinearFactorModelGMM\n",
    "from tabprintin.beautify import *\n",
    "from statsmodels.sandbox.regression import gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "aeb3f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start and end dates of the analysis period\n",
    "start_date = pd.to_datetime('1978-01-01')\n",
    "# start_date = pd.to_datetime('1975-01-01')\n",
    "end_date = pd.to_datetime('2008-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "28cb978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "plots = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.read_csv(resource + 'time_series.csv', parse_dates=['date'], index_col=['date'])\n",
    "ts.index.freq = 'M'\n",
    "\n",
    "# Compute the log change of industrial production over next 12 months (or just growth)\n",
    "ts['log_indprod_growth_nextyear'] = np.log(ts['ind_prod'].shift(-12) / ts['ind_prod'].shift(-1))\n",
    "# ts['indprod_growth_nextyear'] = ts['ind_prod'].shift(-12) / ts['ind_prod'].shift(-1) - 1\n",
    "\n",
    "##################\n",
    "## Base assets ###\n",
    "##################\n",
    "\n",
    "# Market return\n",
    "# ts['ex_mkt'] = ts['ex_mkt'] /100\n",
    "ts['lag_ex_mkt'] = ts['ex_mkt'].shift(1)\n",
    "\n",
    "# Compute the excess return of the long-term government bond portfolio\n",
    "ts['ex_long_gov_ret'] = ts['long_gov_ret'] - ts['rf']\n",
    "ts['lag_ex_long_gov_ret'] = ts['ex_long_gov_ret'].shift(1)\n",
    "\n",
    "# Compute the excess return of the intermediate-term government bond portfolio\n",
    "ts['ex_medium_gov_ret'] = ts['medium_gov_ret'] - ts['rf']\n",
    "ts['lag_ex_medium_gov_ret'] = ts['ex_medium_gov_ret'].shift(1)\n",
    "\n",
    "# Compute the excess return of the high-yield bond portfolio\n",
    "ts['ex_high_yd_bd_ret'] = ts['high_yd_bd_ret'] - ts['rf']\n",
    "ts['lag_ex_high_yd_bd_ret'] = ts['ex_high_yd_bd_ret'].shift(1)\n",
    "\n",
    "# Compute the return for gold index\n",
    "ts['ex_gold_ret'] = ts['gold'].pct_change() - ts['rf']\n",
    "ts['lag_ex_gold_ret'] = ts['ex_gold_ret'].shift(1)\n",
    "\n",
    "# Create dummies for 1987 (stock market crash) and 1996-2002 (Internet bubble period)\n",
    "ts['dummy_87'] = (ts.index.year == 1987).astype(int)\n",
    "ts['dummy_96_02'] = ((ts.index.year >= 1996) & (ts.index.year <= 2002)).astype(int)\n",
    "\n",
    "#########################\n",
    "### Control Variables ###\n",
    "#########################\n",
    "# Compute the 10 year minus 3 month government bond yield\n",
    "ts['lag_10y_3m_gov_bd_yd'] = (ts['DGS10'] - ts['DTB3']).shift(1)\n",
    "\n",
    "# Compute the 1 year minus 3 month government bond yield\n",
    "ts['lag_1y_3m_gov_bd_yd'] = (ts['DGS1'] - ts['DTB3']).shift(1)\n",
    "\n",
    "# Baa minus Aaa corporate bond yield\n",
    "ts['lag_Baa_Aaa_bd_yd'] = (ts['BAA'] - ts['AAA']).shift(1)\n",
    "\n",
    "# Compute the dividend yield on the S&P 500 index\n",
    "ts['lag_sp_div_yd'] = (ts['sp_div'] / ts['sp_price']).shift(1)\n",
    "\n",
    "# Compute the log change of industrial production over last 12 months (or just growth)\n",
    "ts['log_indprod_growth_lastyear'] = np.log(ts['ind_prod'].shift(13) / ts['ind_prod'].shift(1))\n",
    "# ts['indprod_growth_lastyear'] = ts['ind_prod'].shift(13) / ts['ind_prod'].shift(1)  - 1\n",
    "\n",
    "\n",
    "# Compute the inflation over last 12 months\n",
    "# ts['infl_lastyear'] = (ts['cpi'].shift(1) - ts['cpi'].shift(13)) / ts['cpi'].shift(13)\n",
    "ts['infl_lastyear'] = np.log(ts['cpi'].shift(1) / ts['cpi'].shift(13))\n",
    "\n",
    "# Compute the market portfolio excess return over last 12 months\n",
    "# [Controllare e sbagliato]\n",
    "ts['ex_mkt_lastyear'] = (((ts['ex_mkt'] + 100)/100).rolling(13).apply(lambda x: x[:-1].prod()) - 1) * 100\n",
    "\n",
    "# Interactions\n",
    "ts['slope_ex_mkt_87'] = ts['ex_mkt'] * ts['dummy_87']\n",
    "ts['slope_ex_mkt_9602'] = ts['ex_mkt'] * ts['dummy_96_02']\n",
    "\n",
    "ts['lag_slope_ex_mkt_87'] = ts['slope_ex_mkt_87'].shift(1)\n",
    "ts['lag_slope_ex_mkt_9602'] = ts['slope_ex_mkt_9602'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "ae5fddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = ts.loc[(ts.index >= start_date) & (ts.index <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "e4f7ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod1 = PanelOLS.from_formula('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_long_gov_ret+ex_medium_gov_ret+ex_high_yd_bd_ret+ex_gold_ret+ex_mkt*dummy_87+ex_mkt*dummy_96_02+lagged_10y_3m_gov_bd_yd+lagged_1y_3m_gov_bd_yd+lagged_Baa_Aaa_bd_yd+lagged_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear',\n",
    "#                              data=ts)\n",
    "\n",
    "mod1 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_long_gov_ret+ex_medium_gov_ret+ex_high_yd_bd_ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear',\n",
    "                data=ts1)\n",
    "\n",
    "mod2 = smf.ols('log_indprod_growth_nextyear ~ 1+ex_mkt+ex_long_gov_ret+ex_medium_gov_ret+ex_high_yd_bd_ret+ex_gold_ret+slope_ex_mkt_87+slope_ex_mkt_9602+rf+lag_10y_3m_gov_bd_yd+lag_1y_3m_gov_bd_yd+lag_Baa_Aaa_bd_yd+lag_sp_div_yd+log_indprod_growth_lastyear+infl_lastyear+ex_mkt_lastyear+ lag_ex_mkt+lag_ex_long_gov_ret+lag_ex_medium_gov_ret+lag_ex_high_yd_bd_ret+lag_ex_gold_ret+lag_slope_ex_mkt_87+lag_slope_ex_mkt_9602',\n",
    "                data=ts1)\n",
    "\n",
    "reg1 = mod1.fit(cov_type='HAC',cov_kwds={'maxlags':11})\n",
    "reg2 = mod2.fit(cov_type='HAC',cov_kwds={'maxlags':11})\n",
    "\n",
    "tables.append(reg1)\n",
    "tables.append(reg2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acbbb9e5",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "eb9c2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the one-year ahead industrial production growth expectatitions factor\n",
    "coef = reg2.params\n",
    "# ts['myp'] = coef['ex_mkt'] * ts['ex_mkt'] + coef['ex_long_gov_ret'] * ts['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * ts['ex_medium_gov_ret'] + coef['ex_high_yd_bd_ret'] * ts['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * ts['ex_gold_ret'] + coef['slope_ex_mkt_87'] * ts['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * ts['slope_ex_mkt_9602']\n",
    "def compute_myp(row):\n",
    "    if pd.isna(row['ex_high_yd_bd_ret']):\n",
    "        return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_long_gov_ret'] * row['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * row['ex_medium_gov_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "    else:\n",
    "        return coef['ex_mkt'] * row['ex_mkt'] + coef['ex_long_gov_ret'] * row['ex_long_gov_ret'] + coef['ex_medium_gov_ret'] * row['ex_medium_gov_ret'] + coef['ex_high_yd_bd_ret'] * row['ex_high_yd_bd_ret'] + coef['ex_gold_ret'] * row['ex_gold_ret'] + coef['slope_ex_mkt_87'] * row['slope_ex_mkt_87'] + coef['slope_ex_mkt_9602'] * row['slope_ex_mkt_9602']\n",
    "\n",
    "ts['myp'] = ts.apply(compute_myp, axis=1)\n",
    "\n",
    "# Get the unexpected inflation factor\n",
    "ts['infl'] = np.log(ts['cpi'] / ts['cpi'].shift(1))\n",
    "ts['delta_infl'] = ts['infl'] - ts['infl'].shift(1)\n",
    "inf_ma1 = sm.tsa.arima.ARIMA(ts['delta_infl'], order=(0,0,1)).fit()\n",
    "ts['fit_delta_infl'] = inf_ma1.fittedvalues\n",
    "ts['ui'] = ts['delta_infl'] - ts['fit_delta_infl']\n",
    "\n",
    "# Get the change in the aggregate survival probability factor\n",
    "mod3 = smf.ols('v_dsv ~ 1 + my_dsv', data=ts).fit()\n",
    "parm = mod3.params\n",
    "ts['fit_dsv'] = parm['Intercept'] + parm['my_dsv'] * ts['my_dsv']\n",
    "ts['dsv'] = np.where(ts['v_dsv'].notna(), ts['v_dsv'], ts['fit_dsv'])\n",
    "\n",
    "# Get the change in the average level of the term structure factor\n",
    "ts['mean_term_structure'] = ts[['DTB3', 'DGS10']].mean(axis=1)\n",
    "ts['ats'] = ts['mean_term_structure'] - ts['mean_term_structure'].shift(1)\n",
    "\n",
    "# Get the change in the slope of the term strucutre factor\n",
    "ts['diff_term_structure'] = ts['DGS10'] - ts['DTB3']\n",
    "ts['sts'] = ts['diff_term_structure'] - ts['diff_term_structure'].shift(1)\n",
    "\n",
    "# Get the change in the multilateral US dollar exchange rate factor\n",
    "mod4 = smf.ols('TWEXM ~ 1 + DTWEXAFEGS', data=ts).fit()\n",
    "parm = mod4.params\n",
    "ts['fit_TWEXM'] = parm['Intercept'] + parm['DTWEXAFEGS'] * ts['DTWEXAFEGS']\n",
    "ts['exchange_rate'] = np.where(ts['TWEXM'].notna(), ts['TWEXM'], ts['fit_TWEXM'])\n",
    "ts['fx'] = ts['exchange_rate'] - ts['exchange_rate'].shift(1)\n",
    "# ts['fx'] = np.log(ts['exchange_rate'] / ts['exchange_rate'].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "75d6e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start and end dates of the analysis period\n",
    "# start_date = pd.to_datetime('1975-01-01') ##############\n",
    "# end_date = pd.to_datetime('1999-12-31') ##############\n",
    "ts2 = ts.loc[(ts.index >= start_date) & (ts.index <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "6d32923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = ts2[['myp','ui','dsv','ats','sts','fx']].describe().T  \n",
    "tables.append(summary_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a80c551",
   "metadata": {},
   "source": [
    "Granger causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "58dd71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAR model with constant term\n",
    "model = VAR(ts2[['hml','smb','mom','myp','ui','dsv','ats','sts','fx']])\n",
    "var = model.fit(maxlags=1, trend='c')\n",
    "\n",
    "tables.append(var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e27c2d8",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "ba12c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000002\n",
      "         Iterations: 19\n",
      "         Function evaluations: 4166\n",
      "         Gradient evaluations: 62\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 1.167547\n",
      "         Iterations: 136\n",
      "         Function evaluations: 12674\n",
      "         Gradient evaluations: 189\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 12\n",
      "         Function evaluations: 3830\n",
      "         Gradient evaluations: 57\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2.098526\n",
      "         Iterations: 95\n",
      "         Function evaluations: 8923\n",
      "         Gradient evaluations: 133\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 12\n",
      "         Function evaluations: 4233\n",
      "         Gradient evaluations: 63\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.228586\n",
      "         Iterations: 140\n",
      "         Function evaluations: 12541\n",
      "         Gradient evaluations: 187\n"
     ]
    }
   ],
   "source": [
    "premia_port = []\n",
    "premia_t_stat_port = []\n",
    "beta_port = []\n",
    "beta_t_stat_port = []\n",
    "\n",
    "# For book-to-market portfolios\n",
    "for index,ports in enumerate(['bm','size','mom']):\n",
    "    ports_data = pd.read_csv(resource + f'{ports}_port.csv', parse_dates=['date'], index_col=['date'])\n",
    "    ts3 = pd.merge(ts, ports_data, on='date', how='left')\n",
    "\n",
    "    # Set the start and end dates of the analysis period\n",
    "    # start_date = pd.to_datetime('1984-01-01') ##############\n",
    "    # end_date = pd.to_datetime('1999-12-31') ##############\n",
    "    ts3 = ts3.loc[(ts3.index >= start_date) & (ts3.index <= end_date)]\n",
    "\n",
    "    macro_factors = ts3[['myp','ui','dsv','ats','sts','fx']].values\n",
    "    financial_factors = ts3[['ex_mkt','smb','hml','mom']].values\n",
    "    riskfree = ts3['rf'].values\n",
    "    portfolios = ts3[['dec_1','dec_2','dec_3','dec_4','dec_5','dec_6','dec_7','dec_8','dec_9','dec_10']].values\n",
    "\n",
    "    T,N = portfolios.shape\n",
    "    excessRet = portfolios - np.reshape(riskfree,(T,1))\n",
    "    K = np.size(macro_factors,1)\n",
    "\n",
    "    # Starting values for the factor loadings and rick premia are estimated using OLS and simple means.\n",
    "    betas = []\n",
    "    for i in range(N):\n",
    "        res = sm.OLS(excessRet[:,i],sm.add_constant(macro_factors)).fit()\n",
    "        betas.append(res.params[1:])\n",
    "\n",
    "    avgReturn = excessRet.mean(axis=0)\n",
    "    avgReturn.shape = N,1\n",
    "    betas = array(betas)\n",
    "    res = sm.OLS(avgReturn, betas).fit()\n",
    "    riskPremia = res.params\n",
    "\n",
    "    # The starting values are computed the first step estimates are found using the non-linear optimizer. The initial weighting matrix is just the identify matrix.\n",
    "    riskPremia.shape = K\n",
    "    startingVals = np.concatenate((betas.flatten(),riskPremia))\n",
    "\n",
    "    Winv = np.eye(N*(K+1))\n",
    "    args = (excessRet, macro_factors, Winv)\n",
    "    iteration = 0\n",
    "    functionCount = 0\n",
    "    # step1opt = fmin_bfgs(gmm_objective, startingVals, args=args, callback=iter_print)\n",
    "    step1opt = fmin_bfgs(gmm_objective, startingVals, args=args)\n",
    "\n",
    "    # Here we look at the risk premia estimates from the first step (inefficient) estimates.\n",
    "    premia = step1opt[-K:]\n",
    "    premia = Series(premia,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "    # print('Annualized Risk Premia (First step)')\n",
    "    # print(100 * premia)\n",
    "\n",
    "    # Next the first step estimates are used to estimate the moment conditions which are in-turn used to estimate the optimal weighting matrix for the moment conditions. This is then used as an input for the 2nd-step estimates.\n",
    "    out = gmm_objective(step1opt, excessRet, macro_factors, Winv, out=True)\n",
    "    S = np.cov(out[1].T)\n",
    "    Winv2 = inv(S)\n",
    "    args = (excessRet, macro_factors, Winv2)\n",
    "\n",
    "    iteration = 0\n",
    "    functionCount = 0\n",
    "    # step2opt = fmin_bfgs(gmm_objective, step1opt, args=args, callback=iter_print)   \n",
    "    step2opt = fmin_bfgs(gmm_objective, step1opt, args=args)  \n",
    "\n",
    "    # The annualized risk premia.\n",
    "    premia = step2opt[-K:]\n",
    "    # premia = Series(premia,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "    # print('Annualized Risk Premia')\n",
    "    # print(100 * premia)\n",
    "\n",
    "    # Finally the VCV of the parameter estimates is computed.\n",
    "    out = gmm_objective(step2opt, excessRet, macro_factors, Winv2, out=True)\n",
    "    G = gmm_G(step2opt, excessRet, macro_factors)\n",
    "    S = np.cov(out[1].T)\n",
    "    vcv = inv(G @ inv(S) @ G.T)/T\n",
    "    premia_vcv = vcv[-K:,-K:]\n",
    "    premia_stderr = np.diag(premia_vcv)\n",
    "    # premia_stderr = Series(premia_stderr,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "    # print('t-stats')\n",
    "    # print(premia / premia_stderr)\n",
    "    premia_t_stat = premia / premia_stderr\n",
    "\n",
    "    beta = reshape(step2opt[:-K],(N,K))\n",
    "    beta_vcv = vcv[:-K,:-K]\n",
    "    beta_stderr = np.diag(beta_vcv)\n",
    "    beta_t_stat = step2opt[:-K] / beta_stderr\n",
    "    beta_t_stat = reshape(beta_t_stat,(N,K))\n",
    "\n",
    "    premia_port.append(premia)\n",
    "    premia_t_stat_port.append(premia_t_stat)\n",
    "    beta_port.append(beta)\n",
    "    beta_t_stat_port.append(beta_t_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "5b197cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = LinearFactorModelGMM(portfolios, macro_factors)\n",
    "# model = LinearFactorModelGMM(excessRet, macro_factors, risk_free=False)\n",
    "\n",
    "# # Estimate the model parameters\n",
    "# results = model.fit(cov_type='kernel',bandwidth=12)\n",
    "\n",
    "# # Print the summary of results\n",
    "# print(results.full_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "c3abda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.params\n",
    "# results.tstats\n",
    "# results.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "b728192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# startingVals = np.zeros((1,K))\n",
    "# Winv = np.eye(N)\n",
    "# args = (excessRet, macro_factors, Winv)\n",
    "# iteration = 0\n",
    "# functionCount = 0\n",
    "# opt_b = fmin_bfgs(gmm_objective_b, startingVals, args=args, callback=iter_print)\n",
    "# sdf_loading = Series(opt_b,index=['myp','ui','dsv','ats','sts','fx'])\n",
    "# sdf_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "96486fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The GMM objective which needs to be minimized (to get factor loading b)\n",
    "# def moment_b(params, fRets):\n",
    "#     b = params\n",
    "#     error = 1 - fRets @ b\n",
    "#     return error\n",
    "\n",
    "# def moment_consumption1(params, exog):\n",
    "#     beta, gamma = params\n",
    "#     r_forw1, c_forw1, c = exog.T  # unwrap iterable (ndarray)\n",
    "    \n",
    "#     # moment condition without instrument    \n",
    "#     err = 1 - beta * (1 + r_forw1) * np.power(c_forw1 / c, -gamma)\n",
    "#     return -err\n",
    "\n",
    "# endog1 = np.zeros(macro_factors.shape[0])    \n",
    "# mod10 = gmm.NonlinearIVGMM(endog1, macro_factors, excessRet, moment_b)\n",
    "# w0inv = np.eye(N)\n",
    "# res10 = mod10.fit(inv_weights=w0inv, maxiter=100, weights_method='hac', wargs={'maxlag':4}) \n",
    "# print(res10.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
